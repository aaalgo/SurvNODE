{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchdiffeq import odeint\n",
    "# from torchdiffeq import odeint_adjoint as odeint\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "# from sksurv.datasets import load_flchain\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "from pycox.evaluation import EvalSurv\n",
    "from ray import tune\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# random_seed = 1991\n",
    "\n",
    "# torch.manual_seed(random_seed)\n",
    "# np.random.seed(random_seed)\n",
    "\n",
    "# CUDA for PyTorch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "# torch.backends.cudnn.benchmark = True\n",
    "# device = 'cpu'\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, name, patience=7, verbose=False, delta=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.name+'_checkpoint.pt')\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchdiffeq import odeint\n",
    "# from torchdiffeq import odeint_adjoint as odeint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "     \n",
    "        \n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "        Encoding of the initial values of the memory states\n",
    "        Input: \n",
    "            - number of covariates\n",
    "            - number of memory states\n",
    "            - hidden layer neurons, given as array (e.g. [10,10,10])\n",
    "            - dropout for hidden layers, given as array (e.g. [0.2,0.3,0.4])\n",
    "    \"\"\"\n",
    "    def __init__(self,num_in,num_latent,layers,p_dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.net = nn.Sequential(*((nn.Linear(num_in,layers[0]), nn.ReLU(), nn.Dropout(p_dropout[0])) + tuple(tup for element in tuple(((nn.Linear(layers[i],layers[i+1]), nn.ReLU(), nn.Dropout(p_dropout[i+1])) for i in range(len(layers)-1))) for tup in element) + (nn.Linear(layers[-1],num_latent),)))\n",
    "        for m in self.net.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, mean=0, std=0.1)\n",
    "                nn.init.constant_(m.bias, val=0)\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class ODEFunc(nn.Module):\n",
    "    \"\"\"\n",
    "        KFE_KBE function to calculate the derivatives in the ODEsolver\n",
    "        Input: \n",
    "            - transition matrix giving possible transitions (e.g. [[NA,1,1],[NA,NA,1],[NA,NA,NA]] for the illness-death model)\n",
    "            - number of covariates\n",
    "            - number of memory states\n",
    "            - hidden layer neurons, given as array (e.g. [10,10,10])\n",
    "            - dropout for hidden layers, given as array (e.g. [0.2,0.3,0.4])\n",
    "            - softplus parameter (should be left at 1.)\n",
    "    \"\"\"\n",
    "    def __init__(self,transition_matrix,num_in,num_latent,layers,softplus_beta=1.):\n",
    "        super(ODEFunc, self).__init__()\n",
    "        \n",
    "        self.softplus_beta = softplus_beta\n",
    "        self.transition_matrix = transition_matrix\n",
    "        self.trans_dim = transition_matrix.shape[0]\n",
    "        self.num_latent = num_latent\n",
    "        self.number_of_hazards = int(np.nansum(transition_matrix.flatten().cpu()))\n",
    "        self.num_probs = np.prod(transition_matrix.shape)\n",
    "        # use this NN if covariates are to be included\n",
    "#         self.net = nn.Sequential(*((nn.Linear(2*self.num_probs+self.number_of_hazards+num_latent+num_in+1,layers[0]), nn.Tanh()) + tuple(tup for element in tuple(((nn.Linear(layers[i],layers[i+1]), nn.Tanh()) for i in range(len(layers)-1))) for tup in element) + (nn.Linear(layers[-1],self.number_of_hazards+num_latent),)))\n",
    "        self.net = nn.Sequential(*((nn.Linear(2*self.num_probs+self.number_of_hazards+2*num_latent+1,layers[0]), nn.Tanh()) + tuple(tup for element in tuple(((nn.Linear(layers[i],layers[i+1]), nn.Tanh()) for i in range(len(layers)-1))) for tup in element) + (nn.Linear(layers[-1],self.number_of_hazards+num_latent),)))\n",
    "\n",
    "        count = 0\n",
    "        length = len(list(self.net.modules()))\n",
    "        for m in self.net.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, mean=0, std=0.1)\n",
    "                nn.init.constant_(m.bias, val=0.)\n",
    "            if count==length-1:\n",
    "                nn.init.normal_(m.weight, mean=0, std=0)\n",
    "            count += 1\n",
    "        self.num_in = num_in\n",
    "    \n",
    "#     def set_x(self, x):\n",
    "#         self.x = x\n",
    "\n",
    "    def set_y0(self, y0):\n",
    "        self.y0 = y0\n",
    "\n",
    "    # KFE_KBE function\n",
    "    def forward(self, t, y):\n",
    "        # pass values through NN\n",
    "        out = self.net(torch.cat((y,self.y0,torch.tensor([t],device=y.device).repeat((y.shape[0],1))),1))\n",
    "        \n",
    "        # build Q matrix from output\n",
    "        qvec = torch.nn.functional.softplus(out[:,:self.number_of_hazards],beta=self.softplus_beta)\n",
    "        q = torch.zeros(self.trans_dim, self.trans_dim,device=y.device).repeat((y.shape[0],1,1))\n",
    "        q[self.transition_matrix.repeat((y.shape[0],1,1))==1] = qvec.flatten()\n",
    "        q[torch.eye(self.trans_dim, self.trans_dim,device=y.device).repeat((y.shape[0],1,1)) == 1] = -torch.sum(q,2).flatten()\n",
    "        \n",
    "        # get P matrix\n",
    "        P = torch.reshape(y[:,:self.num_probs],(y.shape[0],self.trans_dim,self.trans_dim))\n",
    "        P_back = torch.reshape(y[:,self.num_probs:(2*self.num_probs)],(y.shape[0],self.trans_dim,self.trans_dim))\n",
    "        # calculate right side of KFE and KBE\n",
    "        Pprime = torch.bmm(P, q)\n",
    "        Pprime_back = -torch.bmm(q, P_back)\n",
    "        return torch.cat((Pprime.reshape(y.shape[0],self.num_probs),Pprime_back.reshape(y.shape[0],self.num_probs),qvec,out[:,self.number_of_hazards:]),1)\n",
    "     \n",
    "class ODEBlock(nn.Module):\n",
    "    \"\"\"\n",
    "        Helper Function to define the initial value problem\n",
    "    \"\"\"\n",
    "    def __init__(self, odefunc):\n",
    "        super(ODEBlock, self).__init__()\n",
    "        self.odefunc = odefunc\n",
    "        self.num_probs = odefunc.num_probs\n",
    "        self.trans_dim = odefunc.trans_dim\n",
    "        self.number_of_hazards = odefunc.number_of_hazards\n",
    "        self.transition_matrix = odefunc.transition_matrix\n",
    "\n",
    "    def forward(self, y0, x, tinterval):\n",
    "#         self.odefunc.set_x(x)\n",
    "        self.odefunc.set_y0(y0)\n",
    "        p0 = torch.eye(self.trans_dim,device=y0.device).reshape(self.num_probs).repeat((y0.shape[0],1))\n",
    "        Q0 = torch.zeros(self.number_of_hazards,device=x.device).repeat((y0.shape[0],1))\n",
    "        yin = torch.cat((p0,p0,Q0,y0),1)\n",
    "        out = odeint(self.odefunc, yin, tinterval, method=\"dopri5\", atol=1e-8, rtol=1e-8)\n",
    "        return out       \n",
    "        \n",
    "class SurvNODE(nn.Module):\n",
    "    \"\"\"\n",
    "        SurvNODE class, \n",
    "    \"\"\"\n",
    "    def __init__(self,odeblock,encoder):\n",
    "        super(SurvNODE, self).__init__()\n",
    "        self.odeblock = odeblock\n",
    "        self.encoder = encoder\n",
    "        self.num_probs = odeblock.num_probs\n",
    "        self.trans_dim = odeblock.trans_dim\n",
    "        self.number_of_hazards = odeblock.number_of_hazards\n",
    "        self.transition_matrix = odeblock.transition_matrix\n",
    "\n",
    "    def forward(self, x, tstart, tstop, from_state, to_state):\n",
    "        # get P_ij(0,t) and P_ij(t,0) at all batch times\n",
    "        out = self.odeblock(self.encoder(x),x,torch.unique(torch.cat([torch.tensor([0.],device=x.device),tstart,tstop])))\n",
    "        \n",
    "        # get P_ij(s,t) through Kolmogorov backward equation\n",
    "        tstart_indices = torch.flatten(torch.cat([(torch.unique(torch.cat([torch.tensor([0.],device=x.device),tstart,tstop])) == time).nonzero() for time in tstart]))\n",
    "        Ttstartinv = torch.cat([out[tstart_indices[i],i,self.num_probs:(self.num_probs*2)] for i in range(len(tstart))]).reshape((tstart.shape[0],self.trans_dim,self.trans_dim))\n",
    "        tstop_indices = torch.flatten(torch.cat([(torch.unique(torch.cat([torch.tensor([0.],device=x.device),tstart,tstop])) == time).nonzero() for time in tstop]))\n",
    "        Ttstop = torch.cat([out[tstop_indices[i],i,:self.num_probs] for i in range(len(tstop))]).reshape((tstop.shape[0],self.trans_dim,self.trans_dim))\n",
    "        S = torch.bmm(Ttstartinv,Ttstop)\n",
    "        S = torch.cat([S[i:i+1,from_state[i]-1,from_state[i]-1] for i in range(len(from_state))])\n",
    "        \n",
    "        \n",
    "#         # get P_ij(s,0) by inverting P_ij(0,s)\n",
    "#         tstart_indices = torch.flatten(torch.cat([(torch.unique(torch.cat([torch.tensor([0.],device=x.device),tstart,tstop])) == time).nonzero() for time in tstart]))\n",
    "#         Ttstart = out[tstart_indices,[i for i in range(tstart.shape[0])],:self.num_probs].reshape((tstart.shape[0],self.trans_dim,self.trans_dim))\n",
    "#         Ttstartinv = torch.inverse(Ttstart)\n",
    "#         # # inverse with conditioning (?)\n",
    "#         # Ttstartinv = torch.inverse(Ttstart+1e-5*torch.eye(Ttstart.shape[1],device=x.device).flatten().repeat(Ttstart.shape[0]).reshape(Ttstart.shape))\n",
    "#         tstop_indices = torch.flatten(torch.cat([(torch.unique(torch.cat([torch.tensor([0.],device=x.device),tstart,tstop])) == time).nonzero() for time in tstop]))\n",
    "#         Ttstop = out[tstop_indices,[i for i in range(tstop.shape[0])],:self.num_probs].reshape((tstop.shape[0],self.trans_dim,self.trans_dim))\n",
    "#         S = torch.bmm(Ttstartinv,Ttstop)\n",
    "#         S = torch.cat([S[i:i+1,from_state[i]-1,from_state[i]-1] for i in range(len(from_state))])\n",
    "        \n",
    "        # get lambda at tstop\n",
    "        net_in = torch.cat((torch.cat([out[tstop_indices[i],i:i+1,:] for i in range(len(tstop))]),self.encoder(x),tstop.reshape(-1,1)),1)\n",
    "        qvec = torch.nn.functional.softplus(self.odeblock.odefunc.net(net_in)[:,:self.number_of_hazards],beta=self.odeblock.odefunc.softplus_beta)\n",
    "        q = torch.zeros(self.trans_dim, self.trans_dim,device=x.device).repeat((x.shape[0],1,1))\n",
    "        q[self.transition_matrix.repeat((x.shape[0],1,1))==1] = qvec.flatten()\n",
    "        lam = torch.cat([q[t:t+1,from_state[t]-1,to_state[t]-1] for t in range(len(from_state))])\n",
    "        # get all augmented hazards at the final time (t=multiplier) for loss term\n",
    "        net_in = torch.cat((out[-1,:,:],self.encoder(x),torch.tensor([max(tstop)],device=x.device).repeat(tstop.reshape(-1,1).shape)),1)\n",
    "        out = self.odeblock.odefunc.net(net_in)\n",
    "        all_hazards_T = torch.cat((torch.nn.functional.softplus(out[:,:self.number_of_hazards],beta=self.odeblock.odefunc.softplus_beta),out[:,self.number_of_hazards:]),-1)\n",
    "        \n",
    "        return (S,lam,all_hazards_T)\n",
    "    \n",
    "    def predict(self,x,tvec):\n",
    "        \"\"\"\n",
    "            Prediction of survival based on covariates x at times in tvec.\n",
    "            This function returns the transition matrix P_ij(0,t) at every t in tvec.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            out = self.odeblock(self.encoder(x),x,tvec.float().to(x.device))\n",
    "            T = out[:,:,:self.odeblock.odefunc.num_probs].reshape((tvec.shape[0],x.shape[0],self.trans_dim,self.trans_dim))\n",
    "        return T\n",
    "    \n",
    "    def predict_hazard(self,x,tvec):\n",
    "        \"\"\"\n",
    "            Predict cause specific hazard function based on covariates x at times in tvec.\n",
    "            This function returns the matrix Q of instantaneous hazards over time.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            tvec = tvec.float().to(x.device)\n",
    "            out = self.odeblock(self.encoder(x),x,tvec)\n",
    "            Qvec = torch.zeros((tvec.shape[0],x.shape[0],self.trans_dim,self.trans_dim),device=x.device)\n",
    "            for i in range(tvec.shape[0]):\n",
    "                net_in = torch.cat((out[i,:,:],tvec[i].repeat(x.shape)),1)\n",
    "                temp = self.odeblock.odefunc.net(net_in)\n",
    "                qvec = torch.nn.functional.softplus(temp[:,:self.number_of_hazards],beta=self.odeblock.odefunc.softplus_beta)\n",
    "                Q = torch.zeros(self.trans_dim, self.trans_dim,device=x.device).repeat((x.shape[0],1,1))\n",
    "                Q[self.transition_matrix.repeat((x.shape[0],1,1))==1] = qvec.flatten()\n",
    "                Q[torch.eye(self.trans_dim, self.trans_dim,device=x.device).repeat((x.shape[0],1,1)) == 1] = -torch.sum(Q,2).flatten()\n",
    "                Qvec[i,:,:,:] = Q\n",
    "        return Qvec\n",
    "    \n",
    "    def predict_cumhazard(self,x,tvec):\n",
    "        \"\"\"\n",
    "            Predict cumulative hazard function based on covariates x at times in tvec.\n",
    "            The cumulative cause specific hazards are given as the integral from 0 to t over the cause specific hazards.\n",
    "            This function returns a vector of cause specific cumulative hazards over time.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            tvec = tvec.float().to(x.device)\n",
    "            tvec = torch.unique(torch.cat([torch.tensor([0.],device=x.device),tvec]))\n",
    "            out = self.odeblock(self.encoder(x),x,tvec)\n",
    "            qvec = out[:,:,(2*self.num_probs):(2*self.num_probs+self.number_of_hazards)]\n",
    "            Qvec = torch.zeros((tvec.shape[0],x.shape[0],self.trans_dim,self.trans_dim),device=x.device)\n",
    "            Qvec[self.transition_matrix.repeat((tvec.shape[0],x.shape[0],1,1))==1] = qvec.flatten()\n",
    "        return Qvec\n",
    "\n",
    "            \n",
    "def loss(odesurv,x,Tstart,Tstop,From,To,trans,status,mu=1e-4):\n",
    "    \"\"\"\n",
    "        Loss function\n",
    "        Parameter mu regulates the influence of the Lyapunov loss\n",
    "    \"\"\"\n",
    "    trans_exist = torch.tensor([odesurv.transition_matrix[From[i]-1,To[i]-1] for i in range(len(From))])\n",
    "    trans_exist = torch.where(trans_exist==1)\n",
    "    x = x[trans_exist]\n",
    "    Tstart = Tstart[trans_exist]\n",
    "    Tstop = Tstop[trans_exist]\n",
    "    From = From[trans_exist]\n",
    "    To = To[trans_exist]\n",
    "    status = status[trans_exist]\n",
    "    \n",
    "    S,lam,all_h_T = odesurv(x,Tstart,Tstop,From,To)\n",
    "    loglik = -(status*torch.log(lam)+torch.log(S)).mean()\n",
    "    reg = torch.norm(all_h_T,2,dim=1).mean()\n",
    "\n",
    "    return (loglik + mu*reg), loglik, reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measures(odesurv,initial,x,Tstart,Tstop,From,To,trans,status, multiplier=1.,points=500):\n",
    "    with torch.no_grad():\n",
    "        time_grid = np.linspace(0, multiplier, points)\n",
    "        pvec = torch.zeros((points,x.shape[0]))\n",
    "        surv_ode = odesurv.predict(x,torch.from_numpy(np.linspace(0,multiplier,points)).float().to(x.device))\n",
    "        pvec = torch.einsum(\"ilkj,k->ilj\",(surv_ode[:,:,:,:],initial))[:,:,0].cpu()\n",
    "        pvec = np.array(pvec.cpu().detach())\n",
    "        surv_ode_df = pd.DataFrame(pvec)\n",
    "        surv_ode_df.loc[:,\"time\"] = np.linspace(0,multiplier,points)\n",
    "        surv_ode_df = surv_ode_df.set_index([\"time\"])\n",
    "        ev_ode = EvalSurv(surv_ode_df, np.array(Tstop.cpu()), np.array(status.cpu()), censor_surv='km')\n",
    "        conc = ev_ode.concordance_td('antolini')\n",
    "        ibs = ev_ode.integrated_brier_score(time_grid)\n",
    "        inbll = ev_ode.integrated_nbll(time_grid)\n",
    "    return conc,ibs,inbll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metabrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn_pandas import DataFrameMapper\n",
    "import pandas as pd\n",
    "\n",
    "def make_dataloader(df,Tmax,batchsize):\n",
    "    cols_standardize = ['x0', 'x1', 'x2', 'x3', 'x8']\n",
    "    cols_leave = ['x4', 'x5', 'x6', 'x7']\n",
    "\n",
    "    standardize = [([col], StandardScaler()) for col in cols_standardize]\n",
    "    leave = [(col, None) for col in cols_leave]\n",
    "\n",
    "    x_mapper = DataFrameMapper(standardize + leave)\n",
    "    X = x_mapper.fit_transform(df).astype('float32')\n",
    "    \n",
    "    X = torch.from_numpy(X).float().to(device)\n",
    "    T = torch.from_numpy(df[[\"duration\"]].values).float().flatten().to(device)\n",
    "    T = T/Tmax\n",
    "    T[T==0] = 1e-8\n",
    "    E = torch.from_numpy(df[[\"event\"]].values).float().flatten().to(device)\n",
    "\n",
    "    Tstart = torch.from_numpy(np.array([0 for i in range(T.shape[0])])).float().to(device)\n",
    "    From = torch.tensor([1],device=device).repeat((T.shape))\n",
    "    To = torch.tensor([2],device=device).repeat((T.shape))\n",
    "    trans = torch.tensor([1],device=device).repeat((T.shape))\n",
    "\n",
    "    dataset = TensorDataset(X,Tstart,T,From,To,trans,E)\n",
    "    loader = DataLoader(dataset, batch_size=batchsize, shuffle=True)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def odesurv_manual_benchmark(df_train, df_test,config,name):\n",
    "    torch.cuda.empty_cache()\n",
    "    df_train, df_val = train_test_split(df_train, test_size=0.2, stratify=df_train.loc[:,\"event\"])\n",
    "    \n",
    "    Tmax = df_train[\"duration\"].max()\n",
    "    \n",
    "    train_loader = make_dataloader(df_train,Tmax/config[\"multiplier\"],int(len(df_train)*config[\"batch_size\"]))\n",
    "    val_loader = make_dataloader(df_val,Tmax/config[\"multiplier\"],len(df_val))\n",
    "    test_loader = make_dataloader(df_test,Tmax/config[\"multiplier\"],len(df_test))\n",
    "    \n",
    "    num_in = 9\n",
    "    num_latent = config[\"num_latent\"]\n",
    "    layers_encoder =  [config[\"encoder_neurons\"]]*config[\"num_encoder_layers\"]\n",
    "    dropout_encoder = [config[\"encoder_dropout\"]]*config[\"num_encoder_layers\"]\n",
    "    layers_odefunc =  [config[\"odefunc_neurons\"]]*config[\"num_odefunc_layers\"]\n",
    "\n",
    "    trans_matrix = torch.tensor([[np.nan,1],[np.nan,np.nan]]).to(device)\n",
    "\n",
    "    encoder = Encoder(num_in,num_latent,layers_encoder, dropout_encoder).to(device)\n",
    "    odefunc = ODEFunc(trans_matrix,num_in,num_latent,layers_odefunc,config[\"softplus_beta\"]).to(device)\n",
    "    block = ODEBlock(odefunc).to(device)\n",
    "    odesurv = SurvNODE(block,encoder).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(odesurv.parameters(), weight_decay = config[\"weight_decay\"], lr=config[\"lr\"])\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=config[\"scheduler_gamma\"], patience=config[\"scheduler_epoch\"])\n",
    "\n",
    "    early_stopping = EarlyStopping(name=name,patience=config[\"patience\"], verbose=True)\n",
    "    for i in tqdm(range(1000)):\n",
    "        odesurv.train()\n",
    "        for mini,ds in enumerate(train_loader):\n",
    "            myloss,_,_ = loss(odesurv,*ds,mu=config[\"mu\"])\n",
    "            optimizer.zero_grad()\n",
    "            myloss.backward()    \n",
    "            optimizer.step()\n",
    "\n",
    "        \n",
    "        odesurv.eval()\n",
    "        with torch.no_grad():\n",
    "            lossval,conc,ibs,ibnll = 0., 0., 0., 0.\n",
    "            for _,ds in enumerate(val_loader):\n",
    "                t1,_,_ = loss(odesurv,*ds,mu=config[\"mu\"])\n",
    "                lossval += t1.item()\n",
    "                t1,t2,t3 = measures(odesurv,torch.tensor([1.,0.],device=device),*ds,multiplier=config[\"multiplier\"])\n",
    "                conc += t1\n",
    "                ibs += t2\n",
    "                ibnll += t3\n",
    "            early_stopping(lossval/len(val_loader), odesurv)\n",
    "            scheduler.step(lossval/len(val_loader))\n",
    "            \n",
    "            conc_test,ibs_test,ibnll_test = 0., 0., 0.\n",
    "            print(\"it: \"+str(i)+\", validation loss=\"+str(lossval/len(val_loader))+\", c=\"+str(conc/len(val_loader))+\", ibs=\"+str(ibs/len(val_loader))+\", ibnll=\"+str(ibnll/len(val_loader)))\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "    odesurv.load_state_dict(torch.load(name+'_checkpoint.pt'))\n",
    "\n",
    "    odesurv.eval()\n",
    "    with torch.no_grad():\n",
    "        conc,ibs,ibnll = 0., 0., 0.\n",
    "        for _,ds in enumerate(test_loader):\n",
    "            t1,t2,t3 = measures(odesurv,torch.tensor([1.,0.],device=device),*ds,multiplier=config[\"multiplier\"])\n",
    "            conc += t1\n",
    "            ibs += t2\n",
    "            ibnll += t3\n",
    "    return conc/len(test_loader), ibs/len(test_loader), ibnll/len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da89a60da4a04f66a1e4060c60a0183d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (inf --> 0.886843).  Saving model ...\n",
      "it: 0, validation loss=0.8868432641029358, c=0.6578368497691854, ibs=0.1642798065425157, ibnll=0.498292686670822\n",
      "Validation loss decreased (0.886843 --> 0.879550).  Saving model ...\n",
      "it: 1, validation loss=0.8795498013496399, c=0.6590141586888496, ibs=0.1642293558857981, ibnll=0.49958194464109623\n",
      "Validation loss decreased (0.879550 --> 0.873906).  Saving model ...\n",
      "it: 2, validation loss=0.8739055395126343, c=0.6512377234563311, ibs=0.16426601813039846, ibnll=0.5002501762430546\n",
      "Validation loss decreased (0.873906 --> 0.865098).  Saving model ...\n",
      "it: 3, validation loss=0.8650981187820435, c=0.6407348886203799, ibs=0.1624251734706775, ibnll=0.4957305165253631\n",
      "Validation loss decreased (0.865098 --> 0.853708).  Saving model ...\n",
      "it: 4, validation loss=0.8537078499794006, c=0.627691545063048, ibs=0.15906297169157135, ibnll=0.4869142390563532\n",
      "Validation loss decreased (0.853708 --> 0.841377).  Saving model ...\n",
      "it: 5, validation loss=0.8413766622543335, c=0.6225795458066116, ibs=0.15530020932627064, ibnll=0.47654722964965956\n",
      "Validation loss decreased (0.841377 --> 0.830182).  Saving model ...\n",
      "it: 6, validation loss=0.830181896686554, c=0.6234470365895219, ibs=0.15269822067477715, ibnll=0.4687990907788118\n",
      "Validation loss decreased (0.830182 --> 0.825257).  Saving model ...\n",
      "it: 7, validation loss=0.8252571821212769, c=0.627691545063048, ibs=0.15305574070734715, ibnll=0.46818766443777055\n",
      "Validation loss decreased (0.825257 --> 0.824029).  Saving model ...\n",
      "it: 8, validation loss=0.824029266834259, c=0.6289927812374136, ibs=0.15422642760369132, ibnll=0.4691836155264795\n",
      "Validation loss decreased (0.824029 --> 0.823469).  Saving model ...\n",
      "it: 9, validation loss=0.8234689235687256, c=0.6324007807417046, ibs=0.1550297353154406, ibnll=0.46946898160483636\n",
      "Validation loss decreased (0.823469 --> 0.820756).  Saving model ...\n",
      "it: 10, validation loss=0.8207558393478394, c=0.6354679802955665, ibs=0.1541357125248791, ibnll=0.4661467462988453\n",
      "Validation loss decreased (0.820756 --> 0.816677).  Saving model ...\n",
      "it: 11, validation loss=0.8166767954826355, c=0.6400532887195216, ibs=0.15321190439414742, ibnll=0.4634443191624895\n",
      "Validation loss decreased (0.816677 --> 0.812478).  Saving model ...\n",
      "it: 12, validation loss=0.8124780058860779, c=0.650556123555473, ibs=0.15383875840683178, ibnll=0.4647323479597029\n",
      "Validation loss decreased (0.812478 --> 0.807598).  Saving model ...\n",
      "it: 13, validation loss=0.8075979948043823, c=0.6572791771230287, ibs=0.15278352562758338, ibnll=0.46205988794814346\n",
      "Validation loss decreased (0.807598 --> 0.801619).  Saving model ...\n",
      "it: 14, validation loss=0.8016194701194763, c=0.6627629581435697, ibs=0.14997812495968457, ibnll=0.45501603100238525\n",
      "Validation loss decreased (0.801619 --> 0.797319).  Saving model ...\n",
      "it: 15, validation loss=0.7973188161849976, c=0.6704774297487375, ibs=0.14913656432925304, ibnll=0.4528015805740795\n",
      "Validation loss decreased (0.797319 --> 0.791837).  Saving model ...\n",
      "it: 16, validation loss=0.7918366193771362, c=0.6754345199367972, ibs=0.1476394429317712, ibnll=0.44886896390146874\n",
      "Validation loss decreased (0.791837 --> 0.788350).  Saving model ...\n",
      "it: 17, validation loss=0.7883498072624207, c=0.6793072466462187, ibs=0.14723839450783086, ibnll=0.44755871872067793\n",
      "Validation loss decreased (0.788350 --> 0.785663).  Saving model ...\n",
      "it: 18, validation loss=0.7856630086898804, c=0.6818167735539239, ibs=0.14757806325780828, ibnll=0.4481427683671058\n",
      "EarlyStopping counter: 1 out of 20\n",
      "it: 19, validation loss=0.7867102026939392, c=0.6818787371812746, ibs=0.14810909630276464, ibnll=0.44901132383748293\n",
      "EarlyStopping counter: 2 out of 20\n",
      "it: 20, validation loss=0.7887366414070129, c=0.6826223007094835, ibs=0.14928067806318854, ibnll=0.4515940406999039\n",
      "EarlyStopping counter: 3 out of 20\n",
      "it: 21, validation loss=0.7908920049667358, c=0.6833968460513679, ibs=0.1497614711751333, ibnll=0.45261770837434256\n",
      "EarlyStopping counter: 4 out of 20\n",
      "it: 22, validation loss=0.791750967502594, c=0.6846671004120581, ibs=0.15019439418522287, ibnll=0.4536025393281183\n",
      "EarlyStopping counter: 5 out of 20\n",
      "it: 23, validation loss=0.788741409778595, c=0.6877962635932707, ibs=0.15015878189137877, ibnll=0.4535251304081997\n",
      "Validation loss decreased (0.785663 --> 0.783864).  Saving model ...\n",
      "it: 24, validation loss=0.7838642001152039, c=0.6890355361402857, ibs=0.1492453781786108, ibnll=0.45110932046196733\n",
      "Validation loss decreased (0.783864 --> 0.781617).  Saving model ...\n",
      "it: 25, validation loss=0.7816171646118164, c=0.6904297177556774, ibs=0.14742882886955808, ibnll=0.446264423043887\n",
      "Validation loss decreased (0.781617 --> 0.780166).  Saving model ...\n",
      "it: 26, validation loss=0.7801661491394043, c=0.6911113176565357, ibs=0.14725724089217257, ibnll=0.4456480837567994\n",
      "EarlyStopping counter: 1 out of 20\n",
      "it: 27, validation loss=0.7801940441131592, c=0.6919478266257707, ibs=0.14904349039711146, ibnll=0.4500114053547674\n",
      "EarlyStopping counter: 2 out of 20\n",
      "it: 28, validation loss=0.7822726964950562, c=0.6924435356445766, ibs=0.14967593277757674, ibnll=0.45163455407014913\n",
      "EarlyStopping counter: 3 out of 20\n",
      "it: 29, validation loss=0.7863767743110657, c=0.692722371967655, ibs=0.1501219067433793, ibnll=0.45282018117118955\n",
      "EarlyStopping counter: 4 out of 20\n",
      "it: 30, validation loss=0.7815165519714355, c=0.6943334262787744, ibs=0.1500796446942869, ibnll=0.4528496526810534\n",
      "Validation loss decreased (0.780166 --> 0.775948).  Saving model ...\n",
      "it: 31, validation loss=0.7759479880332947, c=0.6957895715215169, ibs=0.14830420775950667, ibnll=0.4486134181253622\n",
      "EarlyStopping counter: 1 out of 20\n",
      "it: 32, validation loss=0.7761659622192383, c=0.69563466245314, ibs=0.14726003008064695, ibnll=0.4458821400496258\n",
      "EarlyStopping counter: 2 out of 20\n",
      "it: 33, validation loss=0.7788930535316467, c=0.69563466245314, ibs=0.14879514180361217, ibnll=0.44971425608588494\n",
      "EarlyStopping counter: 3 out of 20\n",
      "it: 34, validation loss=0.781093180179596, c=0.6957276078941661, ibs=0.15019121267013116, ibnll=0.453428287654935\n",
      "EarlyStopping counter: 4 out of 20\n",
      "it: 35, validation loss=0.7856882810592651, c=0.6945193171608266, ibs=0.15072507568238347, ibnll=0.4549572705414482\n",
      "EarlyStopping counter: 5 out of 20\n",
      "it: 36, validation loss=0.7882774472236633, c=0.6932800446138117, ibs=0.14998486823965848, ibnll=0.45328731975666064\n",
      "EarlyStopping counter: 6 out of 20\n",
      "it: 37, validation loss=0.7843157649040222, c=0.6941165535830468, ibs=0.1488302451146616, ibnll=0.45059515397355704\n",
      "EarlyStopping counter: 7 out of 20\n",
      "it: 38, validation loss=0.7867279052734375, c=0.6943953899061251, ibs=0.1494432978693145, ibnll=0.4524634042597479\n",
      "EarlyStopping counter: 8 out of 20\n",
      "it: 39, validation loss=0.78531414270401, c=0.6951079716206586, ibs=0.15105610199645295, ibnll=0.45686966300966314\n",
      "EarlyStopping counter: 9 out of 20\n",
      "it: 40, validation loss=0.7896156311035156, c=0.6956966260804908, ibs=0.15063768155979704, ibnll=0.4559535736978504\n",
      "EarlyStopping counter: 10 out of 20\n",
      "it: 41, validation loss=0.7857882380485535, c=0.6974006258326363, ibs=0.15136335540683318, ibnll=0.45787674361745617\n",
      "EarlyStopping counter: 11 out of 20\n",
      "it: 42, validation loss=0.7851154208183289, c=0.6974935712736624, ibs=0.1510240037187428, ibnll=0.45711742768355634\n",
      "EarlyStopping counter: 12 out of 20\n",
      "it: 43, validation loss=0.7906997799873352, c=0.6949220807386064, ibs=0.14967154949627687, ibnll=0.4536993395540332\n",
      "EarlyStopping counter: 13 out of 20\n",
      "it: 44, validation loss=0.7905535697937012, c=0.6938686990736438, ibs=0.15050156879955165, ibnll=0.4560826345384405\n",
      "EarlyStopping counter: 14 out of 20\n",
      "it: 45, validation loss=0.7999827265739441, c=0.6905226631967035, ibs=0.15194893679468527, ibnll=0.4598197591775304\n",
      "EarlyStopping counter: 15 out of 20\n",
      "it: 46, validation loss=0.799243152141571, c=0.6909873904018341, ibs=0.15230699521501026, ibnll=0.46087314872000174\n",
      "EarlyStopping counter: 16 out of 20\n",
      "it: 47, validation loss=0.7943019866943359, c=0.6919168448120953, ibs=0.15239064482049408, ibnll=0.4612384101329574\n",
      "EarlyStopping counter: 17 out of 20\n",
      "it: 48, validation loss=0.8014186024665833, c=0.6920717538804722, ibs=0.15282415587006168, ibnll=0.4627020763052255\n",
      "EarlyStopping counter: 18 out of 20\n",
      "it: 49, validation loss=0.8057621121406555, c=0.6929392446633826, ibs=0.15530194767633018, ibnll=0.47008294862230526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 19 out of 20\n",
      "it: 50, validation loss=0.8102954626083374, c=0.6921337175078229, ibs=0.15710639792846828, ibnll=0.47550219767116625\n",
      "EarlyStopping counter: 20 out of 20\n",
      "it: 51, validation loss=0.8004456162452698, c=0.6918238993710691, ibs=0.15013440134176134, ibnll=0.45619407074613494\n",
      "Early stopping\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b2fab7fe7b54f22877898ac9972ccaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (inf --> 0.969190).  Saving model ...\n",
      "it: 0, validation loss=0.9691903591156006, c=0.6380169867060561, ibs=0.19695217075825364, ibnll=0.5743237405096879\n",
      "Validation loss decreased (0.969190 --> 0.951664).  Saving model ...\n",
      "it: 1, validation loss=0.9516641497612, c=0.6406326932545544, ibs=0.19180906068787804, ibnll=0.5619475653307368\n",
      "Validation loss decreased (0.951664 --> 0.938198).  Saving model ...\n",
      "it: 2, validation loss=0.938197910785675, c=0.6402941900541606, ibs=0.1880617959061868, ibnll=0.5531715458664861\n",
      "Validation loss decreased (0.938198 --> 0.925869).  Saving model ...\n",
      "it: 3, validation loss=0.9258686304092407, c=0.6354012801575578, ibs=0.18510361817857857, ibnll=0.5461399774097812\n",
      "Validation loss decreased (0.925869 --> 0.914031).  Saving model ...\n",
      "it: 4, validation loss=0.9140310883522034, c=0.6324778434268833, ibs=0.18269263148253603, ibnll=0.540119031461439\n",
      "Validation loss decreased (0.914031 --> 0.903365).  Saving model ...\n",
      "it: 5, validation loss=0.9033645391464233, c=0.6312161496799606, ibs=0.18070438294865787, ibnll=0.5348883987608543\n",
      "Validation loss decreased (0.903365 --> 0.895286).  Saving model ...\n",
      "it: 6, validation loss=0.8952862620353699, c=0.6316469719350074, ibs=0.1793662298675018, ibnll=0.5310695537593865\n",
      "Validation loss decreased (0.895286 --> 0.889145).  Saving model ...\n",
      "it: 7, validation loss=0.8891454339027405, c=0.6350935499753816, ibs=0.17834483559222744, ibnll=0.5277970215393452\n",
      "Validation loss decreased (0.889145 --> 0.884927).  Saving model ...\n",
      "it: 8, validation loss=0.8849265575408936, c=0.6400787789266371, ibs=0.17758754013357134, ibnll=0.5251753995448017\n",
      "Validation loss decreased (0.884927 --> 0.880540).  Saving model ...\n",
      "it: 9, validation loss=0.8805398941040039, c=0.6461410635155096, ibs=0.17662415386794728, ibnll=0.5224466294986033\n",
      "Validation loss decreased (0.880540 --> 0.874545).  Saving model ...\n",
      "it: 10, validation loss=0.8745453357696533, c=0.6541420482520925, ibs=0.1753023115055243, ibnll=0.519217381055974\n",
      "Validation loss decreased (0.874545 --> 0.869027).  Saving model ...\n",
      "it: 11, validation loss=0.8690274357795715, c=0.6610659773510585, ibs=0.1743022008099271, ibnll=0.516761060668072\n",
      "Validation loss decreased (0.869027 --> 0.864148).  Saving model ...\n",
      "it: 12, validation loss=0.8641476035118103, c=0.6672821270310192, ibs=0.173340376578063, ibnll=0.51431621361082\n",
      "Validation loss decreased (0.864148 --> 0.858638).  Saving model ...\n",
      "it: 13, validation loss=0.8586384057998657, c=0.6724827671097982, ibs=0.17193338207524542, ibnll=0.5105077019657622\n",
      "Validation loss decreased (0.858638 --> 0.853508).  Saving model ...\n",
      "it: 14, validation loss=0.8535082340240479, c=0.6773756770064008, ibs=0.17104812472863581, ibnll=0.5082079072544983\n",
      "Validation loss decreased (0.853508 --> 0.850203).  Saving model ...\n",
      "it: 15, validation loss=0.8502026200294495, c=0.6789758739537174, ibs=0.17082623152417434, ibnll=0.5076937824278751\n",
      "Validation loss decreased (0.850203 --> 0.849386).  Saving model ...\n",
      "it: 16, validation loss=0.8493858575820923, c=0.68023756770064, ibs=0.17124522170479886, ibnll=0.50898296706999\n",
      "Validation loss decreased (0.849386 --> 0.843779).  Saving model ...\n",
      "it: 17, validation loss=0.8437787890434265, c=0.6825763170851797, ibs=0.16971895479950536, ibnll=0.5044907274202138\n",
      "Validation loss decreased (0.843779 --> 0.842043).  Saving model ...\n",
      "it: 18, validation loss=0.8420429825782776, c=0.684207287050714, ibs=0.1697608419173169, ibnll=0.5045156773912457\n",
      "Validation loss decreased (0.842043 --> 0.841324).  Saving model ...\n",
      "it: 19, validation loss=0.8413241505622864, c=0.6847304283604135, ibs=0.17010657351306133, ibnll=0.505800916419629\n",
      "EarlyStopping counter: 1 out of 20\n",
      "it: 20, validation loss=0.843168318271637, c=0.6846381093057607, ibs=0.1709188662274719, ibnll=0.5080950514116102\n",
      "EarlyStopping counter: 2 out of 20\n",
      "it: 21, validation loss=0.8448587656021118, c=0.6845150172328902, ibs=0.17160439080378742, ibnll=0.5102944562607086\n",
      "Validation loss decreased (0.841324 --> 0.841166).  Saving model ...\n",
      "it: 22, validation loss=0.8411661982536316, c=0.6847304283604135, ibs=0.17028456419917792, ibnll=0.5061534992496936\n",
      "EarlyStopping counter: 1 out of 20\n",
      "it: 23, validation loss=0.8416478037834167, c=0.6862075332348597, ibs=0.17125279292150988, ibnll=0.5084658407854189\n",
      "EarlyStopping counter: 2 out of 20\n",
      "it: 24, validation loss=0.8412662744522095, c=0.6872230428360414, ibs=0.1716611700993417, ibnll=0.5098644867488754\n",
      "Validation loss decreased (0.841166 --> 0.840143).  Saving model ...\n",
      "it: 25, validation loss=0.8401430249214172, c=0.6882385524372231, ibs=0.17149363609317245, ibnll=0.509485480418806\n",
      "Validation loss decreased (0.840143 --> 0.835515).  Saving model ...\n",
      "it: 26, validation loss=0.8355149626731873, c=0.6872845888724766, ibs=0.16993541001297915, ibnll=0.5044316780170527\n",
      "Validation loss decreased (0.835515 --> 0.833895).  Saving model ...\n",
      "it: 27, validation loss=0.8338954448699951, c=0.6877461841457411, ibs=0.16955660125586858, ibnll=0.5039338092245181\n",
      "EarlyStopping counter: 1 out of 20\n",
      "it: 28, validation loss=0.8353846073150635, c=0.687992368291482, ibs=0.16997718359074754, ibnll=0.505473122433042\n",
      "EarlyStopping counter: 2 out of 20\n",
      "it: 29, validation loss=0.8351402282714844, c=0.6869768586903003, ibs=0.16968192936603807, ibnll=0.5038061475139021\n",
      "EarlyStopping counter: 3 out of 20\n",
      "it: 30, validation loss=0.8382664322853088, c=0.6894387001477105, ibs=0.171146883840267, ibnll=0.5078081790556681\n",
      "EarlyStopping counter: 4 out of 20\n",
      "it: 31, validation loss=0.8387836217880249, c=0.6893156080748399, ibs=0.1714977325675923, ibnll=0.5088695320895987\n",
      "EarlyStopping counter: 5 out of 20\n",
      "it: 32, validation loss=0.835374116897583, c=0.6888540128015755, ibs=0.17086349961535371, ibnll=0.505776997452557\n",
      "Validation loss decreased (0.833895 --> 0.833009).  Saving model ...\n",
      "it: 33, validation loss=0.8330087661743164, c=0.6897772033481043, ibs=0.17063971180198714, ibnll=0.5063238617747361\n",
      "EarlyStopping counter: 1 out of 20\n",
      "it: 34, validation loss=0.8345931172370911, c=0.6877461841457411, ibs=0.1714822178808532, ibnll=0.5082416949244716\n",
      "EarlyStopping counter: 2 out of 20\n",
      "it: 35, validation loss=0.8382782340049744, c=0.6883924175283112, ibs=0.17202860745763546, ibnll=0.5106749972668553\n",
      "EarlyStopping counter: 3 out of 20\n",
      "it: 36, validation loss=0.84469074010849, c=0.6894694731659281, ibs=0.17305363643591185, ibnll=0.5144325373053668\n",
      "EarlyStopping counter: 4 out of 20\n",
      "it: 37, validation loss=0.8489507436752319, c=0.6875, ibs=0.17415894417066294, ibnll=0.5151917776219305\n",
      "EarlyStopping counter: 5 out of 20\n",
      "it: 38, validation loss=0.8536615371704102, c=0.6882693254554406, ibs=0.1753923059857929, ibnll=0.5209837449715764\n",
      "EarlyStopping counter: 6 out of 20\n",
      "it: 39, validation loss=0.8508290648460388, c=0.6887616937469226, ibs=0.17485525071739816, ibnll=0.519281936836454\n",
      "EarlyStopping counter: 7 out of 20\n",
      "it: 40, validation loss=0.8472740054130554, c=0.6900849335302807, ibs=0.17395837512353982, ibnll=0.5154621018680959\n",
      "EarlyStopping counter: 8 out of 20\n",
      "it: 41, validation loss=0.8461247682571411, c=0.6924236829148203, ibs=0.17354639157489604, ibnll=0.5151181616079162\n",
      "EarlyStopping counter: 9 out of 20\n",
      "it: 42, validation loss=0.8426793813705444, c=0.6925467749876908, ibs=0.17249354849260445, ibnll=0.5110917193554029\n",
      "EarlyStopping counter: 10 out of 20\n",
      "it: 43, validation loss=0.8389639258384705, c=0.6924236829148203, ibs=0.1712531482549383, ibnll=0.5067341820972432\n",
      "EarlyStopping counter: 11 out of 20\n",
      "it: 44, validation loss=0.8486707210540771, c=0.6923313638601674, ibs=0.1733866958993782, ibnll=0.5154452538921949\n",
      "EarlyStopping counter: 12 out of 20\n",
      "it: 45, validation loss=0.8494868278503418, c=0.6892540620384048, ibs=0.174404063648512, ibnll=0.5148005025208993\n",
      "EarlyStopping counter: 13 out of 20\n",
      "it: 46, validation loss=0.8506022095680237, c=0.6887924667651403, ibs=0.17507642028858036, ibnll=0.5179312163142583\n",
      "EarlyStopping counter: 14 out of 20\n",
      "it: 47, validation loss=0.8478685617446899, c=0.6916543574593796, ibs=0.17409975239341782, ibnll=0.5166062361365086\n",
      "EarlyStopping counter: 15 out of 20\n",
      "it: 48, validation loss=0.8508247137069702, c=0.6920851797144264, ibs=0.17467964170841488, ibnll=0.5152750737734338\n",
      "EarlyStopping counter: 16 out of 20\n",
      "it: 49, validation loss=0.8527624011039734, c=0.6925160019694732, ibs=0.17297158415075617, ibnll=0.5125491493092326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 17 out of 20\n",
      "it: 50, validation loss=0.8534154295921326, c=0.6904849827671098, ibs=0.17198598286196362, ibnll=0.5093434665665905\n",
      "EarlyStopping counter: 18 out of 20\n",
      "it: 51, validation loss=0.8541889786720276, c=0.6878692762186115, ibs=0.17345559406606692, ibnll=0.5124162198150235\n",
      "EarlyStopping counter: 19 out of 20\n",
      "it: 52, validation loss=0.8682430386543274, c=0.6866999015263417, ibs=0.17774390440251786, ibnll=0.527768964546573\n",
      "EarlyStopping counter: 20 out of 20\n",
      "it: 53, validation loss=0.8698647618293762, c=0.6857459379615952, ibs=0.17917397360328216, ibnll=0.5279673268337748\n",
      "Early stopping\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9922e003090e487b9cf0e1f8a74c5d01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (inf --> 0.925974).  Saving model ...\n",
      "it: 0, validation loss=0.9259738922119141, c=0.6282823867361913, ibs=0.16744495257328829, ibnll=0.505397543270754\n",
      "Validation loss decreased (0.925974 --> 0.913842).  Saving model ...\n",
      "it: 1, validation loss=0.9138419032096863, c=0.6349642489149779, ibs=0.1656934450120007, ibnll=0.5026219763640555\n",
      "Validation loss decreased (0.913842 --> 0.902492).  Saving model ...\n",
      "it: 2, validation loss=0.9024922847747803, c=0.6380866144190839, ibs=0.16407102801977297, ibnll=0.49945339384006915\n",
      "Validation loss decreased (0.902492 --> 0.888333).  Saving model ...\n",
      "it: 3, validation loss=0.8883326649665833, c=0.6362444187716614, ibs=0.1612390228089666, ibnll=0.4925897473305442\n",
      "Validation loss decreased (0.888333 --> 0.872614).  Saving model ...\n",
      "it: 4, validation loss=0.8726138472557068, c=0.6355262747057171, ibs=0.1577597809511768, ibnll=0.48357102070836033\n",
      "Validation loss decreased (0.872614 --> 0.857884).  Saving model ...\n",
      "it: 5, validation loss=0.8578841090202332, c=0.6349018016048958, ibs=0.15517250353601178, ibnll=0.4763668649848858\n",
      "Validation loss decreased (0.857884 --> 0.847023).  Saving model ...\n",
      "it: 6, validation loss=0.8470227122306824, c=0.6381802853842071, ibs=0.15514893169213834, ibnll=0.475085734740016\n",
      "Validation loss decreased (0.847023 --> 0.841084).  Saving model ...\n",
      "it: 7, validation loss=0.8410841226577759, c=0.6417085584038468, ibs=0.15697118034799348, ibnll=0.47783122509059966\n",
      "Validation loss decreased (0.841084 --> 0.835558).  Saving model ...\n",
      "it: 8, validation loss=0.8355578184127808, c=0.6443313454272958, ibs=0.15801805473088035, ibnll=0.47927157245017266\n",
      "Validation loss decreased (0.835558 --> 0.828233).  Saving model ...\n",
      "it: 9, validation loss=0.8282334208488464, c=0.6510132076060824, ibs=0.15682867321528363, ibnll=0.4763159031842758\n",
      "Validation loss decreased (0.828233 --> 0.821910).  Saving model ...\n",
      "it: 10, validation loss=0.8219099044799805, c=0.6566646891685141, ibs=0.1555041123499582, ibnll=0.47299750996711937\n",
      "Validation loss decreased (0.821910 --> 0.815607).  Saving model ...\n",
      "it: 11, validation loss=0.8156066536903381, c=0.6610672245293034, ibs=0.15206592122907694, ibnll=0.4652407227594407\n",
      "Validation loss decreased (0.815607 --> 0.811777).  Saving model ...\n",
      "it: 12, validation loss=0.8117766976356506, c=0.6673119555375152, ibs=0.1498460083733464, ibnll=0.4597952027884124\n",
      "Validation loss decreased (0.811777 --> 0.809346).  Saving model ...\n",
      "it: 13, validation loss=0.8093459010124207, c=0.6706216629718675, ibs=0.1486538723116763, ibnll=0.4566745800261991\n",
      "Validation loss decreased (0.809346 --> 0.807591).  Saving model ...\n",
      "it: 14, validation loss=0.8075910210609436, c=0.6743997252318357, ibs=0.14815760681475484, ibnll=0.45525840694025116\n",
      "Validation loss decreased (0.807591 --> 0.803979).  Saving model ...\n",
      "it: 15, validation loss=0.8039788603782654, c=0.6752739875729853, ibs=0.14735187072372352, ibnll=0.45330520134259866\n",
      "Validation loss decreased (0.803979 --> 0.800819).  Saving model ...\n",
      "it: 16, validation loss=0.8008189797401428, c=0.6761170262590939, ibs=0.14691535445405826, ibnll=0.4521749768169763\n",
      "Validation loss decreased (0.800819 --> 0.798329).  Saving model ...\n",
      "it: 17, validation loss=0.7983291149139404, c=0.6754613295032317, ibs=0.1470117237669443, ibnll=0.4526549414084215\n",
      "Validation loss decreased (0.798329 --> 0.798199).  Saving model ...\n",
      "it: 18, validation loss=0.7981987595558167, c=0.6762106972242171, ibs=0.14830595534395366, ibnll=0.455341648594423\n",
      "Validation loss decreased (0.798199 --> 0.796306).  Saving model ...\n",
      "it: 19, validation loss=0.796306312084198, c=0.6780528928716395, ibs=0.1477812045204355, ibnll=0.4539816790869909\n",
      "Validation loss decreased (0.796306 --> 0.794431).  Saving model ...\n",
      "it: 20, validation loss=0.7944308519363403, c=0.6778967745964343, ibs=0.14657952397010565, ibnll=0.4512646275728587\n",
      "Validation loss decreased (0.794431 --> 0.793146).  Saving model ...\n",
      "it: 21, validation loss=0.7931457161903381, c=0.6779904455615574, ibs=0.1459838460371161, ibnll=0.4499248122937612\n",
      "Validation loss decreased (0.793146 --> 0.791495).  Saving model ...\n",
      "it: 22, validation loss=0.7914950847625732, c=0.6792393917631998, ibs=0.14462903296323673, ibnll=0.44688970093983243\n",
      "Validation loss decreased (0.791495 --> 0.791076).  Saving model ...\n",
      "it: 23, validation loss=0.7910764813423157, c=0.679364286383364, ibs=0.14450479491713966, ibnll=0.44692576632518183\n",
      "Validation loss decreased (0.791076 --> 0.790796).  Saving model ...\n",
      "it: 24, validation loss=0.7907959818840027, c=0.6800512067942673, ibs=0.14504501301542636, ibnll=0.44854440394438283\n",
      "EarlyStopping counter: 1 out of 20\n",
      "it: 25, validation loss=0.7922145128250122, c=0.6803634433446779, ibs=0.1466732725590219, ibnll=0.45205969803623686\n",
      "EarlyStopping counter: 2 out of 20\n",
      "it: 26, validation loss=0.7922315001487732, c=0.68042589065476, ibs=0.14726907796525074, ibnll=0.4532997243102073\n",
      "Validation loss decreased (0.790796 --> 0.788425).  Saving model ...\n",
      "it: 27, validation loss=0.7884253859519958, c=0.681456271271115, ibs=0.14538158710625196, ibnll=0.4498919719003783\n",
      "Validation loss decreased (0.788425 --> 0.787758).  Saving model ...\n",
      "it: 28, validation loss=0.7877577543258667, c=0.6813313766509508, ibs=0.14504536822097347, ibnll=0.44944662934630025\n",
      "EarlyStopping counter: 1 out of 20\n",
      "it: 29, validation loss=0.7877775430679321, c=0.6801448777593905, ibs=0.14509144925936007, ibnll=0.44877866463027377\n",
      "EarlyStopping counter: 2 out of 20\n",
      "it: 30, validation loss=0.7887677550315857, c=0.6789896025228713, ibs=0.14555533014410948, ibnll=0.44975134803024447\n",
      "EarlyStopping counter: 3 out of 20\n",
      "it: 31, validation loss=0.7880500555038452, c=0.6807069035501295, ibs=0.14391021891862224, ibnll=0.44745624775106846\n",
      "EarlyStopping counter: 4 out of 20\n",
      "it: 32, validation loss=0.7896841764450073, c=0.6808317981702938, ibs=0.1463572269062873, ibnll=0.45149642582004584\n",
      "Validation loss decreased (0.787758 --> 0.787748).  Saving model ...\n",
      "it: 33, validation loss=0.7877481579780579, c=0.6825803228525932, ibs=0.1447279045463706, ibnll=0.4498011361368089\n",
      "EarlyStopping counter: 1 out of 20\n",
      "it: 34, validation loss=0.7908452749252319, c=0.6840478346395229, ibs=0.14629464049938304, ibnll=0.45489640199767134\n",
      "EarlyStopping counter: 2 out of 20\n",
      "it: 35, validation loss=0.7950344085693359, c=0.6827364411277984, ibs=0.1495335938883223, ibnll=0.4603209183430648\n",
      "EarlyStopping counter: 3 out of 20\n",
      "it: 36, validation loss=0.7953108549118042, c=0.6851094389109189, ibs=0.14540722645782198, ibnll=0.4546226429301167\n",
      "EarlyStopping counter: 4 out of 20\n",
      "it: 37, validation loss=0.7942776679992676, c=0.6782402348018859, ibs=0.14811532346109565, ibnll=0.45623199565295763\n",
      "EarlyStopping counter: 5 out of 20\n",
      "it: 38, validation loss=0.7931658029556274, c=0.6763980391544634, ibs=0.14624238357560856, ibnll=0.4525459504460212\n",
      "EarlyStopping counter: 6 out of 20\n",
      "it: 39, validation loss=0.7953619360923767, c=0.681456271271115, ibs=0.1455501603449972, ibnll=0.4538708352432148\n",
      "EarlyStopping counter: 7 out of 20\n",
      "it: 40, validation loss=0.7958738207817078, c=0.6811440347207044, ibs=0.1481262004831884, ibnll=0.456812332150236\n",
      "EarlyStopping counter: 8 out of 20\n",
      "it: 41, validation loss=0.7949411273002625, c=0.6848596496705904, ibs=0.1472865724851822, ibnll=0.4566556560466004\n",
      "EarlyStopping counter: 9 out of 20\n",
      "it: 42, validation loss=0.7975560426712036, c=0.6859837012520685, ibs=0.14937567985419234, ibnll=0.4617577656253357\n",
      "EarlyStopping counter: 10 out of 20\n",
      "it: 43, validation loss=0.7983803153038025, c=0.6864208324226434, ibs=0.14862362770519552, ibnll=0.4617804061035858\n",
      "EarlyStopping counter: 11 out of 20\n",
      "it: 44, validation loss=0.7961086630821228, c=0.6825803228525932, ibs=0.14969889579259557, ibnll=0.4609965254779626\n",
      "EarlyStopping counter: 12 out of 20\n",
      "it: 45, validation loss=0.7982621788978577, c=0.6837043744340713, ibs=0.14712656036135424, ibnll=0.4583490608988147\n",
      "EarlyStopping counter: 13 out of 20\n",
      "it: 46, validation loss=0.8002173900604248, c=0.6823617572673057, ibs=0.14973177038393268, ibnll=0.460843213675398\n",
      "EarlyStopping counter: 14 out of 20\n",
      "it: 47, validation loss=0.80550217628479, c=0.6817997314765667, ibs=0.15234541036320207, ibnll=0.46542730181031094\n",
      "EarlyStopping counter: 15 out of 20\n",
      "it: 48, validation loss=0.8101363182067871, c=0.6817997314765667, ibs=0.15294407604685592, ibnll=0.46956419192932947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 16 out of 20\n",
      "it: 49, validation loss=0.8077951669692993, c=0.6795516283136104, ibs=0.15263318620534955, ibnll=0.46805898388930967\n",
      "EarlyStopping counter: 17 out of 20\n",
      "it: 50, validation loss=0.8085388541221619, c=0.6780528928716395, ibs=0.15282452749143882, ibnll=0.47017549135525577\n",
      "EarlyStopping counter: 18 out of 20\n",
      "it: 51, validation loss=0.8105988502502441, c=0.6769288412901614, ibs=0.15177586051466127, ibnll=0.4698440423952807\n",
      "EarlyStopping counter: 19 out of 20\n",
      "it: 52, validation loss=0.8149831891059875, c=0.6768351703250383, ibs=0.15557607267929938, ibnll=0.47574658550635157\n",
      "EarlyStopping counter: 20 out of 20\n",
      "it: 53, validation loss=0.8202458620071411, c=0.6808942454803759, ibs=0.15694789578665178, ibnll=0.47847663070005925\n",
      "Early stopping\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3be0448f282948c48b7c7e9f036b3e41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (inf --> 0.883227).  Saving model ...\n",
      "it: 0, validation loss=0.8832269906997681, c=0.61139815249726, ibs=0.17484855283106804, ibnll=0.5226598268959193\n",
      "Validation loss decreased (0.883227 --> 0.874571).  Saving model ...\n",
      "it: 1, validation loss=0.8745711445808411, c=0.6167840926882731, ibs=0.1730970687156704, ibnll=0.5190862526259866\n",
      "Validation loss decreased (0.874571 --> 0.866605).  Saving model ...\n",
      "it: 2, validation loss=0.8666046857833862, c=0.6199154532644434, ibs=0.17134838994084223, ibnll=0.5151493836007667\n",
      "Validation loss decreased (0.866605 --> 0.856726).  Saving model ...\n",
      "it: 3, validation loss=0.8567256927490234, c=0.6223266009080946, ibs=0.168623848345707, ibnll=0.5083971347664455\n",
      "Validation loss decreased (0.856726 --> 0.844075).  Saving model ...\n",
      "it: 4, validation loss=0.8440749645233154, c=0.6246438077344606, ibs=0.16475663342694616, ibnll=0.49825757935017645\n",
      "Validation loss decreased (0.844075 --> 0.833154).  Saving model ...\n",
      "it: 5, validation loss=0.833154022693634, c=0.6254579614842649, ibs=0.1616313207102584, ibnll=0.48942904122756103\n",
      "Validation loss decreased (0.833154 --> 0.827483).  Saving model ...\n",
      "it: 6, validation loss=0.8274832963943481, c=0.6286832628777204, ibs=0.1606924049515179, ibnll=0.4856137328648757\n",
      "EarlyStopping counter: 1 out of 20\n",
      "it: 7, validation loss=0.8290462493896484, c=0.6320025050884609, ibs=0.16286480655805055, ibnll=0.4891687409007235\n",
      "EarlyStopping counter: 2 out of 20\n",
      "it: 8, validation loss=0.828243613243103, c=0.63400657585721, ibs=0.16355427879095394, ibnll=0.4893735336593621\n",
      "Validation loss decreased (0.827483 --> 0.826714).  Saving model ...\n",
      "it: 9, validation loss=0.8267143368721008, c=0.6369813684045718, ibs=0.16375237437780546, ibnll=0.4888097383239627\n",
      "Validation loss decreased (0.826714 --> 0.818472).  Saving model ...\n",
      "it: 10, validation loss=0.8184718489646912, c=0.640018788163457, ibs=0.16086600446740582, ibnll=0.4814260489987294\n",
      "Validation loss decreased (0.818472 --> 0.810863).  Saving model ...\n",
      "it: 11, validation loss=0.8108631372451782, c=0.6441208705182402, ibs=0.15863297638097298, ibnll=0.47571879789088417\n",
      "Validation loss decreased (0.810863 --> 0.807386).  Saving model ...\n",
      "it: 12, validation loss=0.8073856234550476, c=0.6489744794113043, ibs=0.15795783296087065, ibnll=0.47391586883248243\n",
      "Validation loss decreased (0.807386 --> 0.806979).  Saving model ...\n",
      "it: 13, validation loss=0.8069792985916138, c=0.6496633787380617, ibs=0.15826811314825978, ibnll=0.47438107059151086\n",
      "Validation loss decreased (0.806979 --> 0.806080).  Saving model ...\n",
      "it: 14, validation loss=0.806080162525177, c=0.6507906685454831, ibs=0.15843068725272913, ibnll=0.4741913839134467\n",
      "Validation loss decreased (0.806080 --> 0.798488).  Saving model ...\n",
      "it: 15, validation loss=0.7984879016876221, c=0.6516987631125725, ibs=0.15642593674571456, ibnll=0.46845112862980415\n",
      "Validation loss decreased (0.798488 --> 0.797567).  Saving model ...\n",
      "it: 16, validation loss=0.7975668907165527, c=0.6525442304681384, ibs=0.15671615747079198, ibnll=0.46863903713508853\n",
      "EarlyStopping counter: 1 out of 20\n",
      "it: 17, validation loss=0.8062009811401367, c=0.6523876624393299, ibs=0.1598947973903797, ibnll=0.47613532412500503\n",
      "EarlyStopping counter: 2 out of 20\n",
      "it: 18, validation loss=0.799080491065979, c=0.6548927509002662, ibs=0.15863302556598538, ibnll=0.47248394093134394\n",
      "Validation loss decreased (0.797567 --> 0.797543).  Saving model ...\n",
      "it: 19, validation loss=0.7975427508354187, c=0.6557695318615938, ibs=0.15938255533611728, ibnll=0.47401949388882975\n",
      "EarlyStopping counter: 1 out of 20\n",
      "it: 20, validation loss=0.8061859011650085, c=0.656176608736496, ibs=0.1625946551510403, ibnll=0.4815489649024616\n",
      "EarlyStopping counter: 2 out of 20\n",
      "it: 21, validation loss=0.798376202583313, c=0.6558321590731173, ibs=0.16076148967850157, ibnll=0.47663920765348466\n",
      "Validation loss decreased (0.797543 --> 0.794704).  Saving model ...\n",
      "it: 22, validation loss=0.7947040796279907, c=0.6560826679192109, ibs=0.1606771319955629, ibnll=0.4763050961231354\n",
      "EarlyStopping counter: 1 out of 20\n",
      "it: 23, validation loss=0.8000593185424805, c=0.6558321590731173, ibs=0.1635020570998478, ibnll=0.4830566890280364\n",
      "EarlyStopping counter: 2 out of 20\n",
      "it: 24, validation loss=0.7956506013870239, c=0.6569281352747769, ibs=0.16279363339004743, ibnll=0.48126263776122963\n",
      "EarlyStopping counter: 3 out of 20\n",
      "it: 25, validation loss=0.7976096868515015, c=0.6568968216690152, ibs=0.16393210379010478, ibnll=0.48402431712906274\n",
      "Validation loss decreased (0.794704 --> 0.794209).  Saving model ...\n",
      "it: 26, validation loss=0.7942091226577759, c=0.6576483482072961, ibs=0.16317306251626215, ibnll=0.48203563215115003\n",
      "Validation loss decreased (0.794209 --> 0.794050).  Saving model ...\n",
      "it: 27, validation loss=0.7940502166748047, c=0.6577736026303429, ibs=0.1633528038231897, ibnll=0.48224762222363937\n",
      "EarlyStopping counter: 1 out of 20\n",
      "it: 28, validation loss=0.806562602519989, c=0.6566776264286832, ibs=0.1676683067383232, ibnll=0.49246056373311714\n",
      "EarlyStopping counter: 2 out of 20\n",
      "it: 29, validation loss=0.8096532821655273, c=0.6554877094097386, ibs=0.16840933451252008, ibnll=0.49440009448183764\n",
      "EarlyStopping counter: 3 out of 20\n",
      "it: 30, validation loss=0.8052268624305725, c=0.6555190230155002, ibs=0.16757231730549424, ibnll=0.4928512318625684\n",
      "EarlyStopping counter: 4 out of 20\n",
      "it: 31, validation loss=0.8089083433151245, c=0.6558947862846407, ibs=0.1699755931676574, ibnll=0.4985077953546618\n",
      "EarlyStopping counter: 5 out of 20\n",
      "it: 32, validation loss=0.8075467348098755, c=0.6566776264286832, ibs=0.17104466847226305, ibnll=0.5008635668778245\n",
      "Validation loss decreased (0.794050 --> 0.793301).  Saving model ...\n",
      "it: 33, validation loss=0.7933009266853333, c=0.6578049162361046, ibs=0.16653783035353006, ibnll=0.4912818397382044\n",
      "EarlyStopping counter: 1 out of 20\n",
      "it: 34, validation loss=0.8040041327476501, c=0.655644277438547, ibs=0.17005748088693382, ibnll=0.4986460900711987\n",
      "EarlyStopping counter: 2 out of 20\n",
      "it: 35, validation loss=0.8114209175109863, c=0.6557695318615938, ibs=0.17216487385603726, ibnll=0.5037930847996502\n",
      "EarlyStopping counter: 3 out of 20\n",
      "it: 36, validation loss=0.8107935190200806, c=0.6551119461405981, ibs=0.17079423151014342, ibnll=0.5016332046026514\n",
      "EarlyStopping counter: 4 out of 20\n",
      "it: 37, validation loss=0.8262553215026855, c=0.6523563488335682, ibs=0.17567511273856382, ibnll=0.5137686624996359\n",
      "EarlyStopping counter: 5 out of 20\n",
      "it: 38, validation loss=0.8147354125976562, c=0.6533583842179427, ibs=0.1725410070977416, ibnll=0.5077866865037202\n",
      "EarlyStopping counter: 6 out of 20\n",
      "it: 39, validation loss=0.8150852918624878, c=0.6526068576796619, ibs=0.17418155977999342, ibnll=0.5117721993778949\n",
      "EarlyStopping counter: 7 out of 20\n",
      "it: 40, validation loss=0.8162261247634888, c=0.6542038515735087, ibs=0.1753426015718248, ibnll=0.5150088205363667\n",
      "EarlyStopping counter: 8 out of 20\n",
      "it: 41, validation loss=0.8195080757141113, c=0.6540159699389385, ibs=0.17578930843421522, ibnll=0.5162816260794415\n",
      "EarlyStopping counter: 9 out of 20\n",
      "it: 42, validation loss=0.8281903266906738, c=0.6533583842179427, ibs=0.17821874349585898, ibnll=0.5223024175741589\n",
      "EarlyStopping counter: 10 out of 20\n",
      "it: 43, validation loss=0.8197673559188843, c=0.6545169876311258, ibs=0.17494564014780176, ibnll=0.5167519218426297\n",
      "EarlyStopping counter: 11 out of 20\n",
      "it: 44, validation loss=0.833189070224762, c=0.6523250352278065, ibs=0.18065093378786232, ibnll=0.530515885512752\n",
      "EarlyStopping counter: 12 out of 20\n",
      "it: 45, validation loss=0.8240052461624146, c=0.651855331141381, ibs=0.17731995487937788, ibnll=0.5243299887648035\n",
      "EarlyStopping counter: 13 out of 20\n",
      "it: 46, validation loss=0.8285602331161499, c=0.6497573195553468, ibs=0.1781957622815943, ibnll=0.5244172122441828\n",
      "EarlyStopping counter: 14 out of 20\n",
      "it: 47, validation loss=0.8307910561561584, c=0.6500391420072021, ibs=0.1780340544140482, ibnll=0.5237682684820316\n",
      "EarlyStopping counter: 15 out of 20\n",
      "it: 48, validation loss=0.843635618686676, c=0.6478471896038829, ibs=0.18341795842957268, ibnll=0.5383592103501037\n",
      "EarlyStopping counter: 16 out of 20\n",
      "it: 49, validation loss=0.8358929753303528, c=0.6496320651323, ibs=0.1795980101656475, ibnll=0.5318151583760543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 17 out of 20\n",
      "it: 50, validation loss=0.8597412109375, c=0.6462188821042744, ibs=0.18639451274299226, ibnll=0.544071290281756\n",
      "EarlyStopping counter: 18 out of 20\n",
      "it: 51, validation loss=0.8254054188728333, c=0.648254266478785, ibs=0.17547225914576578, ibnll=0.5196281598252479\n",
      "EarlyStopping counter: 19 out of 20\n",
      "it: 52, validation loss=0.8574323654174805, c=0.6446532018161891, ibs=0.18644548010390183, ibnll=0.5431163717446457\n",
      "EarlyStopping counter: 20 out of 20\n",
      "it: 53, validation loss=0.8328456282615662, c=0.6471269766713638, ibs=0.17899096251831356, ibnll=0.5258723474377706\n",
      "Early stopping\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73abc43cf2e143febf4e4e46424af98c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (inf --> 0.920997).  Saving model ...\n",
      "it: 0, validation loss=0.9209967255592346, c=0.5847460225008431, ibs=0.1707706373454809, ibnll=0.5122529842311946\n",
      "Validation loss decreased (0.920997 --> 0.920639).  Saving model ...\n",
      "it: 1, validation loss=0.9206393361091614, c=0.5893442874222127, ibs=0.17054967250163733, ibnll=0.5115441566002966\n",
      "Validation loss decreased (0.920639 --> 0.919600).  Saving model ...\n",
      "it: 2, validation loss=0.9196001291275024, c=0.5894975629195917, ibs=0.17014371968946895, ibnll=0.5101185053152638\n",
      "Validation loss decreased (0.919600 --> 0.918890).  Saving model ...\n",
      "it: 3, validation loss=0.9188899993896484, c=0.5935746911498728, ibs=0.16983432993018863, ibnll=0.5088977251952463\n",
      "Validation loss decreased (0.918890 --> 0.917291).  Saving model ...\n",
      "it: 4, validation loss=0.917290985584259, c=0.5968547867937831, ibs=0.16939227077640998, ibnll=0.5074904377499211\n",
      "Validation loss decreased (0.917291 --> 0.913012).  Saving model ...\n",
      "it: 5, validation loss=0.9130121469497681, c=0.593237485055639, ibs=0.16849079213507082, ibnll=0.5053825638471549\n",
      "Validation loss decreased (0.913012 --> 0.905911).  Saving model ...\n",
      "it: 6, validation loss=0.9059109687805176, c=0.5856350203856412, ibs=0.1674164420524907, ibnll=0.5037773828717919\n",
      "Validation loss decreased (0.905911 --> 0.902251).  Saving model ...\n",
      "it: 7, validation loss=0.902251124382019, c=0.577174212930321, ibs=0.1669222683222048, ibnll=0.5028016102739558\n",
      "Validation loss decreased (0.902251 --> 0.899872).  Saving model ...\n",
      "it: 8, validation loss=0.8998719453811646, c=0.5763771803439502, ibs=0.1663600739864682, ibnll=0.5010214900091529\n",
      "Validation loss decreased (0.899872 --> 0.894682).  Saving model ...\n",
      "it: 9, validation loss=0.8946821689605713, c=0.5806382391710861, ibs=0.16538212565221683, ibnll=0.4986217520501925\n",
      "Validation loss decreased (0.894682 --> 0.885898).  Saving model ...\n",
      "it: 10, validation loss=0.8858979344367981, c=0.5905704914012446, ibs=0.16420576844499993, ibnll=0.4966421053405203\n",
      "Validation loss decreased (0.885898 --> 0.880150).  Saving model ...\n",
      "it: 11, validation loss=0.8801496028900146, c=0.6029244964899911, ibs=0.1642492485992947, ibnll=0.49766791579544184\n",
      "Validation loss decreased (0.880150 --> 0.877884).  Saving model ...\n",
      "it: 12, validation loss=0.8778841495513916, c=0.6116611998405935, ibs=0.16540958446678752, ibnll=0.5006876357621708\n",
      "Validation loss decreased (0.877884 --> 0.875194).  Saving model ...\n",
      "it: 13, validation loss=0.8751940727233887, c=0.6192943196100671, ibs=0.16570933548027938, ibnll=0.5010129689992783\n",
      "Validation loss decreased (0.875194 --> 0.864380).  Saving model ...\n",
      "it: 14, validation loss=0.8643803596496582, c=0.6254253395052267, ibs=0.16131432022249895, ibnll=0.4890766865063698\n",
      "Validation loss decreased (0.864380 --> 0.861470).  Saving model ...\n",
      "it: 15, validation loss=0.8614699840545654, c=0.6237393090340578, ibs=0.15891651911456997, ibnll=0.4813917130770889\n",
      "Validation loss decreased (0.861470 --> 0.860912).  Saving model ...\n",
      "it: 16, validation loss=0.8609120845794678, c=0.6219919683639373, ibs=0.1595929640450677, ibnll=0.4830388573029107\n",
      "EarlyStopping counter: 1 out of 20\n",
      "it: 17, validation loss=0.8617526888847351, c=0.6196008706048252, ibs=0.16202804947819957, ibnll=0.4892725395905815\n",
      "EarlyStopping counter: 2 out of 20\n",
      "it: 18, validation loss=0.8678566813468933, c=0.6203365929922443, ibs=0.16799865563735264, ibnll=0.503287788469585\n",
      "EarlyStopping counter: 3 out of 20\n",
      "it: 19, validation loss=0.8701221346855164, c=0.6201220072959137, ibs=0.17044777397934666, ibnll=0.507937141569374\n",
      "EarlyStopping counter: 4 out of 20\n",
      "it: 20, validation loss=0.8725375533103943, c=0.6203979031911958, ibs=0.1725734177568752, ibnll=0.5121693385124994\n",
      "EarlyStopping counter: 5 out of 20\n",
      "it: 21, validation loss=0.8754432201385498, c=0.6242297906256706, ibs=0.1741092893822329, ibnll=0.5156668686580476\n",
      "EarlyStopping counter: 6 out of 20\n",
      "it: 22, validation loss=0.8695836663246155, c=0.6233101376413966, ibs=0.17060189237498066, ibnll=0.50676524598687\n",
      "EarlyStopping counter: 7 out of 20\n",
      "it: 23, validation loss=0.8687037825584412, c=0.6202139725943411, ibs=0.16903504376076625, ibnll=0.5026531749125582\n",
      "EarlyStopping counter: 8 out of 20\n",
      "it: 24, validation loss=0.8693341016769409, c=0.6146653995892216, ibs=0.16810268422467123, ibnll=0.500305389226989\n",
      "EarlyStopping counter: 9 out of 20\n",
      "it: 25, validation loss=0.8680902719497681, c=0.6104963060605132, ibs=0.16595912822336142, ibnll=0.4952303500095117\n",
      "Validation loss decreased (0.860912 --> 0.859076).  Saving model ...\n",
      "it: 26, validation loss=0.8590760827064514, c=0.6130406793170043, ibs=0.16205070799049623, ibnll=0.4856860832606491\n",
      "Validation loss decreased (0.859076 --> 0.855109).  Saving model ...\n",
      "it: 27, validation loss=0.8551090955734253, c=0.6177615646362773, ibs=0.1630970591553921, ibnll=0.49003273615113735\n",
      "EarlyStopping counter: 1 out of 20\n",
      "it: 28, validation loss=0.858056366443634, c=0.6195089053063977, ibs=0.1646760869901414, ibnll=0.4949562536910676\n",
      "Validation loss decreased (0.855109 --> 0.852572).  Saving model ...\n",
      "it: 29, validation loss=0.8525720238685608, c=0.6178228748352288, ibs=0.16066252398965086, ibnll=0.4847058995188827\n",
      "Validation loss decreased (0.852572 --> 0.852014).  Saving model ...\n",
      "it: 30, validation loss=0.8520142436027527, c=0.619171699212164, ibs=0.15930270265456536, ibnll=0.48069400497313164\n",
      "EarlyStopping counter: 1 out of 20\n",
      "it: 31, validation loss=0.8590767979621887, c=0.6278164372643389, ibs=0.16331616990137396, ibnll=0.4930739553481407\n",
      "EarlyStopping counter: 2 out of 20\n",
      "it: 32, validation loss=0.8680930137634277, c=0.6319242205940958, ibs=0.16816856678369538, ibnll=0.50566463574162\n",
      "EarlyStopping counter: 3 out of 20\n",
      "it: 33, validation loss=0.8535951375961304, c=0.6307593268140155, ibs=0.16249197348331482, ibnll=0.49094061220980256\n",
      "Validation loss decreased (0.852014 --> 0.848622).  Saving model ...\n",
      "it: 34, validation loss=0.8486220240592957, c=0.6266821985837344, ibs=0.15923211512265914, ibnll=0.4805529927246008\n",
      "EarlyStopping counter: 1 out of 20\n",
      "it: 35, validation loss=0.8490313291549683, c=0.6251494436099445, ibs=0.15947706877084736, ibnll=0.48102002837588903\n",
      "Validation loss decreased (0.848622 --> 0.848427).  Saving model ...\n",
      "it: 36, validation loss=0.848426878452301, c=0.6276631617669599, ibs=0.1606385926807612, ibnll=0.4843455813856566\n",
      "EarlyStopping counter: 1 out of 20\n",
      "it: 37, validation loss=0.8517146706581116, c=0.6366757610128445, ibs=0.16388013004227944, ibnll=0.4934978185465944\n",
      "EarlyStopping counter: 2 out of 20\n",
      "it: 38, validation loss=0.8575045466423035, c=0.6409368198399804, ibs=0.16804360645238567, ibnll=0.5036511070505822\n",
      "Validation loss decreased (0.848427 --> 0.845767).  Saving model ...\n",
      "it: 39, validation loss=0.8457674980163574, c=0.641151405536311, ibs=0.16416570716383957, ibnll=0.4928620497554024\n",
      "Validation loss decreased (0.845767 --> 0.832185).  Saving model ...\n",
      "it: 40, validation loss=0.8321854472160339, c=0.6478955274209864, ibs=0.15752864404324832, ibnll=0.47536971523661387\n",
      "Validation loss decreased (0.832185 --> 0.827968).  Saving model ...\n",
      "it: 41, validation loss=0.8279678821563721, c=0.652984273933969, ibs=0.15395838174197984, ibnll=0.4651984497314597\n",
      "EarlyStopping counter: 1 out of 20\n",
      "it: 42, validation loss=0.8280026316642761, c=0.6544250636093314, ibs=0.15323563581297284, ibnll=0.46268905865578347\n",
      "EarlyStopping counter: 2 out of 20\n",
      "it: 43, validation loss=0.8302129507064819, c=0.6558965083841697, ibs=0.15336728888999934, ibnll=0.4626516103477584\n",
      "Validation loss decreased (0.827968 --> 0.826708).  Saving model ...\n",
      "it: 44, validation loss=0.826708197593689, c=0.6568468164679194, ibs=0.153063946407245, ibnll=0.4621644218184597\n",
      "Validation loss decreased (0.826708 --> 0.825978).  Saving model ...\n",
      "it: 45, validation loss=0.8259782791137695, c=0.6575825388553386, ibs=0.15298122112379647, ibnll=0.46156665065261454\n",
      "EarlyStopping counter: 1 out of 20\n",
      "it: 46, validation loss=0.8325640559196472, c=0.6571533674626774, ibs=0.15360455032343498, ibnll=0.4619061447178215\n",
      "EarlyStopping counter: 2 out of 20\n",
      "it: 47, validation loss=0.8323719501495361, c=0.6596977407191686, ibs=0.15324775315851083, ibnll=0.46145229364869794\n",
      "Validation loss decreased (0.825978 --> 0.821971).  Saving model ...\n",
      "it: 48, validation loss=0.8219708800315857, c=0.6605867386039668, ibs=0.15187881644312912, ibnll=0.4600239194877152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.821971 --> 0.819276).  Saving model ...\n",
      "it: 49, validation loss=0.8192763924598694, c=0.662518009870942, ibs=0.1528380616550121, ibnll=0.463059390873329\n",
      "EarlyStopping counter: 1 out of 20\n",
      "it: 50, validation loss=0.8198210597038269, c=0.6629778363630789, ibs=0.1546636756482239, ibnll=0.4680279871580439\n",
      "EarlyStopping counter: 2 out of 20\n",
      "it: 51, validation loss=0.8274588584899902, c=0.6650623831274333, ibs=0.15843708098244075, ibnll=0.47819486065775324\n",
      "EarlyStopping counter: 3 out of 20\n",
      "it: 52, validation loss=0.8418700695037842, c=0.6621194935777567, ibs=0.16489022098532138, ibnll=0.4937049073224103\n",
      "EarlyStopping counter: 4 out of 20\n",
      "it: 53, validation loss=0.8457547426223755, c=0.6598816713160234, ibs=0.1670082862698515, ibnll=0.49814121308549714\n",
      "EarlyStopping counter: 5 out of 20\n",
      "it: 54, validation loss=0.8321608304977417, c=0.6551914410962264, ibs=0.15903152442515037, ibnll=0.47798056855422705\n",
      "EarlyStopping counter: 6 out of 20\n",
      "it: 55, validation loss=0.8420799374580383, c=0.6402010974525613, ibs=0.15712852662855062, ibnll=0.4700347350911526\n",
      "EarlyStopping counter: 7 out of 20\n",
      "it: 56, validation loss=0.8600991368293762, c=0.6368290365102235, ibs=0.1590120033902738, ibnll=0.47328562389877216\n",
      "EarlyStopping counter: 8 out of 20\n",
      "it: 57, validation loss=0.8586774468421936, c=0.6466999785414304, ibs=0.1577964333448985, ibnll=0.47038136737558034\n",
      "EarlyStopping counter: 9 out of 20\n",
      "it: 58, validation loss=0.8363848924636841, c=0.6613531160908617, ibs=0.15308391574363758, ibnll=0.4606670441980169\n",
      "EarlyStopping counter: 10 out of 20\n",
      "it: 59, validation loss=0.8228198885917664, c=0.6736764660801324, ibs=0.15085882727744424, ibnll=0.4596537437536416\n",
      "EarlyStopping counter: 11 out of 20\n",
      "it: 60, validation loss=0.8340689539909363, c=0.6757916679439625, ibs=0.15676088260700538, ibnll=0.477328936396433\n",
      "EarlyStopping counter: 12 out of 20\n",
      "it: 61, validation loss=0.8518993258476257, c=0.6749026700591644, ibs=0.1646579372628025, ibnll=0.4966453802506645\n",
      "EarlyStopping counter: 13 out of 20\n",
      "it: 62, validation loss=0.8469772934913635, c=0.674963980258116, ibs=0.16233944896702945, ibnll=0.49109008724623426\n",
      "EarlyStopping counter: 14 out of 20\n",
      "it: 63, validation loss=0.8267393112182617, c=0.6787039023941632, ibs=0.1547933331966961, ibnll=0.4712191696733554\n",
      "EarlyStopping counter: 15 out of 20\n",
      "it: 64, validation loss=0.8195775747299194, c=0.6779681800067441, ibs=0.15095211460248703, ibnll=0.45795032019473253\n",
      "EarlyStopping counter: 16 out of 20\n",
      "it: 65, validation loss=0.8236655592918396, c=0.6751785659544465, ibs=0.15131288823538813, ibnll=0.4563406299451276\n",
      "EarlyStopping counter: 17 out of 20\n",
      "it: 66, validation loss=0.823336660861969, c=0.6761595291376721, ibs=0.1511320938775967, ibnll=0.4554816870496559\n",
      "Validation loss decreased (0.819276 --> 0.814173).  Saving model ...\n",
      "it: 67, validation loss=0.8141728043556213, c=0.679807485975292, ibs=0.1500130489690521, ibnll=0.4547649975919074\n",
      "Validation loss decreased (0.814173 --> 0.810208).  Saving model ...\n",
      "it: 68, validation loss=0.8102075457572937, c=0.6786425921952117, ibs=0.14964987861474416, ibnll=0.4552347211002314\n",
      "EarlyStopping counter: 1 out of 20\n",
      "it: 69, validation loss=0.8112612962722778, c=0.6777535943104135, ibs=0.14979403778818504, ibnll=0.45575399155346813\n",
      "EarlyStopping counter: 2 out of 20\n",
      "it: 70, validation loss=0.8184998631477356, c=0.6712547132215444, ibs=0.15132433536748688, ibnll=0.4594337419017094\n",
      "EarlyStopping counter: 3 out of 20\n",
      "it: 71, validation loss=0.8198574185371399, c=0.66493976272953, ibs=0.15209413084700607, ibnll=0.4621026695364212\n",
      "EarlyStopping counter: 4 out of 20\n",
      "it: 72, validation loss=0.8155357241630554, c=0.6624873547714663, ibs=0.1519981769324254, ibnll=0.46234349962332005\n",
      "EarlyStopping counter: 5 out of 20\n",
      "it: 73, validation loss=0.8134185671806335, c=0.6603108427086846, ibs=0.15218312511780321, ibnll=0.46246342056172657\n",
      "Validation loss decreased (0.810208 --> 0.809631).  Saving model ...\n",
      "it: 74, validation loss=0.8096309304237366, c=0.6611078752950553, ibs=0.1519767746652132, ibnll=0.46140514710761177\n",
      "Validation loss decreased (0.809631 --> 0.807009).  Saving model ...\n",
      "it: 75, validation loss=0.8070092797279358, c=0.6623034241746114, ibs=0.15103081623832187, ibnll=0.4583192458715068\n",
      "EarlyStopping counter: 1 out of 20\n",
      "it: 76, validation loss=0.8076218962669373, c=0.6646025566352963, ibs=0.1499384086899115, ibnll=0.45485404274082036\n",
      "Validation loss decreased (0.807009 --> 0.804774).  Saving model ...\n",
      "it: 77, validation loss=0.8047738671302795, c=0.6654915545200945, ibs=0.14955893836204437, ibnll=0.45470435597543696\n",
      "EarlyStopping counter: 1 out of 20\n",
      "it: 78, validation loss=0.813433825969696, c=0.6615063915882406, ibs=0.15270995453340114, ibnll=0.46590080560387737\n",
      "EarlyStopping counter: 2 out of 20\n",
      "it: 79, validation loss=0.8252029418945312, c=0.6566322307715888, ibs=0.15574864326249308, ibnll=0.475035434412592\n",
      "EarlyStopping counter: 3 out of 20\n",
      "it: 80, validation loss=0.8291130065917969, c=0.6592685693265075, ibs=0.15741630697291795, ibnll=0.4795377612731217\n",
      "EarlyStopping counter: 4 out of 20\n",
      "it: 81, validation loss=0.8270394206047058, c=0.6610772201955796, ibs=0.15596177201362263, ibnll=0.47491020681433826\n",
      "EarlyStopping counter: 5 out of 20\n",
      "it: 82, validation loss=0.8228684067726135, c=0.6595444652217897, ibs=0.1530822199178513, ibnll=0.4660144005112607\n",
      "EarlyStopping counter: 6 out of 20\n",
      "it: 83, validation loss=0.8214126825332642, c=0.6592072591275558, ibs=0.15159485334063927, ibnll=0.46054538034557896\n",
      "EarlyStopping counter: 7 out of 20\n",
      "it: 84, validation loss=0.8202254176139832, c=0.6591459489286042, ibs=0.15103588926922865, ibnll=0.4583706193005408\n",
      "EarlyStopping counter: 8 out of 20\n",
      "it: 85, validation loss=0.8186277747154236, c=0.6604947733055394, ibs=0.15039851789389763, ibnll=0.4554094498905556\n",
      "EarlyStopping counter: 9 out of 20\n",
      "it: 86, validation loss=0.817432165145874, c=0.6613224609913859, ibs=0.1500718486399403, ibnll=0.45412362696317216\n",
      "EarlyStopping counter: 10 out of 20\n",
      "it: 87, validation loss=0.8208746910095215, c=0.6634989730541676, ibs=0.1502672970733335, ibnll=0.45271863312676675\n",
      "EarlyStopping counter: 11 out of 20\n",
      "it: 88, validation loss=0.8318079710006714, c=0.6638055240489256, ibs=0.15211320268203346, ibnll=0.45474108070902974\n",
      "EarlyStopping counter: 12 out of 20\n",
      "it: 89, validation loss=0.8417088985443115, c=0.6641733852426351, ibs=0.1535882952454671, ibnll=0.4573174200441132\n",
      "EarlyStopping counter: 13 out of 20\n",
      "it: 90, validation loss=0.839320182800293, c=0.665246313724288, ibs=0.15247139387759187, ibnll=0.45459943722784296\n",
      "EarlyStopping counter: 14 out of 20\n",
      "it: 91, validation loss=0.8264483213424683, c=0.6663498973054167, ibs=0.15005463181664294, ibnll=0.45039023272462736\n",
      "EarlyStopping counter: 15 out of 20\n",
      "it: 92, validation loss=0.8244962692260742, c=0.667668066582876, ibs=0.15068599738185426, ibnll=0.45397751612369236\n",
      "EarlyStopping counter: 16 out of 20\n",
      "it: 93, validation loss=0.825829267501831, c=0.6679133073786825, ibs=0.1524554170415794, ibnll=0.45973107148809556\n",
      "EarlyStopping counter: 17 out of 20\n",
      "it: 94, validation loss=0.8191072344779968, c=0.6709175071273106, ibs=0.15096809873164216, ibnll=0.4556923020761205\n",
      "EarlyStopping counter: 18 out of 20\n",
      "it: 95, validation loss=0.8168548941612244, c=0.6724196070016247, ibs=0.1495114696088973, ibnll=0.4490219207492496\n",
      "EarlyStopping counter: 19 out of 20\n",
      "it: 96, validation loss=0.831622838973999, c=0.6703657153367463, ibs=0.1520386032620127, ibnll=0.4537005984453779\n",
      "EarlyStopping counter: 20 out of 20\n",
      "it: 97, validation loss=0.8249347805976868, c=0.6705189908341253, ibs=0.15135472663652802, ibnll=0.45248261741241036\n",
      "Early stopping\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from pycox import datasets\n",
    "\n",
    "kfold = StratifiedKFold(5, shuffle=True)\n",
    "df_all = datasets.metabric.read_df()\n",
    "gen = kfold.split(df_all.iloc[:,df_all.columns.values!=\"event\"],df_all.loc[:,\"event\"])\n",
    "\n",
    "config = {\n",
    "    \"lr\": 1e-4,\n",
    "    \"weight_decay\": 1e-3,\n",
    "    \"num_latent\": 25,\n",
    "    \"encoder_neurons\": 800,\n",
    "    \"num_encoder_layers\": 2,\n",
    "    \"encoder_dropout\": 0.1,\n",
    "    \"odefunc_neurons\": 1000,\n",
    "    \"num_odefunc_layers\": 3,\n",
    "    \"batch_size\": 1/3,\n",
    "    \"multiplier\": 3.,\n",
    "    \"mu\": 1e-4,\n",
    "    \"softplus_beta\": 1.,\n",
    "    \"scheduler_epoch\": 50,\n",
    "    \"scheduler_gamma\": 0.1,\n",
    "    \"patience\": 20\n",
    "}\n",
    "\n",
    "odesurv_bench_vals = []\n",
    "for g in gen:\n",
    "    df_train = df_all.iloc[g[0]]\n",
    "    df_test =  df_all.iloc[g[1]]\n",
    "    conc, ibs, ibnll = odesurv_manual_benchmark(df_train,df_test,config,\"metabrick_test\")\n",
    "    odesurv_bench_vals.append([conc,ibs,ibnll])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c=0.6558965225605643+-0.012243617513931153\n",
      "ibs=0.16124182424646324+-0.008117888646458552\n",
      "ibnll=0.48242733142488453+-0.022724151456051847\n"
     ]
    }
   ],
   "source": [
    "print(\"c=\"+str(np.mean(np.array(odesurv_bench_vals)[:,0]))+\"+-\"+str(np.std(np.array(odesurv_bench_vals)[:,0])))\n",
    "print(\"ibs=\"+str(np.mean(np.array(odesurv_bench_vals)[:,1]))+\"+-\"+str(np.std(np.array(odesurv_bench_vals)[:,1])))\n",
    "print(\"ibnll=\"+str(np.mean(np.array(odesurv_bench_vals)[:,2]))+\"+-\"+str(np.std(np.array(odesurv_bench_vals)[:,2])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "def odesurv_benchmark(df_train, df_test, name, fold_num):\n",
    "    torch.cuda.empty_cache()\n",
    "    df_train, df_val = train_test_split(df_train, test_size=0.2)\n",
    "    \n",
    "    Tmax = df_train[\"duration\"].max()*1.2\n",
    "    \n",
    "    def train_model(config):\n",
    "        train_loader = make_dataloader(df_train,int(len(df_train)*config[\"batch_size\"]))\n",
    "        val_loader = make_dataloader(df_val,len(df_val))\n",
    "        \n",
    "        num_in = 9\n",
    "        num_latent = config[\"num_latent\"]\n",
    "        layers_encoder =  [config[\"encoder_neurons\"]]*config[\"num_encoder_layers\"]\n",
    "        dropout_encoder = [config[\"encoder_dropout\"]]*config[\"num_encoder_layers\"]\n",
    "        layers_odefunc =  [config[\"odefunc_neurons\"]]*config[\"num_odefunc_layers\"]\n",
    "        dropout_odefunc = []\n",
    "        \n",
    "        trans_matrix = torch.tensor([[np.nan,1],[np.nan,np.nan]]).to(device)\n",
    "\n",
    "        encoder = Encoder(num_in,num_latent,layers_encoder, dropout_encoder).to(device)\n",
    "        odefunc = ODEFunc(trans_matrix,num_in,num_latent,layers_odefunc,dropout_odefunc,config[\"softplus_beta\"]).to(device)\n",
    "        block = ODEBlock(odefunc).to(device)\n",
    "        odesurv = ODEsurv(block,encoder,Tmax,config[\"multiplier\"]).to(device)\n",
    "\n",
    "        optimizer = torch.optim.Adam(odesurv.parameters(), weight_decay = config[\"weight_decay\"], lr=config[\"lr\"])\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, config[\"scheduler_epoch\"], gamma=config[\"scheduler_gamma\"])\n",
    "#         scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=config[\"scheduler_gamma\"], patience=config[\"scheduler_epoch\"])\n",
    "\n",
    "        loss_min = np.inf\n",
    "        for i in range(1000):\n",
    "            odesurv.train()\n",
    "            for mini,ds in enumerate(train_loader):\n",
    "                myloss,_,_ = loss(odesurv,*ds,mu=config[\"mu\"])\n",
    "                optimizer.zero_grad()\n",
    "                myloss.backward()    \n",
    "                optimizer.step()\n",
    "       \n",
    "            odesurv.eval()\n",
    "            with torch.no_grad():\n",
    "                train_loss = 0.\n",
    "                for mini,ds in enumerate(train_loader):\n",
    "                    myloss,_,_ = loss(odesurv,*ds,mu=config[\"mu\"])\n",
    "                    train_loss += myloss.item()\n",
    "                lossval,conc,ibs,ibnll = 0., 0., 0., 0.\n",
    "                for _,ds in enumerate(val_loader):\n",
    "                    t1,_,_ = loss(odesurv,*ds,mu=config[\"mu\"])\n",
    "                    lossval += t1.item()\n",
    "                    t1,t2,t3 = measures(odesurv,torch.tensor([1.,0.],device=device),*ds)\n",
    "                    conc += t1\n",
    "                    ibs += t2\n",
    "                    ibnll += t3\n",
    "                track.log(concordance=conc/len(val_loader),int_brier_score=ibs/len(val_loader),int_bin_nll = ibnll/len(val_loader), loss=lossval/len(val_loader), train_loss=train_loss/len(val_loader))\n",
    "                scheduler.step(lossval/len(val_loader))\n",
    "                if lossval/len(val_loader) < loss_min:\n",
    "                    torch.save(odesurv.state_dict(), 'checkpoint.pt')\n",
    "                    \n",
    "    \n",
    "    dt_string = name+\"_fold_\"+str(fold_num)+datetime.now().strftime(\"_date_%d_%m_%Y_time_%H_%M_%S\")\n",
    "    analysis = tune.run(train_model, name=dt_string, num_samples=4, stop={\"training_iteration\": 100}, config=search_space, resources_per_trial={\"cpu\": 1, \"gpu\": 1}, verbose=1, raise_on_failed_trial=False)\n",
    "#     analysis = tune.run(train_model, name=dt_string, num_samples=300, scheduler=ASHAScheduler(metric=\"loss\", mode=\"min\"), config=search_space, resources_per_trial={\"cpu\": 1, \"gpu\": 0.25}, verbose=1, raise_on_failed_trial=False)\n",
    "#     analysis = tune.run(train_model, name=dt_string, num_samples=50, scheduler=ASHAScheduler(metric=\"concordance\", mode=\"max\", max_t=80, grace_period=20, reduction_factor=2, brackets=3), config=search_space, resources_per_trial={\"cpu\": 1, \"gpu\": 0.25}, verbose=1, raise_on_failed_trial=False)\n",
    "    best_config = analysis.get_best_config(metric=\"concordance\", mode=\"max\")\n",
    "    logdir = analysis.get_best_logdir(\"concordance\", mode=\"max\")\n",
    "    \n",
    "    num_in = 9\n",
    "    num_latent = best_config[\"num_latent\"]\n",
    "    layers_encoder =  [best_config[\"encoder_neurons\"]]*best_config[\"num_encoder_layers\"]\n",
    "    dropout_encoder = [best_config[\"encoder_dropout\"]]*best_config[\"num_encoder_layers\"]\n",
    "    layers_odefunc =  [best_config[\"odefunc_neurons\"]]*best_config[\"num_odefunc_layers\"]\n",
    "    dropout_odefunc = []\n",
    "\n",
    "    trans_matrix = torch.tensor([[np.nan,1],[np.nan,np.nan]]).to(device)\n",
    "\n",
    "    encoder = Encoder(num_in,num_latent,layers_encoder, dropout_encoder).to(device)\n",
    "    odefunc = ODEFunc(trans_matrix,num_in,num_latent,layers_odefunc,dropout_odefunc,best_config[\"softplus_beta\"]).to(device)\n",
    "    block = ODEBlock(odefunc).to(device)\n",
    "    odesurv = ODEsurv(block,encoder,Tmax,best_config[\"multiplier\"]).to(device)\n",
    "    \n",
    "    odesurv.load_state_dict(torch.load(os.path.join(logdir, \"checkpoint.pt\")))\n",
    "\n",
    "    loader = make_dataloader(df_test,len(df_test))\n",
    "    odesurv.eval()\n",
    "    with torch.no_grad():\n",
    "        conc,ibs,ibnll = 0., 0., 0.\n",
    "        for _,ds in enumerate(loader):\n",
    "            t1,t2,t3 = measures(odesurv,torch.tensor([1.,0.],device=device),*ds)\n",
    "            conc += t1\n",
    "            ibs += t2\n",
    "            ibnll += t3\n",
    "    return conc/len(loader), ibs/len(loader), ibnll/len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune import track\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "# search_space = {\n",
    "#     \"lr\": tune.loguniform(5e-5,1e-3),\n",
    "#     \"weight_decay\": tune.loguniform(1e-7,1e-2),\n",
    "#     \"num_latent\": tune.randint(10,150),\n",
    "#     \"encoder_neurons\": 20,\n",
    "#     \"num_encoder_layers\": 1,\n",
    "#     \"encoder_dropout\": 0.2,\n",
    "#     \"odefunc_neurons\": tune.randint(100,1000),\n",
    "#     \"num_odefunc_layers\": tune.randint(1,3),\n",
    "#     \"batch_size\": 512,\n",
    "#     \"multiplier\": 1.,\n",
    "#     \"mu\": 1e-4,\n",
    "#     \"softplus_beta\": 1.,\n",
    "#     \"scheduler_epoch\": 10,\n",
    "#     \"scheduler_gamma\": tune.uniform(0.1,1.)\n",
    "# }\n",
    "\n",
    "# search_space = {\n",
    "#     \"lr\": 4e-4,\n",
    "#     \"weight_decay\": 1e-7,\n",
    "#     \"num_latent\": 200,\n",
    "#     \"encoder_neurons\": 50,\n",
    "#     \"num_encoder_layers\": 2,\n",
    "#     \"encoder_dropout\": 0.2,\n",
    "#     \"odefunc_neurons\": tune.grid_search([400,800]),\n",
    "#     \"num_odefunc_layers\": tune.grid_search([1,2]),\n",
    "#     \"batch_size\": 1/3,\n",
    "#     \"multiplier\": 3.,\n",
    "#     \"mu\": 0.1,\n",
    "#     \"softplus_beta\": 1.,\n",
    "#     \"scheduler_epoch\": 20,\n",
    "#     \"scheduler_gamma\": 0.5\n",
    "# }\n",
    "\n",
    "search_space = {\n",
    "    \"lr\": tune.grid_search([1e-4,2e-4]),\n",
    "    \"weight_decay\": 1e-3,\n",
    "    \"num_latent\": 200,\n",
    "    \"encoder_neurons\": 50,\n",
    "    \"num_encoder_layers\": 2,\n",
    "    \"encoder_dropout\": 0.,\n",
    "    \"odefunc_neurons\": 800,\n",
    "    \"num_odefunc_layers\": 2,\n",
    "    \"batch_size\": 1/4,\n",
    "    \"multiplier\": 1.,\n",
    "    \"mu\": 0.1,\n",
    "    \"softplus_beta\": 1.,\n",
    "    \"scheduler_epoch\": 20,\n",
    "    \"scheduler_gamma\": 0.1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/59.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/16 CPUs, 4/4 GPUs, 0.0/33.4 GiB heap, 0.0/11.52 GiB objects<br>Result logdir: /home/jupyter/ray_results/metabric_stratkfold_test_fold_1_date_15_05_2020_time_22_43_00<br>Number of trials: 8 (4 PENDING, 4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name       </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_model_00000</td><td>RUNNING </td><td>10.142.0.7:5997</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">     0</td><td style=\"text-align: right;\">         22.5935</td></tr>\n",
       "<tr><td>train_model_00001</td><td>RUNNING </td><td>               </td><td style=\"text-align: right;\">0.0002</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_model_00002</td><td>RUNNING </td><td>               </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_model_00003</td><td>RUNNING </td><td>               </td><td style=\"text-align: right;\">0.0002</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_model_00004</td><td>PENDING </td><td>               </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_model_00005</td><td>PENDING </td><td>               </td><td style=\"text-align: right;\">0.0002</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_model_00006</td><td>PENDING </td><td>               </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_model_00007</td><td>PENDING </td><td>               </td><td style=\"text-align: right;\">0.0002</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-1b236ea189e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mdf_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mdf_test\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mdf_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mconc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mibs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mibnll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0modesurv_benchmark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"metabric_stratkfold_test\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfold_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mfold_num\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0modesurv_bench_vals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mibs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mibnll\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-c1df02488e52>\u001b[0m in \u001b[0;36modesurv_benchmark\u001b[0;34m(df_train, df_test, name, fold_num)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mdt_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_fold_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_date_%d_%m_%Y_time_%H_%M_%S\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0manalysis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtune\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdt_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"training_iteration\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msearch_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresources_per_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"gpu\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_on_failed_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;31m#     analysis = tune.run(train_model, name=dt_string, num_samples=300, scheduler=ASHAScheduler(metric=\"loss\", mode=\"min\"), config=search_space, resources_per_trial={\"cpu\": 1, \"gpu\": 0.25}, verbose=1, raise_on_failed_trial=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;31m#     analysis = tune.run(train_model, name=dt_string, num_samples=50, scheduler=ASHAScheduler(metric=\"concordance\", mode=\"max\", max_t=80, grace_period=20, reduction_factor=2, brackets=3), config=search_space, resources_per_trial={\"cpu\": 1, \"gpu\": 0.25}, verbose=1, raise_on_failed_trial=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, stop, config, resources_per_trial, num_samples, local_dir, upload_dir, trial_name_creator, loggers, sync_to_cloud, sync_to_driver, checkpoint_freq, checkpoint_at_end, sync_on_checkpoint, keep_checkpoints_num, checkpoint_score_attr, global_checkpoint_period, export_formats, max_failures, fail_fast, restore, search_alg, scheduler, with_server, server_port, verbose, progress_reporter, resume, queue_trials, reuse_actors, trial_executor, raise_on_failed_trial, return_trials, ray_auto_init)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_finished\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0m_report_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrunner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_reporter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/ray/tune/trial_runner.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    337\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrial_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_trial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrial_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_running_trials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# blocking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrial_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_no_available_trials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/ray/tune/trial_runner.py\u001b[0m in \u001b[0;36m_process_events\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;31m# TODO(ujvl): Consider combining get_next_available_trial and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0;31m#  fetch_result functionality so that we don't timeout on fetch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m             \u001b[0mtrial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrial_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next_available_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# blocking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_restoring\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mwarn_if_slow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"process_trial_restore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py\u001b[0m in \u001b[0;36mget_next_available_trial\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[0;31m# See https://github.com/ray-project/ray/issues/4211 for details.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m         \u001b[0;34m[\u001b[0m\u001b[0mresult_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshuffled_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m         \u001b[0mwait_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait_time\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mNONTRIVIAL_WAIT_TIME_THRESHOLD_S\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/ray/worker.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_ids, num_returns, timeout)\u001b[0m\n\u001b[1;32m   1646\u001b[0m             \u001b[0mnum_returns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m             \u001b[0mtimeout_milliseconds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1648\u001b[0;31m             \u001b[0mworker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_task_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1649\u001b[0m         )\n\u001b[1;32m   1650\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mready_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpython/ray/_raylet.pyx\u001b[0m in \u001b[0;36mray._raylet.CoreWorker.wait\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpython/ray/_raylet.pyx\u001b[0m in \u001b[0;36mray._raylet.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from pycox import datasets\n",
    "\n",
    "kfold = KFold(5, shuffle=True)\n",
    "df_all = datasets.metabric.read_df()\n",
    "gen = kfold.split(df_all)\n",
    "\n",
    "\n",
    "ray.init(webui_host='0.0.0.0')\n",
    "\n",
    "odesurv_bench_vals = []\n",
    "fold_num = 1\n",
    "for g in gen:\n",
    "    df_train = df_all.iloc[g[0]]\n",
    "    df_test =  df_all.iloc[g[1]]\n",
    "    conc, ibs, ibnll = odesurv_benchmark(df_train,df_test,\"metabric_stratkfold_test\",fold_num)\n",
    "    fold_num+=1\n",
    "    odesurv_bench_vals.append([conc,ibs,ibnll])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c=0.6620480281168408+-0.014956263887575002\n",
      "ibs=0.16545908680706367+-0.006979003894020888\n",
      "ibnll=0.4956326341833181+-0.017867351361639697\n"
     ]
    }
   ],
   "source": [
    "print(\"c=\"+str(np.mean(np.array(odesurv_bench_vals)[:,0]))+\"+-\"+str(np.std(np.array(odesurv_bench_vals)[:,0])))\n",
    "print(\"ibs=\"+str(np.mean(np.array(odesurv_bench_vals)[:,1]))+\"+-\"+str(np.std(np.array(odesurv_bench_vals)[:,1])))\n",
    "print(\"ibnll=\"+str(np.mean(np.array(odesurv_bench_vals)[:,2]))+\"+-\"+str(np.std(np.array(odesurv_bench_vals)[:,2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c=0.6672544577555329+-0.018223778302013317\n",
      "ibs=0.15723787183862553+-0.013663084535737445\n",
      "ibnll=0.47655865936706887+-0.04510651962063296\n"
     ]
    }
   ],
   "source": [
    "print(\"c=\"+str(np.mean(np.array(odesurv_bench_vals)[:,0]))+\"+-\"+str(np.std(np.array(odesurv_bench_vals)[:,0])))\n",
    "print(\"ibs=\"+str(np.mean(np.array(odesurv_bench_vals)[:,1]))+\"+-\"+str(np.std(np.array(odesurv_bench_vals)[:,1])))\n",
    "print(\"ibnll=\"+str(np.mean(np.array(odesurv_bench_vals)[:,2]))+\"+-\"+str(np.std(np.array(odesurv_bench_vals)[:,2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c=0.6683081896353393+-0.01586286779252497\n",
      "ibs=0.157976523120051+-0.007545165226842307\n",
      "ibnll=0.4741855078212528+-0.02476666721566905\n"
     ]
    }
   ],
   "source": [
    "print(\"c=\"+str(np.mean(np.array(odesurv_bench_vals)[:,0]))+\"+-\"+str(np.std(np.array(odesurv_bench_vals)[:,0])))\n",
    "print(\"ibs=\"+str(np.mean(np.array(odesurv_bench_vals)[:,1]))+\"+-\"+str(np.std(np.array(odesurv_bench_vals)[:,1])))\n",
    "print(\"ibnll=\"+str(np.mean(np.array(odesurv_bench_vals)[:,2]))+\"+-\"+str(np.std(np.array(odesurv_bench_vals)[:,2])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn_pandas import DataFrameMapper\n",
    "import pandas as pd\n",
    "\n",
    "def make_dataloader(df,Tmax,batchsize):\n",
    "    cols_standardize = ['x3', 'x4', 'x5', 'x6']\n",
    "    cols_leave = [\"x0\",\"x1\",\"x2\"]\n",
    "\n",
    "    standardize = [([col], StandardScaler()) for col in cols_standardize]\n",
    "    leave = [(col, None) for col in cols_leave]\n",
    "\n",
    "    x_mapper = DataFrameMapper(standardize + leave)\n",
    "    X = x_mapper.fit_transform(df).astype('float32')\n",
    "    \n",
    "    X = torch.from_numpy(X).float().to(device)\n",
    "    T = torch.from_numpy(df[[\"duration\"]].values).float().flatten().to(device)\n",
    "    Tmax = torch.tensor(Tmax).to(device)\n",
    "    T = T/Tmax\n",
    "    T[T==0] = 1e-8\n",
    "    E = torch.from_numpy(df[[\"event\"]].values).float().flatten().to(device)\n",
    "\n",
    "    Tstart = torch.from_numpy(np.array([0 for i in range(T.shape[0])])).float().to(device)\n",
    "    From = torch.tensor([1],device=device).repeat((T.shape))\n",
    "    To = torch.tensor([2],device=device).repeat((T.shape))\n",
    "    trans = torch.tensor([1],device=device).repeat((T.shape))\n",
    "\n",
    "    dataset = TensorDataset(X,Tstart,T,From,To,trans,E)\n",
    "    loader = DataLoader(dataset, batch_size=batchsize, shuffle=True)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune import track\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "# search_space = {\n",
    "#     \"lr\": tune.loguniform(5e-5,1e-3),\n",
    "#     \"weight_decay\": tune.loguniform(1e-7,1e-2),\n",
    "#     \"num_latent\": tune.randint(10,150),\n",
    "#     \"encoder_neurons\": 20,\n",
    "#     \"num_encoder_layers\": 1,\n",
    "#     \"encoder_dropout\": 0.2,\n",
    "#     \"odefunc_neurons\": tune.randint(100,1000),\n",
    "#     \"num_odefunc_layers\": tune.randint(1,3),\n",
    "#     \"batch_size\": 512,\n",
    "#     \"multiplier\": 1.,\n",
    "#     \"mu\": 1e-4,\n",
    "#     \"softplus_beta\": 1.,\n",
    "#     \"scheduler_epoch\": 10,\n",
    "#     \"scheduler_gamma\": tune.uniform(0.1,1.)\n",
    "# }\n",
    "\n",
    "search_space = {\n",
    "    \"lr\": tune.grid_search([2e-4,1e-4,8e-5]),\n",
    "    \"weight_decay\": 1e-2,\n",
    "    \"num_latent\": 50,\n",
    "    \"encoder_neurons\": 50,\n",
    "    \"num_encoder_layers\": 2,\n",
    "    \"encoder_dropout\": 0.2,\n",
    "    \"odefunc_neurons\": tune.grid_search([400,800]),\n",
    "    \"num_odefunc_layers\": tune.grid_search([1,2]),\n",
    "    \"batch_size\": 512,\n",
    "    \"multiplier\": 2.,\n",
    "    \"mu\": 1e-4,\n",
    "    \"softplus_beta\": 1.,\n",
    "    \"scheduler_epoch\": 10,\n",
    "    \"scheduler_gamma\": 0.3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "def odesurv_benchmark(df_train, df_test, name):\n",
    "    torch.cuda.empty_cache()\n",
    "    df_train, df_val = train_test_split(df_train, test_size=0.2, stratify=df_train.loc[:,\"event\"])\n",
    "    \n",
    "    Tmax = df_train[\"duration\"].max()*1.2\n",
    "    \n",
    "    def train_model(config):\n",
    "        train_loader = make_dataloader(df_train,Tmax/config[\"multiplier\"],int(len(df_train)/3))\n",
    "        val_loader = make_dataloader(df_val,Tmax/config[\"multiplier\"],len(df_val))\n",
    "        \n",
    "        num_in = 7\n",
    "        num_latent = config[\"num_latent\"]\n",
    "        layers_encoder =  [config[\"encoder_neurons\"]]*config[\"num_encoder_layers\"]\n",
    "        dropout_encoder = [config[\"encoder_dropout\"]]*config[\"num_encoder_layers\"]\n",
    "        layers_odefunc =  [config[\"odefunc_neurons\"]]*config[\"num_odefunc_layers\"]\n",
    "        dropout_odefunc = []\n",
    "        \n",
    "        trans_matrix = torch.tensor([[np.nan,1],[np.nan,np.nan]]).to(device)\n",
    "\n",
    "        encoder = Encoder(num_in,num_latent,layers_encoder, dropout_encoder).to(device)\n",
    "        odefunc = ODEFunc(trans_matrix,num_in,num_latent,layers_odefunc,dropout_odefunc,config[\"softplus_beta\"]).to(device)\n",
    "        block = ODEBlock(odefunc).to(device)\n",
    "        odesurv = ODEsurv(block,encoder).to(device)\n",
    "\n",
    "        optimizer = torch.optim.Adam(odesurv.parameters(), weight_decay = config[\"weight_decay\"], lr=config[\"lr\"])\n",
    "#         scheduler = torch.optim.lr_scheduler.StepLR(optimizer, config[\"scheduler_epoch\"], gamma=config[\"scheduler_gamma\"])\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=config[\"scheduler_gamma\"], patience=config[\"scheduler_epoch\"])\n",
    "\n",
    "        conc_max = - np.inf\n",
    "        for i in range(1000):\n",
    "            odesurv.train()\n",
    "            for mini,ds in enumerate(train_loader):\n",
    "                myloss,_,_ = loss(odesurv,*ds,mu=config[\"mu\"])\n",
    "                optimizer.zero_grad()\n",
    "                myloss.backward()    \n",
    "                optimizer.step()\n",
    "       \n",
    "            odesurv.eval()\n",
    "            with torch.no_grad():\n",
    "                train_loss = 0.\n",
    "                for mini,ds in enumerate(train_loader):\n",
    "                    myloss,_,_ = loss(odesurv,*ds,mu=config[\"mu\"])\n",
    "                    train_loss += myloss.item()\n",
    "                lossval,conc,ibs,ibnll = 0., 0., 0., 0.\n",
    "                for _,ds in enumerate(val_loader):\n",
    "                    t1,_,_ = loss(odesurv,*ds,mu=config[\"mu\"])\n",
    "                    lossval += t1.item()\n",
    "                    t1,t2,t3 = measures(odesurv,torch.tensor([1.,0.],device=device),Tmax,*ds,multiplier=config[\"multiplier\"])\n",
    "                    conc += t1\n",
    "                    ibs += t2\n",
    "                    ibnll += t3\n",
    "                track.log(concordance=conc/len(val_loader),int_brier_score=ibs/len(val_loader),int_bin_nll = ibnll/len(val_loader), loss=lossval/len(val_loader), train_loss=train_loss/len(val_loader))\n",
    "                scheduler.step(lossval/len(val_loader))\n",
    "                if conc/len(val_loader) > conc_max:\n",
    "                    torch.save(odesurv.state_dict(), 'checkpoint.pt')\n",
    "                    \n",
    "    \n",
    "    dt_string = name+datetime.now().strftime(\"_date_%d_%m_%Y_time_%H_%M_%S\")\n",
    "    analysis = tune.run(train_model, name=dt_string, num_samples=1, stop={\"training_iteration\": 200}, config=search_space, resources_per_trial={\"cpu\": 1, \"gpu\": 0.33}, verbose=1, raise_on_failed_trial=False)\n",
    "#     analysis = tune.run(train_model, name=dt_string, num_samples=300, scheduler=ASHAScheduler(metric=\"loss\", mode=\"min\"), config=search_space, resources_per_trial={\"cpu\": 1, \"gpu\": 0.25}, verbose=1, raise_on_failed_trial=False)\n",
    "#     analysis = tune.run(train_model, name=dt_string, num_samples=50, scheduler=ASHAScheduler(metric=\"concordance\", mode=\"max\", max_t=80, grace_period=20, reduction_factor=2, brackets=3), config=search_space, resources_per_trial={\"cpu\": 1, \"gpu\": 0.25}, verbose=1, raise_on_failed_trial=False)\n",
    "    best_config = analysis.get_best_config(metric=\"concordance\", mode=\"max\")\n",
    "    logdir = analysis.get_best_logdir(\"concordance\", mode=\"max\")\n",
    "    \n",
    "    num_in = 7\n",
    "    num_latent = best_config[\"num_latent\"]\n",
    "    layers_encoder =  [best_config[\"encoder_neurons\"]]*best_config[\"num_encoder_layers\"]\n",
    "    dropout_encoder = [best_config[\"encoder_dropout\"]]*best_config[\"num_encoder_layers\"]\n",
    "    layers_odefunc =  [best_config[\"odefunc_neurons\"]]*best_config[\"num_odefunc_layers\"]\n",
    "    dropout_odefunc = []\n",
    "\n",
    "    trans_matrix = torch.tensor([[np.nan,1],[np.nan,np.nan]]).to(device)\n",
    "\n",
    "    encoder = Encoder(num_in,num_latent,layers_encoder, dropout_encoder).to(device)\n",
    "    odefunc = ODEFunc(trans_matrix,num_in,num_latent,layers_odefunc,dropout_odefunc,best_config[\"softplus_beta\"]).to(device)\n",
    "    block = ODEBlock(odefunc).to(device)\n",
    "    odesurv = ODEsurv(block,encoder).to(device)\n",
    "    \n",
    "    odesurv.load_state_dict(torch.load(os.path.join(logdir, \"checkpoint.pt\")))\n",
    "\n",
    "    loader = make_dataloader(df_test,Tmax/best_config[\"multiplier\"],len(df_test))\n",
    "    odesurv.eval()\n",
    "    with torch.no_grad():\n",
    "        conc,ibs,ibnll = 0., 0., 0.\n",
    "        for _,ds in enumerate(loader):\n",
    "            t1,t2,t3 = measures(odesurv,torch.tensor([1.,0.],device=device),Tmax,*ds,multiplier=best_config[\"multiplier\"])\n",
    "            conc += t1\n",
    "            ibs += t2\n",
    "            ibnll += t3\n",
    "    return conc/len(loader), ibs/len(loader), ibnll/len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 5.3/59.0 GiB<br>Using AsyncHyperBand: num_stopped=300\n",
       "Bracket: Iter 64.000: -0.5211372673511505 | Iter 16.000: -0.5307373255491257 | Iter 4.000: -0.5411171615123749 | Iter 1.000: -0.5509990602731705<br>Resources requested: 0/16 CPUs, 0.0/4 GPUs, 0.0/33.11 GiB heap, 0.0/11.43 GiB objects<br>Result logdir: /home/jupyter/ray_results/rot_date_14_05_2020_time_11_27_54<br>Number of trials: 300 (300 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name       </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">  num_latent</th><th style=\"text-align: right;\">  num_odefunc_layers</th><th style=\"text-align: right;\">  odefunc_neurons</th><th style=\"text-align: right;\">  scheduler_gamma</th><th style=\"text-align: right;\">  weight_decay</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_model_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000743329</td><td style=\"text-align: right;\">          19</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              818</td><td style=\"text-align: right;\">         0.862176</td><td style=\"text-align: right;\">   3.69822e-07</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       3934.91  </td></tr>\n",
       "<tr><td>train_model_00001</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000511077</td><td style=\"text-align: right;\">         111</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              989</td><td style=\"text-align: right;\">         0.595397</td><td style=\"text-align: right;\">   0.00495276 </td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        771.29  </td></tr>\n",
       "<tr><td>train_model_00002</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000203526</td><td style=\"text-align: right;\">         136</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              233</td><td style=\"text-align: right;\">         0.713797</td><td style=\"text-align: right;\">   1.86807e-07</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        125.353 </td></tr>\n",
       "<tr><td>train_model_00003</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000209287</td><td style=\"text-align: right;\">         134</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              508</td><td style=\"text-align: right;\">         0.89779 </td><td style=\"text-align: right;\">   9.93566e-07</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        123.428 </td></tr>\n",
       "<tr><td>train_model_00004</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000722869</td><td style=\"text-align: right;\">          25</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              547</td><td style=\"text-align: right;\">         0.603554</td><td style=\"text-align: right;\">   1.28015e-06</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       4216.59  </td></tr>\n",
       "<tr><td>train_model_00005</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000790787</td><td style=\"text-align: right;\">         145</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              917</td><td style=\"text-align: right;\">         0.936546</td><td style=\"text-align: right;\">   1.20681e-07</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        255.307 </td></tr>\n",
       "<tr><td>train_model_00006</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">9.29906e-05</td><td style=\"text-align: right;\">         107</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              198</td><td style=\"text-align: right;\">         0.260943</td><td style=\"text-align: right;\">   3.31935e-07</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        105.203 </td></tr>\n",
       "<tr><td>train_model_00007</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000123262</td><td style=\"text-align: right;\">          26</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              436</td><td style=\"text-align: right;\">         0.828129</td><td style=\"text-align: right;\">   1.01854e-06</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         99.3676</td></tr>\n",
       "<tr><td>train_model_00008</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">8.4564e-05 </td><td style=\"text-align: right;\">         114</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              388</td><td style=\"text-align: right;\">         0.587675</td><td style=\"text-align: right;\">   2.52835e-06</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        114.197 </td></tr>\n",
       "<tr><td>train_model_00009</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">9.5973e-05 </td><td style=\"text-align: right;\">          61</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              800</td><td style=\"text-align: right;\">         0.355959</td><td style=\"text-align: right;\">   1.65136e-06</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        111.309 </td></tr>\n",
       "<tr><td>train_model_00010</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">6.5581e-05 </td><td style=\"text-align: right;\">          77</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              250</td><td style=\"text-align: right;\">         0.66071 </td><td style=\"text-align: right;\">   0.00899257 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        113.645 </td></tr>\n",
       "<tr><td>train_model_00011</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000145609</td><td style=\"text-align: right;\">         102</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              929</td><td style=\"text-align: right;\">         0.200475</td><td style=\"text-align: right;\">   2.60851e-06</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        237.432 </td></tr>\n",
       "<tr><td>train_model_00012</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000669313</td><td style=\"text-align: right;\">          14</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              486</td><td style=\"text-align: right;\">         0.531647</td><td style=\"text-align: right;\">   1.38946e-06</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        101.761 </td></tr>\n",
       "<tr><td>train_model_00013</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">7.10543e-05</td><td style=\"text-align: right;\">          89</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              430</td><td style=\"text-align: right;\">         0.805835</td><td style=\"text-align: right;\">   2.3939e-05 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        104.147 </td></tr>\n",
       "<tr><td>train_model_00014</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000245526</td><td style=\"text-align: right;\">         109</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              514</td><td style=\"text-align: right;\">         0.811964</td><td style=\"text-align: right;\">   0.000137915</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        103.788 </td></tr>\n",
       "<tr><td>train_model_00015</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000166704</td><td style=\"text-align: right;\">          82</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              747</td><td style=\"text-align: right;\">         0.833007</td><td style=\"text-align: right;\">   1.80141e-07</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        102.441 </td></tr>\n",
       "<tr><td>train_model_00016</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000784478</td><td style=\"text-align: right;\">          81</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              729</td><td style=\"text-align: right;\">         0.688269</td><td style=\"text-align: right;\">   1.11179e-07</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        230.195 </td></tr>\n",
       "<tr><td>train_model_00017</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">9.26886e-05</td><td style=\"text-align: right;\">          81</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              290</td><td style=\"text-align: right;\">         0.427967</td><td style=\"text-align: right;\">   0.000215321</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         95.9127</td></tr>\n",
       "<tr><td>train_model_00018</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000802198</td><td style=\"text-align: right;\">          29</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              358</td><td style=\"text-align: right;\">         0.865526</td><td style=\"text-align: right;\">   4.47184e-07</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         98.4686</td></tr>\n",
       "<tr><td>train_model_00019</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000166571</td><td style=\"text-align: right;\">         141</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              562</td><td style=\"text-align: right;\">         0.612715</td><td style=\"text-align: right;\">   0.000163692</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         98.9613</td></tr>\n",
       "<tr><td>train_model_00020</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000142944</td><td style=\"text-align: right;\">          31</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              889</td><td style=\"text-align: right;\">         0.68749 </td><td style=\"text-align: right;\">   0.00211669 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         96.6351</td></tr>\n",
       "<tr><td>train_model_00021</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000102134</td><td style=\"text-align: right;\">          67</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              635</td><td style=\"text-align: right;\">         0.895201</td><td style=\"text-align: right;\">   1.6503e-06 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         99.3217</td></tr>\n",
       "<tr><td>train_model_00022</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000102341</td><td style=\"text-align: right;\">          45</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              321</td><td style=\"text-align: right;\">         0.929054</td><td style=\"text-align: right;\">   4.42791e-06</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        100.529 </td></tr>\n",
       "<tr><td>train_model_00023</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000218713</td><td style=\"text-align: right;\">         116</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              255</td><td style=\"text-align: right;\">         0.292248</td><td style=\"text-align: right;\">   1.20901e-05</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        106.091 </td></tr>\n",
       "<tr><td>train_model_00024</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000263621</td><td style=\"text-align: right;\">          49</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              676</td><td style=\"text-align: right;\">         0.625068</td><td style=\"text-align: right;\">   0.000854091</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        246.231 </td></tr>\n",
       "<tr><td>train_model_00025</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">6.31626e-05</td><td style=\"text-align: right;\">          20</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              402</td><td style=\"text-align: right;\">         0.883451</td><td style=\"text-align: right;\">   1.80864e-05</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         82.6727</td></tr>\n",
       "<tr><td>train_model_00026</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000374284</td><td style=\"text-align: right;\">          56</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              511</td><td style=\"text-align: right;\">         0.471445</td><td style=\"text-align: right;\">   0.000349695</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        202.063 </td></tr>\n",
       "<tr><td>train_model_00027</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.0009153  </td><td style=\"text-align: right;\">          55</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              793</td><td style=\"text-align: right;\">         0.318237</td><td style=\"text-align: right;\">   1.12596e-06</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         98.0227</td></tr>\n",
       "<tr><td>train_model_00028</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">8.15985e-05</td><td style=\"text-align: right;\">         125</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              330</td><td style=\"text-align: right;\">         0.962355</td><td style=\"text-align: right;\">   3.97224e-07</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        111.312 </td></tr>\n",
       "<tr><td>train_model_00029</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000190077</td><td style=\"text-align: right;\">          68</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              382</td><td style=\"text-align: right;\">         0.641497</td><td style=\"text-align: right;\">   3.55393e-06</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        110.201 </td></tr>\n",
       "<tr><td>train_model_00030</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000832365</td><td style=\"text-align: right;\">         112</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              171</td><td style=\"text-align: right;\">         0.64617 </td><td style=\"text-align: right;\">   5.82381e-06</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       4637.83  </td></tr>\n",
       "<tr><td>train_model_00031</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">5.15592e-05</td><td style=\"text-align: right;\">         119</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              558</td><td style=\"text-align: right;\">         0.953367</td><td style=\"text-align: right;\">   5.8818e-07 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        101.095 </td></tr>\n",
       "<tr><td>train_model_00032</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">9.95222e-05</td><td style=\"text-align: right;\">          25</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              886</td><td style=\"text-align: right;\">         0.202056</td><td style=\"text-align: right;\">   0.000247523</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         82.3384</td></tr>\n",
       "<tr><td>train_model_00033</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000260233</td><td style=\"text-align: right;\">         105</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              764</td><td style=\"text-align: right;\">         0.107391</td><td style=\"text-align: right;\">   7.02888e-07</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        265.428 </td></tr>\n",
       "<tr><td>train_model_00034</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000796084</td><td style=\"text-align: right;\">         116</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              995</td><td style=\"text-align: right;\">         0.936118</td><td style=\"text-align: right;\">   0.00470406 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        114.317 </td></tr>\n",
       "<tr><td>train_model_00035</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000966926</td><td style=\"text-align: right;\">         140</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              725</td><td style=\"text-align: right;\">         0.579119</td><td style=\"text-align: right;\">   2.73023e-05</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        266.206 </td></tr>\n",
       "<tr><td>train_model_00036</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000753405</td><td style=\"text-align: right;\">          19</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              754</td><td style=\"text-align: right;\">         0.485981</td><td style=\"text-align: right;\">   5.83107e-07</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         75.2222</td></tr>\n",
       "<tr><td>train_model_00037</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">6.43145e-05</td><td style=\"text-align: right;\">         115</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              183</td><td style=\"text-align: right;\">         0.276644</td><td style=\"text-align: right;\">   1.97616e-05</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        108.939 </td></tr>\n",
       "<tr><td>train_model_00038</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">7.02875e-05</td><td style=\"text-align: right;\">          40</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              533</td><td style=\"text-align: right;\">         0.156084</td><td style=\"text-align: right;\">   0.00021253 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         84.118 </td></tr>\n",
       "<tr><td>train_model_00039</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000567138</td><td style=\"text-align: right;\">          37</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              865</td><td style=\"text-align: right;\">         0.194228</td><td style=\"text-align: right;\">   0.000186285</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        169.99  </td></tr>\n",
       "<tr><td>train_model_00040</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000151515</td><td style=\"text-align: right;\">         126</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              783</td><td style=\"text-align: right;\">         0.146335</td><td style=\"text-align: right;\">   1.61376e-07</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        297.116 </td></tr>\n",
       "<tr><td>train_model_00041</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000184698</td><td style=\"text-align: right;\">          53</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              369</td><td style=\"text-align: right;\">         0.916093</td><td style=\"text-align: right;\">   5.73935e-07</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        103.715 </td></tr>\n",
       "<tr><td>train_model_00042</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">9.75909e-05</td><td style=\"text-align: right;\">         106</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              805</td><td style=\"text-align: right;\">         0.710817</td><td style=\"text-align: right;\">   8.4303e-07 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        108.851 </td></tr>\n",
       "<tr><td>train_model_00043</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">6.45084e-05</td><td style=\"text-align: right;\">          89</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              871</td><td style=\"text-align: right;\">         0.160046</td><td style=\"text-align: right;\">   0.000459918</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         85.1916</td></tr>\n",
       "<tr><td>train_model_00044</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">5.66812e-05</td><td style=\"text-align: right;\">          55</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              554</td><td style=\"text-align: right;\">         0.4341  </td><td style=\"text-align: right;\">   2.92861e-06</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         84.7941</td></tr>\n",
       "<tr><td>train_model_00045</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.00015657 </td><td style=\"text-align: right;\">          24</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              675</td><td style=\"text-align: right;\">         0.504155</td><td style=\"text-align: right;\">   0.00536878 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         91.8517</td></tr>\n",
       "<tr><td>train_model_00046</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">5.43693e-05</td><td style=\"text-align: right;\">          99</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              266</td><td style=\"text-align: right;\">         0.892277</td><td style=\"text-align: right;\">   9.55128e-05</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        116.214 </td></tr>\n",
       "<tr><td>train_model_00047</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">5.28873e-05</td><td style=\"text-align: right;\">          73</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              755</td><td style=\"text-align: right;\">         0.159485</td><td style=\"text-align: right;\">   6.98492e-06</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        111.563 </td></tr>\n",
       "<tr><td>train_model_00048</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000948185</td><td style=\"text-align: right;\">         125</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              770</td><td style=\"text-align: right;\">         0.336266</td><td style=\"text-align: right;\">   0.00049361 </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        245.558 </td></tr>\n",
       "<tr><td>train_model_00049</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000780958</td><td style=\"text-align: right;\">          93</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              736</td><td style=\"text-align: right;\">         0.867015</td><td style=\"text-align: right;\">   0.000256212</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        714.005 </td></tr>\n",
       "<tr><td>train_model_00050</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000102768</td><td style=\"text-align: right;\">          38</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              529</td><td style=\"text-align: right;\">         0.455111</td><td style=\"text-align: right;\">   0.00701215 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         80.1407</td></tr>\n",
       "<tr><td>train_model_00051</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000695643</td><td style=\"text-align: right;\">         138</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              952</td><td style=\"text-align: right;\">         0.566539</td><td style=\"text-align: right;\">   0.00132203 </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        313.486 </td></tr>\n",
       "<tr><td>train_model_00052</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000168738</td><td style=\"text-align: right;\">          45</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              636</td><td style=\"text-align: right;\">         0.222301</td><td style=\"text-align: right;\">   0.00665878 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         90.8608</td></tr>\n",
       "<tr><td>train_model_00053</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">6.57656e-05</td><td style=\"text-align: right;\">          17</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              396</td><td style=\"text-align: right;\">         0.322469</td><td style=\"text-align: right;\">   0.00325732 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         85.0495</td></tr>\n",
       "<tr><td>train_model_00054</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000206152</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              680</td><td style=\"text-align: right;\">         0.739313</td><td style=\"text-align: right;\">   4.02818e-07</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         79.9966</td></tr>\n",
       "<tr><td>train_model_00055</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000916297</td><td style=\"text-align: right;\">          65</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              267</td><td style=\"text-align: right;\">         0.653819</td><td style=\"text-align: right;\">   1.76315e-06</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        237.79  </td></tr>\n",
       "<tr><td>train_model_00056</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000164673</td><td style=\"text-align: right;\">          11</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              621</td><td style=\"text-align: right;\">         0.454808</td><td style=\"text-align: right;\">   2.56928e-07</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        180.516 </td></tr>\n",
       "<tr><td>train_model_00057</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000240166</td><td style=\"text-align: right;\">         121</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              485</td><td style=\"text-align: right;\">         0.104197</td><td style=\"text-align: right;\">   0.00469434 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        124.57  </td></tr>\n",
       "<tr><td>train_model_00058</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000339834</td><td style=\"text-align: right;\">          87</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              815</td><td style=\"text-align: right;\">         0.725493</td><td style=\"text-align: right;\">   0.000162013</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        216.202 </td></tr>\n",
       "<tr><td>train_model_00059</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000284354</td><td style=\"text-align: right;\">         126</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              900</td><td style=\"text-align: right;\">         0.529848</td><td style=\"text-align: right;\">   5.34229e-05</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        241.695 </td></tr>\n",
       "<tr><td>train_model_00060</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">9.91146e-05</td><td style=\"text-align: right;\">          54</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              564</td><td style=\"text-align: right;\">         0.170264</td><td style=\"text-align: right;\">   1.13081e-06</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        103.313 </td></tr>\n",
       "<tr><td>train_model_00061</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">8.48975e-05</td><td style=\"text-align: right;\">         133</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              925</td><td style=\"text-align: right;\">         0.481593</td><td style=\"text-align: right;\">   2.36981e-06</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        114.902 </td></tr>\n",
       "<tr><td>train_model_00062</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000163198</td><td style=\"text-align: right;\">          35</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              529</td><td style=\"text-align: right;\">         0.756043</td><td style=\"text-align: right;\">   3.45563e-05</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        102.629 </td></tr>\n",
       "<tr><td>train_model_00063</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000194372</td><td style=\"text-align: right;\">          66</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              237</td><td style=\"text-align: right;\">         0.414161</td><td style=\"text-align: right;\">   2.35291e-07</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        119.413 </td></tr>\n",
       "<tr><td>train_model_00064</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">6.18065e-05</td><td style=\"text-align: right;\">         113</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              423</td><td style=\"text-align: right;\">         0.812108</td><td style=\"text-align: right;\">   4.51571e-07</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        117.588 </td></tr>\n",
       "<tr><td>train_model_00065</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">9.47949e-05</td><td style=\"text-align: right;\">          70</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              657</td><td style=\"text-align: right;\">         0.950383</td><td style=\"text-align: right;\">   0.000140084</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         91.2469</td></tr>\n",
       "<tr><td>train_model_00066</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000764112</td><td style=\"text-align: right;\">          66</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              522</td><td style=\"text-align: right;\">         0.490314</td><td style=\"text-align: right;\">   0.000406852</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        223.206 </td></tr>\n",
       "<tr><td>train_model_00067</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">7.1201e-05 </td><td style=\"text-align: right;\">         116</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              232</td><td style=\"text-align: right;\">         0.960421</td><td style=\"text-align: right;\">   0.000575094</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        104.637 </td></tr>\n",
       "<tr><td>train_model_00068</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000458532</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              220</td><td style=\"text-align: right;\">         0.323994</td><td style=\"text-align: right;\">   2.29019e-06</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        114.909 </td></tr>\n",
       "<tr><td>train_model_00069</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000788881</td><td style=\"text-align: right;\">         104</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              634</td><td style=\"text-align: right;\">         0.900797</td><td style=\"text-align: right;\">   4.22662e-05</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        100.641 </td></tr>\n",
       "<tr><td>train_model_00070</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">9.04596e-05</td><td style=\"text-align: right;\">         131</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              905</td><td style=\"text-align: right;\">         0.589861</td><td style=\"text-align: right;\">   1.68162e-06</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        118.995 </td></tr>\n",
       "<tr><td>train_model_00071</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000195001</td><td style=\"text-align: right;\">          17</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              861</td><td style=\"text-align: right;\">         0.633996</td><td style=\"text-align: right;\">   4.14066e-07</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         78.0992</td></tr>\n",
       "<tr><td>train_model_00072</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000277515</td><td style=\"text-align: right;\">          15</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              595</td><td style=\"text-align: right;\">         0.403335</td><td style=\"text-align: right;\">   0.000333994</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        166.105 </td></tr>\n",
       "<tr><td>train_model_00073</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000359143</td><td style=\"text-align: right;\">          14</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              168</td><td style=\"text-align: right;\">         0.424242</td><td style=\"text-align: right;\">   1.05316e-05</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         81.0868</td></tr>\n",
       "<tr><td>train_model_00074</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">7.76346e-05</td><td style=\"text-align: right;\">          62</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              472</td><td style=\"text-align: right;\">         0.526921</td><td style=\"text-align: right;\">   1.30043e-05</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         96.9098</td></tr>\n",
       "<tr><td>train_model_00075</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.00059498 </td><td style=\"text-align: right;\">         106</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              999</td><td style=\"text-align: right;\">         0.329803</td><td style=\"text-align: right;\">   2.76112e-07</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        233.124 </td></tr>\n",
       "<tr><td>train_model_00076</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">9.328e-05  </td><td style=\"text-align: right;\">         109</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              440</td><td style=\"text-align: right;\">         0.635916</td><td style=\"text-align: right;\">   4.50973e-05</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        100.014 </td></tr>\n",
       "<tr><td>train_model_00077</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000158327</td><td style=\"text-align: right;\">         144</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              977</td><td style=\"text-align: right;\">         0.201273</td><td style=\"text-align: right;\">   1.41284e-06</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         89.4998</td></tr>\n",
       "<tr><td>train_model_00078</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000124288</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              758</td><td style=\"text-align: right;\">         0.177929</td><td style=\"text-align: right;\">   0.000184469</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         84.31  </td></tr>\n",
       "<tr><td>train_model_00079</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000104489</td><td style=\"text-align: right;\">          12</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              185</td><td style=\"text-align: right;\">         0.698186</td><td style=\"text-align: right;\">   0.000104721</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         83.5076</td></tr>\n",
       "<tr><td>train_model_00080</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000885509</td><td style=\"text-align: right;\">         106</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              633</td><td style=\"text-align: right;\">         0.752383</td><td style=\"text-align: right;\">   2.92913e-06</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        229.516 </td></tr>\n",
       "<tr><td>train_model_00081</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">6.96758e-05</td><td style=\"text-align: right;\">         123</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              302</td><td style=\"text-align: right;\">         0.466148</td><td style=\"text-align: right;\">   3.62707e-06</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        110.84  </td></tr>\n",
       "<tr><td>train_model_00082</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">9.30274e-05</td><td style=\"text-align: right;\">          72</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              291</td><td style=\"text-align: right;\">         0.700025</td><td style=\"text-align: right;\">   3.07047e-07</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        102.618 </td></tr>\n",
       "<tr><td>train_model_00083</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000679624</td><td style=\"text-align: right;\">         108</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              212</td><td style=\"text-align: right;\">         0.136709</td><td style=\"text-align: right;\">   1.01103e-06</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        275.526 </td></tr>\n",
       "<tr><td>train_model_00084</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000387425</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              849</td><td style=\"text-align: right;\">         0.989861</td><td style=\"text-align: right;\">   0.00354374 </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        203.34  </td></tr>\n",
       "<tr><td>train_model_00085</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000111775</td><td style=\"text-align: right;\">          24</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              249</td><td style=\"text-align: right;\">         0.217843</td><td style=\"text-align: right;\">   3.15027e-07</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         72.9338</td></tr>\n",
       "<tr><td>train_model_00086</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000401629</td><td style=\"text-align: right;\">          60</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              963</td><td style=\"text-align: right;\">         0.112286</td><td style=\"text-align: right;\">   1.77438e-06</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        600.657 </td></tr>\n",
       "<tr><td>train_model_00087</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000111385</td><td style=\"text-align: right;\">          11</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              854</td><td style=\"text-align: right;\">         0.776852</td><td style=\"text-align: right;\">   7.57463e-05</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         86.6755</td></tr>\n",
       "<tr><td>train_model_00088</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000183786</td><td style=\"text-align: right;\">         115</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              475</td><td style=\"text-align: right;\">         0.371561</td><td style=\"text-align: right;\">   0.00022006 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        118.704 </td></tr>\n",
       "<tr><td>train_model_00089</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000396276</td><td style=\"text-align: right;\">          22</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              757</td><td style=\"text-align: right;\">         0.601499</td><td style=\"text-align: right;\">   1.33058e-07</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        101.865 </td></tr>\n",
       "<tr><td>train_model_00090</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000963952</td><td style=\"text-align: right;\">         100</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              124</td><td style=\"text-align: right;\">         0.508504</td><td style=\"text-align: right;\">   2.95597e-06</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         91.8253</td></tr>\n",
       "<tr><td>train_model_00091</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000122098</td><td style=\"text-align: right;\">         120</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              896</td><td style=\"text-align: right;\">         0.804263</td><td style=\"text-align: right;\">   0.000101178</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        276.002 </td></tr>\n",
       "<tr><td>train_model_00092</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.00051037 </td><td style=\"text-align: right;\">         134</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              942</td><td style=\"text-align: right;\">         0.500791</td><td style=\"text-align: right;\">   0.000288585</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        133.066 </td></tr>\n",
       "<tr><td>train_model_00093</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000114133</td><td style=\"text-align: right;\">         104</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              356</td><td style=\"text-align: right;\">         0.402795</td><td style=\"text-align: right;\">   4.63853e-07</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         95.7137</td></tr>\n",
       "<tr><td>train_model_00094</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.00028922 </td><td style=\"text-align: right;\">         149</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              555</td><td style=\"text-align: right;\">         0.512377</td><td style=\"text-align: right;\">   2.6131e-06 </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        289.088 </td></tr>\n",
       "<tr><td>train_model_00095</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000241009</td><td style=\"text-align: right;\">         136</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              785</td><td style=\"text-align: right;\">         0.940275</td><td style=\"text-align: right;\">   2.22371e-07</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        282.275 </td></tr>\n",
       "<tr><td>train_model_00096</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000761806</td><td style=\"text-align: right;\">         122</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              177</td><td style=\"text-align: right;\">         0.744301</td><td style=\"text-align: right;\">   1.70964e-05</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        139.204 </td></tr>\n",
       "<tr><td>train_model_00097</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">8.67983e-05</td><td style=\"text-align: right;\">         127</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              346</td><td style=\"text-align: right;\">         0.919396</td><td style=\"text-align: right;\">   3.13853e-05</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        143.718 </td></tr>\n",
       "<tr><td>train_model_00098</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000530558</td><td style=\"text-align: right;\">          33</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              729</td><td style=\"text-align: right;\">         0.845561</td><td style=\"text-align: right;\">   4.53112e-06</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        588.162 </td></tr>\n",
       "<tr><td>train_model_00099</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000937896</td><td style=\"text-align: right;\">          83</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              355</td><td style=\"text-align: right;\">         0.395341</td><td style=\"text-align: right;\">   0.00825227 </td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        745.747 </td></tr>\n",
       "<tr><td>train_model_00100</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000312761</td><td style=\"text-align: right;\">          45</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              164</td><td style=\"text-align: right;\">         0.503714</td><td style=\"text-align: right;\">   0.000238939</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         77.5539</td></tr>\n",
       "<tr><td>train_model_00101</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000181842</td><td style=\"text-align: right;\">          66</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              354</td><td style=\"text-align: right;\">         0.543209</td><td style=\"text-align: right;\">   1.77035e-07</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         91.703 </td></tr>\n",
       "<tr><td>train_model_00102</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000367322</td><td style=\"text-align: right;\">          25</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              298</td><td style=\"text-align: right;\">         0.960954</td><td style=\"text-align: right;\">   0.00299519 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        109.102 </td></tr>\n",
       "<tr><td>train_model_00103</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">6.20469e-05</td><td style=\"text-align: right;\">          22</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              992</td><td style=\"text-align: right;\">         0.938471</td><td style=\"text-align: right;\">   7.26619e-07</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         69.5348</td></tr>\n",
       "<tr><td>train_model_00104</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">7.11086e-05</td><td style=\"text-align: right;\">          76</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              508</td><td style=\"text-align: right;\">         0.162466</td><td style=\"text-align: right;\">   1.29464e-06</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        109.444 </td></tr>\n",
       "<tr><td>train_model_00105</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000188028</td><td style=\"text-align: right;\">          79</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              126</td><td style=\"text-align: right;\">         0.510235</td><td style=\"text-align: right;\">   0.00010879 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        106.06  </td></tr>\n",
       "<tr><td>train_model_00106</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000912082</td><td style=\"text-align: right;\">          22</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              202</td><td style=\"text-align: right;\">         0.23973 </td><td style=\"text-align: right;\">   0.00239018 </td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        613.339 </td></tr>\n",
       "<tr><td>train_model_00107</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000488191</td><td style=\"text-align: right;\">          80</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              223</td><td style=\"text-align: right;\">         0.586492</td><td style=\"text-align: right;\">   1.39982e-05</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         97.4244</td></tr>\n",
       "<tr><td>train_model_00108</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">6.00306e-05</td><td style=\"text-align: right;\">         107</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              236</td><td style=\"text-align: right;\">         0.282057</td><td style=\"text-align: right;\">   1.21966e-07</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         99.532 </td></tr>\n",
       "<tr><td>train_model_00109</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">7.61572e-05</td><td style=\"text-align: right;\">          67</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              884</td><td style=\"text-align: right;\">         0.94072 </td><td style=\"text-align: right;\">   7.11824e-06</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        103.294 </td></tr>\n",
       "<tr><td>train_model_00110</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000240539</td><td style=\"text-align: right;\">         105</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              292</td><td style=\"text-align: right;\">         0.176825</td><td style=\"text-align: right;\">   5.87423e-07</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         90.3415</td></tr>\n",
       "<tr><td>train_model_00111</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">8.945e-05  </td><td style=\"text-align: right;\">         112</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              927</td><td style=\"text-align: right;\">         0.561875</td><td style=\"text-align: right;\">   7.65662e-07</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        116.859 </td></tr>\n",
       "<tr><td>train_model_00112</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000322027</td><td style=\"text-align: right;\">          52</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              184</td><td style=\"text-align: right;\">         0.37004 </td><td style=\"text-align: right;\">   0.000201622</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        101.348 </td></tr>\n",
       "<tr><td>train_model_00113</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000868387</td><td style=\"text-align: right;\">         124</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              324</td><td style=\"text-align: right;\">         0.695244</td><td style=\"text-align: right;\">   0.00620189 </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        236.649 </td></tr>\n",
       "<tr><td>train_model_00114</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">7.03449e-05</td><td style=\"text-align: right;\">         107</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              835</td><td style=\"text-align: right;\">         0.769314</td><td style=\"text-align: right;\">   1.46363e-06</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        103.553 </td></tr>\n",
       "<tr><td>train_model_00115</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">7.48142e-05</td><td style=\"text-align: right;\">          37</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              522</td><td style=\"text-align: right;\">         0.826214</td><td style=\"text-align: right;\">   2.69007e-05</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         96.5465</td></tr>\n",
       "<tr><td>train_model_00116</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000316134</td><td style=\"text-align: right;\">         131</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              316</td><td style=\"text-align: right;\">         0.672609</td><td style=\"text-align: right;\">   0.000206157</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        116.031 </td></tr>\n",
       "<tr><td>train_model_00117</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000238899</td><td style=\"text-align: right;\">         130</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              646</td><td style=\"text-align: right;\">         0.678946</td><td style=\"text-align: right;\">   0.00868935 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        108.657 </td></tr>\n",
       "<tr><td>train_model_00118</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000180581</td><td style=\"text-align: right;\">          82</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              479</td><td style=\"text-align: right;\">         0.642634</td><td style=\"text-align: right;\">   0.00562297 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         83.0772</td></tr>\n",
       "<tr><td>train_model_00119</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000847019</td><td style=\"text-align: right;\">         105</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              983</td><td style=\"text-align: right;\">         0.895779</td><td style=\"text-align: right;\">   0.00320265 </td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       3903.5   </td></tr>\n",
       "<tr><td>train_model_00120</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000554848</td><td style=\"text-align: right;\">         101</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              100</td><td style=\"text-align: right;\">         0.458661</td><td style=\"text-align: right;\">   2.66506e-07</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         95.055 </td></tr>\n",
       "<tr><td>train_model_00121</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000464925</td><td style=\"text-align: right;\">          33</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              796</td><td style=\"text-align: right;\">         0.798231</td><td style=\"text-align: right;\">   3.48112e-06</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        214.657 </td></tr>\n",
       "<tr><td>train_model_00122</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000809573</td><td style=\"text-align: right;\">          29</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              164</td><td style=\"text-align: right;\">         0.131485</td><td style=\"text-align: right;\">   2.65389e-06</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">       2827.64  </td></tr>\n",
       "<tr><td>train_model_00123</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.00011802 </td><td style=\"text-align: right;\">          34</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              404</td><td style=\"text-align: right;\">         0.527062</td><td style=\"text-align: right;\">   0.00078735 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         83.0929</td></tr>\n",
       "<tr><td>train_model_00124</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000364079</td><td style=\"text-align: right;\">          27</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              317</td><td style=\"text-align: right;\">         0.336235</td><td style=\"text-align: right;\">   0.000300818</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         75.5934</td></tr>\n",
       "<tr><td>train_model_00125</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000162544</td><td style=\"text-align: right;\">          25</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              600</td><td style=\"text-align: right;\">         0.797214</td><td style=\"text-align: right;\">   0.0012151  </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         89.8199</td></tr>\n",
       "<tr><td>train_model_00126</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">5.29886e-05</td><td style=\"text-align: right;\">         123</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              884</td><td style=\"text-align: right;\">         0.234062</td><td style=\"text-align: right;\">   0.00104823 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         96.2855</td></tr>\n",
       "<tr><td>train_model_00127</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000392886</td><td style=\"text-align: right;\">          57</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              346</td><td style=\"text-align: right;\">         0.701428</td><td style=\"text-align: right;\">   0.000986034</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        182.694 </td></tr>\n",
       "<tr><td>train_model_00128</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000222515</td><td style=\"text-align: right;\">          62</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              315</td><td style=\"text-align: right;\">         0.381145</td><td style=\"text-align: right;\">   0.000228824</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         92.2866</td></tr>\n",
       "<tr><td>train_model_00129</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000174137</td><td style=\"text-align: right;\">          14</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              856</td><td style=\"text-align: right;\">         0.558556</td><td style=\"text-align: right;\">   7.29831e-06</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         61.9119</td></tr>\n",
       "<tr><td>train_model_00130</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000735581</td><td style=\"text-align: right;\">          93</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              844</td><td style=\"text-align: right;\">         0.954613</td><td style=\"text-align: right;\">   0.000110135</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        214.598 </td></tr>\n",
       "<tr><td>train_model_00131</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000212226</td><td style=\"text-align: right;\">         140</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              683</td><td style=\"text-align: right;\">         0.925799</td><td style=\"text-align: right;\">   1.26144e-07</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        242.97  </td></tr>\n",
       "<tr><td>train_model_00132</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000104005</td><td style=\"text-align: right;\">          23</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              949</td><td style=\"text-align: right;\">         0.143256</td><td style=\"text-align: right;\">   0.000498267</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         87.2789</td></tr>\n",
       "<tr><td>train_model_00133</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000653688</td><td style=\"text-align: right;\">         101</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              340</td><td style=\"text-align: right;\">         0.546689</td><td style=\"text-align: right;\">   0.000165586</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         92.9794</td></tr>\n",
       "<tr><td>train_model_00134</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000274205</td><td style=\"text-align: right;\">         103</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              300</td><td style=\"text-align: right;\">         0.258224</td><td style=\"text-align: right;\">   0.00435914 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         95.768 </td></tr>\n",
       "<tr><td>train_model_00135</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000289097</td><td style=\"text-align: right;\">          49</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              462</td><td style=\"text-align: right;\">         0.607084</td><td style=\"text-align: right;\">   0.000392097</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         81.1574</td></tr>\n",
       "<tr><td>train_model_00136</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">5.07035e-05</td><td style=\"text-align: right;\">          53</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              164</td><td style=\"text-align: right;\">         0.373689</td><td style=\"text-align: right;\">   6.8091e-07 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         98.2709</td></tr>\n",
       "<tr><td>train_model_00137</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000232072</td><td style=\"text-align: right;\">          82</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              219</td><td style=\"text-align: right;\">         0.138083</td><td style=\"text-align: right;\">   1.67193e-07</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        107.255 </td></tr>\n",
       "<tr><td>train_model_00138</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000113344</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              176</td><td style=\"text-align: right;\">         0.446167</td><td style=\"text-align: right;\">   2.96771e-07</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         75.2807</td></tr>\n",
       "<tr><td>train_model_00139</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">6.19402e-05</td><td style=\"text-align: right;\">          97</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              869</td><td style=\"text-align: right;\">         0.401782</td><td style=\"text-align: right;\">   0.000675762</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         93.1085</td></tr>\n",
       "<tr><td>train_model_00140</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000771355</td><td style=\"text-align: right;\">         106</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              839</td><td style=\"text-align: right;\">         0.768853</td><td style=\"text-align: right;\">   1.28673e-05</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         83.2351</td></tr>\n",
       "<tr><td>train_model_00141</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000167186</td><td style=\"text-align: right;\">          92</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              448</td><td style=\"text-align: right;\">         0.398478</td><td style=\"text-align: right;\">   0.00042278 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         95.637 </td></tr>\n",
       "<tr><td>train_model_00142</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000126115</td><td style=\"text-align: right;\">          39</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              765</td><td style=\"text-align: right;\">         0.468672</td><td style=\"text-align: right;\">   0.000938794</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         76.9582</td></tr>\n",
       "<tr><td>train_model_00143</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000770319</td><td style=\"text-align: right;\">          27</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              260</td><td style=\"text-align: right;\">         0.76381 </td><td style=\"text-align: right;\">   0.00015022 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         77.907 </td></tr>\n",
       "<tr><td>train_model_00144</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000823037</td><td style=\"text-align: right;\">          24</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              634</td><td style=\"text-align: right;\">         0.114366</td><td style=\"text-align: right;\">   3.33323e-06</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        213.482 </td></tr>\n",
       "<tr><td>train_model_00145</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000197922</td><td style=\"text-align: right;\">          33</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              551</td><td style=\"text-align: right;\">         0.478998</td><td style=\"text-align: right;\">   0.000494397</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         87.3808</td></tr>\n",
       "<tr><td>train_model_00146</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">7.60147e-05</td><td style=\"text-align: right;\">          48</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              316</td><td style=\"text-align: right;\">         0.123685</td><td style=\"text-align: right;\">   0.00840901 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         86.6485</td></tr>\n",
       "<tr><td>train_model_00147</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000133826</td><td style=\"text-align: right;\">          69</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              408</td><td style=\"text-align: right;\">         0.536291</td><td style=\"text-align: right;\">   4.23021e-05</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         86.7115</td></tr>\n",
       "<tr><td>train_model_00148</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000324126</td><td style=\"text-align: right;\">          94</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              190</td><td style=\"text-align: right;\">         0.417422</td><td style=\"text-align: right;\">   2.06874e-06</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         82.3813</td></tr>\n",
       "<tr><td>train_model_00149</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000488101</td><td style=\"text-align: right;\">          20</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              367</td><td style=\"text-align: right;\">         0.48331 </td><td style=\"text-align: right;\">   5.25808e-06</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         79.1192</td></tr>\n",
       "<tr><td>train_model_00150</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000476509</td><td style=\"text-align: right;\">         112</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              186</td><td style=\"text-align: right;\">         0.456547</td><td style=\"text-align: right;\">   0.00100731 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         98.0964</td></tr>\n",
       "<tr><td>train_model_00151</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000859724</td><td style=\"text-align: right;\">         127</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              849</td><td style=\"text-align: right;\">         0.534498</td><td style=\"text-align: right;\">   1.50313e-06</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        101.755 </td></tr>\n",
       "<tr><td>train_model_00152</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000123869</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              924</td><td style=\"text-align: right;\">         0.894167</td><td style=\"text-align: right;\">   1.07584e-06</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        259.966 </td></tr>\n",
       "<tr><td>train_model_00153</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000228578</td><td style=\"text-align: right;\">         103</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              457</td><td style=\"text-align: right;\">         0.231591</td><td style=\"text-align: right;\">   0.000180507</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         99.1708</td></tr>\n",
       "<tr><td>train_model_00154</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000623261</td><td style=\"text-align: right;\">         119</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              937</td><td style=\"text-align: right;\">         0.909338</td><td style=\"text-align: right;\">   0.00119289 </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        275.095 </td></tr>\n",
       "<tr><td>train_model_00155</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000203993</td><td style=\"text-align: right;\">         147</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              405</td><td style=\"text-align: right;\">         0.45231 </td><td style=\"text-align: right;\">   0.000582715</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        121.621 </td></tr>\n",
       "<tr><td>train_model_00156</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000341967</td><td style=\"text-align: right;\">         112</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              189</td><td style=\"text-align: right;\">         0.68483 </td><td style=\"text-align: right;\">   0.00148958 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        103.196 </td></tr>\n",
       "<tr><td>train_model_00157</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">5.92771e-05</td><td style=\"text-align: right;\">          95</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              220</td><td style=\"text-align: right;\">         0.475234</td><td style=\"text-align: right;\">   3.21974e-07</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        104.252 </td></tr>\n",
       "<tr><td>train_model_00158</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">6.96279e-05</td><td style=\"text-align: right;\">         129</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              760</td><td style=\"text-align: right;\">         0.601498</td><td style=\"text-align: right;\">   5.2493e-06 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        129.038 </td></tr>\n",
       "<tr><td>train_model_00159</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000137676</td><td style=\"text-align: right;\">         140</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              917</td><td style=\"text-align: right;\">         0.578215</td><td style=\"text-align: right;\">   0.00397191 </td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        949.787 </td></tr>\n",
       "<tr><td>train_model_00160</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000328857</td><td style=\"text-align: right;\">         119</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              951</td><td style=\"text-align: right;\">         0.531221</td><td style=\"text-align: right;\">   8.07753e-07</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        118.455 </td></tr>\n",
       "<tr><td>train_model_00161</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000353727</td><td style=\"text-align: right;\">          78</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              764</td><td style=\"text-align: right;\">         0.79229 </td><td style=\"text-align: right;\">   1.15054e-07</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         84.5268</td></tr>\n",
       "<tr><td>train_model_00162</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">9.99611e-05</td><td style=\"text-align: right;\">          92</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              235</td><td style=\"text-align: right;\">         0.283211</td><td style=\"text-align: right;\">   2.84066e-07</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        120.919 </td></tr>\n",
       "<tr><td>train_model_00163</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000301167</td><td style=\"text-align: right;\">          25</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              330</td><td style=\"text-align: right;\">         0.641221</td><td style=\"text-align: right;\">   0.000248197</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         90.7693</td></tr>\n",
       "<tr><td>train_model_00164</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000126222</td><td style=\"text-align: right;\">          51</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              328</td><td style=\"text-align: right;\">         0.296135</td><td style=\"text-align: right;\">   6.87861e-05</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         74.9362</td></tr>\n",
       "<tr><td>train_model_00165</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">8.41348e-05</td><td style=\"text-align: right;\">          24</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              740</td><td style=\"text-align: right;\">         0.127483</td><td style=\"text-align: right;\">   2.2e-07    </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         73.644 </td></tr>\n",
       "<tr><td>train_model_00166</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000234365</td><td style=\"text-align: right;\">         124</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              129</td><td style=\"text-align: right;\">         0.195295</td><td style=\"text-align: right;\">   1.91974e-05</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        110.396 </td></tr>\n",
       "<tr><td>train_model_00167</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000830765</td><td style=\"text-align: right;\">          76</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              832</td><td style=\"text-align: right;\">         0.836162</td><td style=\"text-align: right;\">   0.000742227</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        217.877 </td></tr>\n",
       "<tr><td>train_model_00168</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">5.39204e-05</td><td style=\"text-align: right;\">         119</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              462</td><td style=\"text-align: right;\">         0.333507</td><td style=\"text-align: right;\">   0.00428777 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        126.406 </td></tr>\n",
       "<tr><td>train_model_00169</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000166889</td><td style=\"text-align: right;\">         135</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              179</td><td style=\"text-align: right;\">         0.811313</td><td style=\"text-align: right;\">   4.15437e-06</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        112.226 </td></tr>\n",
       "<tr><td>train_model_00170</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000136188</td><td style=\"text-align: right;\">         108</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              647</td><td style=\"text-align: right;\">         0.983199</td><td style=\"text-align: right;\">   8.5222e-06 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        110.552 </td></tr>\n",
       "<tr><td>train_model_00171</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000220998</td><td style=\"text-align: right;\">         125</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              704</td><td style=\"text-align: right;\">         0.24661 </td><td style=\"text-align: right;\">   1.13128e-06</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        113.581 </td></tr>\n",
       "<tr><td>train_model_00172</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000121434</td><td style=\"text-align: right;\">          72</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              948</td><td style=\"text-align: right;\">         0.85174 </td><td style=\"text-align: right;\">   8.00868e-06</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        245.163 </td></tr>\n",
       "<tr><td>train_model_00173</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000142599</td><td style=\"text-align: right;\">          94</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              305</td><td style=\"text-align: right;\">         0.550226</td><td style=\"text-align: right;\">   9.52765e-07</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        117.179 </td></tr>\n",
       "<tr><td>train_model_00174</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000403014</td><td style=\"text-align: right;\">         110</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              792</td><td style=\"text-align: right;\">         0.507447</td><td style=\"text-align: right;\">   0.00013323 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        120.585 </td></tr>\n",
       "<tr><td>train_model_00175</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000392732</td><td style=\"text-align: right;\">          58</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              341</td><td style=\"text-align: right;\">         0.958285</td><td style=\"text-align: right;\">   0.000744571</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        195.242 </td></tr>\n",
       "<tr><td>train_model_00176</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">7.55991e-05</td><td style=\"text-align: right;\">          86</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              211</td><td style=\"text-align: right;\">         0.174223</td><td style=\"text-align: right;\">   2.93967e-05</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        124.242 </td></tr>\n",
       "<tr><td>train_model_00177</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000106544</td><td style=\"text-align: right;\">          48</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              624</td><td style=\"text-align: right;\">         0.86869 </td><td style=\"text-align: right;\">   0.00237574 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         81.064 </td></tr>\n",
       "<tr><td>train_model_00178</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000382781</td><td style=\"text-align: right;\">          61</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              432</td><td style=\"text-align: right;\">         0.126729</td><td style=\"text-align: right;\">   1.78329e-07</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        721.685 </td></tr>\n",
       "<tr><td>train_model_00179</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000994107</td><td style=\"text-align: right;\">         136</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              848</td><td style=\"text-align: right;\">         0.597359</td><td style=\"text-align: right;\">   6.73411e-06</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        112.293 </td></tr>\n",
       "<tr><td>train_model_00180</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">8.84136e-05</td><td style=\"text-align: right;\">         111</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              520</td><td style=\"text-align: right;\">         0.504656</td><td style=\"text-align: right;\">   0.000651742</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        108.941 </td></tr>\n",
       "<tr><td>train_model_00181</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000518739</td><td style=\"text-align: right;\">          94</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              647</td><td style=\"text-align: right;\">         0.360647</td><td style=\"text-align: right;\">   3.62397e-05</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        113.124 </td></tr>\n",
       "<tr><td>train_model_00182</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000121953</td><td style=\"text-align: right;\">          95</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              501</td><td style=\"text-align: right;\">         0.466331</td><td style=\"text-align: right;\">   0.00485732 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        109.946 </td></tr>\n",
       "<tr><td>train_model_00183</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000905126</td><td style=\"text-align: right;\">          53</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              950</td><td style=\"text-align: right;\">         0.592416</td><td style=\"text-align: right;\">   7.0379e-07 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         96.5825</td></tr>\n",
       "<tr><td>train_model_00184</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000727089</td><td style=\"text-align: right;\">         127</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              236</td><td style=\"text-align: right;\">         0.334804</td><td style=\"text-align: right;\">   5.85306e-06</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        797.916 </td></tr>\n",
       "<tr><td>train_model_00185</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000344518</td><td style=\"text-align: right;\">          65</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              452</td><td style=\"text-align: right;\">         0.465394</td><td style=\"text-align: right;\">   6.34869e-07</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        114.529 </td></tr>\n",
       "<tr><td>train_model_00186</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000507477</td><td style=\"text-align: right;\">          52</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              920</td><td style=\"text-align: right;\">         0.383206</td><td style=\"text-align: right;\">   0.0043672  </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        186.132 </td></tr>\n",
       "<tr><td>train_model_00187</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">9.12539e-05</td><td style=\"text-align: right;\">          75</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              457</td><td style=\"text-align: right;\">         0.178813</td><td style=\"text-align: right;\">   0.00100831 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         94.41  </td></tr>\n",
       "<tr><td>train_model_00188</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000129376</td><td style=\"text-align: right;\">          45</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              730</td><td style=\"text-align: right;\">         0.707847</td><td style=\"text-align: right;\">   0.000375426</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         94.5817</td></tr>\n",
       "<tr><td>train_model_00189</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.00018997 </td><td style=\"text-align: right;\">         149</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              882</td><td style=\"text-align: right;\">         0.541886</td><td style=\"text-align: right;\">   3.95762e-06</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        312.653 </td></tr>\n",
       "<tr><td>train_model_00190</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000452768</td><td style=\"text-align: right;\">          23</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              129</td><td style=\"text-align: right;\">         0.960261</td><td style=\"text-align: right;\">   6.53808e-06</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         85.7775</td></tr>\n",
       "<tr><td>train_model_00191</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000132815</td><td style=\"text-align: right;\">         137</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              454</td><td style=\"text-align: right;\">         0.924875</td><td style=\"text-align: right;\">   1.99714e-05</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         98.0051</td></tr>\n",
       "<tr><td>train_model_00192</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000225605</td><td style=\"text-align: right;\">         122</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              771</td><td style=\"text-align: right;\">         0.943956</td><td style=\"text-align: right;\">   3.78188e-07</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        129.531 </td></tr>\n",
       "<tr><td>train_model_00193</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">9.61994e-05</td><td style=\"text-align: right;\">          30</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              481</td><td style=\"text-align: right;\">         0.729499</td><td style=\"text-align: right;\">   2.32477e-06</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         82.7945</td></tr>\n",
       "<tr><td>train_model_00194</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">5.00358e-05</td><td style=\"text-align: right;\">         115</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              325</td><td style=\"text-align: right;\">         0.91083 </td><td style=\"text-align: right;\">   0.00982183 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        141.242 </td></tr>\n",
       "<tr><td>train_model_00195</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">7.49955e-05</td><td style=\"text-align: right;\">          26</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              868</td><td style=\"text-align: right;\">         0.983494</td><td style=\"text-align: right;\">   0.00014441 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         87.5466</td></tr>\n",
       "<tr><td>train_model_00196</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000167252</td><td style=\"text-align: right;\">          58</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              268</td><td style=\"text-align: right;\">         0.172629</td><td style=\"text-align: right;\">   3.80599e-07</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         98.3096</td></tr>\n",
       "<tr><td>train_model_00197</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000156523</td><td style=\"text-align: right;\">          19</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              593</td><td style=\"text-align: right;\">         0.41131 </td><td style=\"text-align: right;\">   1.08901e-07</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         69.75  </td></tr>\n",
       "<tr><td>train_model_00198</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000615397</td><td style=\"text-align: right;\">          81</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              300</td><td style=\"text-align: right;\">         0.859198</td><td style=\"text-align: right;\">   1.01945e-07</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        225.595 </td></tr>\n",
       "<tr><td>train_model_00199</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000151867</td><td style=\"text-align: right;\">          88</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              695</td><td style=\"text-align: right;\">         0.502447</td><td style=\"text-align: right;\">   0.00145322 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        120.344 </td></tr>\n",
       "<tr><td>train_model_00200</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000140499</td><td style=\"text-align: right;\">          78</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              481</td><td style=\"text-align: right;\">         0.525799</td><td style=\"text-align: right;\">   1.62053e-07</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         89.8178</td></tr>\n",
       "<tr><td>train_model_00201</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000201924</td><td style=\"text-align: right;\">          24</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              921</td><td style=\"text-align: right;\">         0.493901</td><td style=\"text-align: right;\">   4.98884e-06</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        199.49  </td></tr>\n",
       "<tr><td>train_model_00202</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000322191</td><td style=\"text-align: right;\">         129</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              779</td><td style=\"text-align: right;\">         0.29772 </td><td style=\"text-align: right;\">   6.53375e-06</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        240.805 </td></tr>\n",
       "<tr><td>train_model_00203</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000103495</td><td style=\"text-align: right;\">         148</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              868</td><td style=\"text-align: right;\">         0.249033</td><td style=\"text-align: right;\">   8.2886e-06 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        123.568 </td></tr>\n",
       "<tr><td>train_model_00204</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">9.75857e-05</td><td style=\"text-align: right;\">          27</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              189</td><td style=\"text-align: right;\">         0.796833</td><td style=\"text-align: right;\">   0.000650687</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        102.219 </td></tr>\n",
       "<tr><td>train_model_00205</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">9.23291e-05</td><td style=\"text-align: right;\">          68</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              633</td><td style=\"text-align: right;\">         0.603492</td><td style=\"text-align: right;\">   0.00220209 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        100.995 </td></tr>\n",
       "<tr><td>train_model_00206</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000130846</td><td style=\"text-align: right;\">          71</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              953</td><td style=\"text-align: right;\">         0.113522</td><td style=\"text-align: right;\">   2.78098e-06</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        245.01  </td></tr>\n",
       "<tr><td>train_model_00207</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000404939</td><td style=\"text-align: right;\">          72</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              852</td><td style=\"text-align: right;\">         0.796062</td><td style=\"text-align: right;\">   1.47667e-07</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        844.439 </td></tr>\n",
       "<tr><td>train_model_00208</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000136247</td><td style=\"text-align: right;\">          83</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              578</td><td style=\"text-align: right;\">         0.237191</td><td style=\"text-align: right;\">   1.07421e-07</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        116.767 </td></tr>\n",
       "<tr><td>train_model_00209</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000504785</td><td style=\"text-align: right;\">         136</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              462</td><td style=\"text-align: right;\">         0.480058</td><td style=\"text-align: right;\">   2.62513e-06</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        118.955 </td></tr>\n",
       "<tr><td>train_model_00210</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000115208</td><td style=\"text-align: right;\">         145</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              118</td><td style=\"text-align: right;\">         0.324515</td><td style=\"text-align: right;\">   1.02687e-05</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        103.479 </td></tr>\n",
       "<tr><td>train_model_00211</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000450454</td><td style=\"text-align: right;\">          17</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              121</td><td style=\"text-align: right;\">         0.910441</td><td style=\"text-align: right;\">   1.70178e-06</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         90.8344</td></tr>\n",
       "<tr><td>train_model_00212</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000264756</td><td style=\"text-align: right;\">         149</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              649</td><td style=\"text-align: right;\">         0.508346</td><td style=\"text-align: right;\">   5.19533e-06</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        291.528 </td></tr>\n",
       "<tr><td>train_model_00213</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000219072</td><td style=\"text-align: right;\">         124</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              236</td><td style=\"text-align: right;\">         0.100207</td><td style=\"text-align: right;\">   3.44448e-07</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         98.9801</td></tr>\n",
       "<tr><td>train_model_00214</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000176673</td><td style=\"text-align: right;\">          71</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              386</td><td style=\"text-align: right;\">         0.400229</td><td style=\"text-align: right;\">   9.44366e-06</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        103.914 </td></tr>\n",
       "<tr><td>train_model_00215</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">8.22161e-05</td><td style=\"text-align: right;\">          20</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              760</td><td style=\"text-align: right;\">         0.185375</td><td style=\"text-align: right;\">   1.1731e-06 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         83.354 </td></tr>\n",
       "<tr><td>train_model_00216</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000239716</td><td style=\"text-align: right;\">          20</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              577</td><td style=\"text-align: right;\">         0.659644</td><td style=\"text-align: right;\">   2.0435e-07 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         73.7391</td></tr>\n",
       "<tr><td>train_model_00217</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000184653</td><td style=\"text-align: right;\">         138</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              887</td><td style=\"text-align: right;\">         0.150174</td><td style=\"text-align: right;\">   0.00130295 </td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        893.313 </td></tr>\n",
       "<tr><td>train_model_00218</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">6.44853e-05</td><td style=\"text-align: right;\">         143</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              611</td><td style=\"text-align: right;\">         0.726125</td><td style=\"text-align: right;\">   7.87084e-06</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        109.599 </td></tr>\n",
       "<tr><td>train_model_00219</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000323852</td><td style=\"text-align: right;\">          11</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              314</td><td style=\"text-align: right;\">         0.916689</td><td style=\"text-align: right;\">   0.00150738 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         90.8641</td></tr>\n",
       "<tr><td>train_model_00220</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000232684</td><td style=\"text-align: right;\">          27</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              215</td><td style=\"text-align: right;\">         0.270877</td><td style=\"text-align: right;\">   5.88539e-07</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         88.5543</td></tr>\n",
       "<tr><td>train_model_00221</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000176146</td><td style=\"text-align: right;\">         113</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              846</td><td style=\"text-align: right;\">         0.238149</td><td style=\"text-align: right;\">   1.88555e-06</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        118.668 </td></tr>\n",
       "<tr><td>train_model_00222</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000239374</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              161</td><td style=\"text-align: right;\">         0.826389</td><td style=\"text-align: right;\">   1.99629e-07</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        121.214 </td></tr>\n",
       "<tr><td>train_model_00223</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000154587</td><td style=\"text-align: right;\">         145</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              939</td><td style=\"text-align: right;\">         0.979119</td><td style=\"text-align: right;\">   0.000105868</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        115.116 </td></tr>\n",
       "<tr><td>train_model_00224</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000172243</td><td style=\"text-align: right;\">         135</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              892</td><td style=\"text-align: right;\">         0.177963</td><td style=\"text-align: right;\">   0.00168745 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        125.934 </td></tr>\n",
       "<tr><td>train_model_00225</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000404256</td><td style=\"text-align: right;\">         130</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              427</td><td style=\"text-align: right;\">         0.931416</td><td style=\"text-align: right;\">   6.33016e-05</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        891.915 </td></tr>\n",
       "<tr><td>train_model_00226</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">6.93434e-05</td><td style=\"text-align: right;\">          87</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              663</td><td style=\"text-align: right;\">         0.190908</td><td style=\"text-align: right;\">   2.58825e-06</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        107.018 </td></tr>\n",
       "<tr><td>train_model_00227</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000214087</td><td style=\"text-align: right;\">          25</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              832</td><td style=\"text-align: right;\">         0.740262</td><td style=\"text-align: right;\">   0.000288249</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         86.1127</td></tr>\n",
       "<tr><td>train_model_00228</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">9.33123e-05</td><td style=\"text-align: right;\">          66</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              221</td><td style=\"text-align: right;\">         0.875119</td><td style=\"text-align: right;\">   0.000136709</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        101.926 </td></tr>\n",
       "<tr><td>train_model_00229</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000241742</td><td style=\"text-align: right;\">         127</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              567</td><td style=\"text-align: right;\">         0.422693</td><td style=\"text-align: right;\">   1.92751e-05</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        110.821 </td></tr>\n",
       "<tr><td>train_model_00230</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000728657</td><td style=\"text-align: right;\">          43</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              714</td><td style=\"text-align: right;\">         0.232689</td><td style=\"text-align: right;\">   1.59108e-07</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        242.815 </td></tr>\n",
       "<tr><td>train_model_00231</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000113452</td><td style=\"text-align: right;\">         133</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              972</td><td style=\"text-align: right;\">         0.819994</td><td style=\"text-align: right;\">   2.20443e-07</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        102.235 </td></tr>\n",
       "<tr><td>train_model_00232</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">5.60581e-05</td><td style=\"text-align: right;\">          47</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              176</td><td style=\"text-align: right;\">         0.150975</td><td style=\"text-align: right;\">   1.34267e-07</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        100.668 </td></tr>\n",
       "<tr><td>train_model_00233</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000753808</td><td style=\"text-align: right;\">         138</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              400</td><td style=\"text-align: right;\">         0.954646</td><td style=\"text-align: right;\">   5.30914e-05</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        232.127 </td></tr>\n",
       "<tr><td>train_model_00234</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">5.46717e-05</td><td style=\"text-align: right;\">         139</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              262</td><td style=\"text-align: right;\">         0.424329</td><td style=\"text-align: right;\">   0.00178353 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        122.284 </td></tr>\n",
       "<tr><td>train_model_00235</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000218665</td><td style=\"text-align: right;\">         106</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              343</td><td style=\"text-align: right;\">         0.245962</td><td style=\"text-align: right;\">   0.00211919 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        113.062 </td></tr>\n",
       "<tr><td>train_model_00236</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000406013</td><td style=\"text-align: right;\">         114</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              682</td><td style=\"text-align: right;\">         0.954314</td><td style=\"text-align: right;\">   0.0024251  </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        252.227 </td></tr>\n",
       "<tr><td>train_model_00237</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000496373</td><td style=\"text-align: right;\">          60</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              730</td><td style=\"text-align: right;\">         0.732943</td><td style=\"text-align: right;\">   0.00121017 </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        250.132 </td></tr>\n",
       "<tr><td>train_model_00238</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000468047</td><td style=\"text-align: right;\">          74</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              413</td><td style=\"text-align: right;\">         0.523947</td><td style=\"text-align: right;\">   6.02902e-07</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       2498.72  </td></tr>\n",
       "<tr><td>train_model_00239</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000956499</td><td style=\"text-align: right;\">          60</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              563</td><td style=\"text-align: right;\">         0.788484</td><td style=\"text-align: right;\">   3.35756e-06</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">       2011.04  </td></tr>\n",
       "<tr><td>train_model_00240</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">8.02271e-05</td><td style=\"text-align: right;\">         106</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              519</td><td style=\"text-align: right;\">         0.657363</td><td style=\"text-align: right;\">   0.00164667 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        112.773 </td></tr>\n",
       "<tr><td>train_model_00241</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">8.51781e-05</td><td style=\"text-align: right;\">          88</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              637</td><td style=\"text-align: right;\">         0.668024</td><td style=\"text-align: right;\">   2.75881e-06</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        111.402 </td></tr>\n",
       "<tr><td>train_model_00242</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000254821</td><td style=\"text-align: right;\">          79</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              880</td><td style=\"text-align: right;\">         0.411177</td><td style=\"text-align: right;\">   0.00412811 </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        250.126 </td></tr>\n",
       "<tr><td>train_model_00243</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000187084</td><td style=\"text-align: right;\">          70</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              335</td><td style=\"text-align: right;\">         0.420302</td><td style=\"text-align: right;\">   2.14247e-05</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         91.5567</td></tr>\n",
       "<tr><td>train_model_00244</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">6.65381e-05</td><td style=\"text-align: right;\">          39</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              678</td><td style=\"text-align: right;\">         0.459021</td><td style=\"text-align: right;\">   0.000581383</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         89.7672</td></tr>\n",
       "<tr><td>train_model_00245</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.00084029 </td><td style=\"text-align: right;\">          98</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              913</td><td style=\"text-align: right;\">         0.530611</td><td style=\"text-align: right;\">   1.33216e-05</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        114.725 </td></tr>\n",
       "<tr><td>train_model_00246</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000680747</td><td style=\"text-align: right;\">         123</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              435</td><td style=\"text-align: right;\">         0.979176</td><td style=\"text-align: right;\">   0.000248087</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        120.226 </td></tr>\n",
       "<tr><td>train_model_00247</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000127345</td><td style=\"text-align: right;\">         144</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              410</td><td style=\"text-align: right;\">         0.728893</td><td style=\"text-align: right;\">   0.000759183</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        122.745 </td></tr>\n",
       "<tr><td>train_model_00248</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">6.26489e-05</td><td style=\"text-align: right;\">          69</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              408</td><td style=\"text-align: right;\">         0.708959</td><td style=\"text-align: right;\">   1.93721e-05</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         92.5524</td></tr>\n",
       "<tr><td>train_model_00249</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000141752</td><td style=\"text-align: right;\">          39</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              206</td><td style=\"text-align: right;\">         0.345751</td><td style=\"text-align: right;\">   8.62476e-05</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         91.644 </td></tr>\n",
       "<tr><td>train_model_00250</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">6.15589e-05</td><td style=\"text-align: right;\">          34</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              397</td><td style=\"text-align: right;\">         0.19941 </td><td style=\"text-align: right;\">   0.00232712 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        106.092 </td></tr>\n",
       "<tr><td>train_model_00251</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000226207</td><td style=\"text-align: right;\">          55</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              295</td><td style=\"text-align: right;\">         0.742775</td><td style=\"text-align: right;\">   0.000973136</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         88.8261</td></tr>\n",
       "<tr><td>train_model_00252</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">7.86953e-05</td><td style=\"text-align: right;\">          92</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              827</td><td style=\"text-align: right;\">         0.685469</td><td style=\"text-align: right;\">   1.03487e-07</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        116.428 </td></tr>\n",
       "<tr><td>train_model_00253</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000911871</td><td style=\"text-align: right;\">         109</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              521</td><td style=\"text-align: right;\">         0.410093</td><td style=\"text-align: right;\">   3.58982e-07</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        109.862 </td></tr>\n",
       "<tr><td>train_model_00254</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000439554</td><td style=\"text-align: right;\">         134</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              718</td><td style=\"text-align: right;\">         0.796497</td><td style=\"text-align: right;\">   0.000121894</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        266.538 </td></tr>\n",
       "<tr><td>train_model_00255</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">6.37898e-05</td><td style=\"text-align: right;\">         137</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              411</td><td style=\"text-align: right;\">         0.604938</td><td style=\"text-align: right;\">   0.00682578 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        125.086 </td></tr>\n",
       "<tr><td>train_model_00256</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000981943</td><td style=\"text-align: right;\">         110</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              531</td><td style=\"text-align: right;\">         0.624904</td><td style=\"text-align: right;\">   1.00809e-07</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        289.509 </td></tr>\n",
       "<tr><td>train_model_00257</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000648452</td><td style=\"text-align: right;\">         121</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              924</td><td style=\"text-align: right;\">         0.712128</td><td style=\"text-align: right;\">   1.67892e-06</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        131.599 </td></tr>\n",
       "<tr><td>train_model_00258</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000448959</td><td style=\"text-align: right;\">          69</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              851</td><td style=\"text-align: right;\">         0.915726</td><td style=\"text-align: right;\">   1.11646e-05</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        212.71  </td></tr>\n",
       "<tr><td>train_model_00259</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000118154</td><td style=\"text-align: right;\">          44</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              770</td><td style=\"text-align: right;\">         0.472024</td><td style=\"text-align: right;\">   3.43651e-05</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         99.2005</td></tr>\n",
       "<tr><td>train_model_00260</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000666468</td><td style=\"text-align: right;\">          82</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              630</td><td style=\"text-align: right;\">         0.765447</td><td style=\"text-align: right;\">   1.25001e-05</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        225.174 </td></tr>\n",
       "<tr><td>train_model_00261</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000765911</td><td style=\"text-align: right;\">          19</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              400</td><td style=\"text-align: right;\">         0.110995</td><td style=\"text-align: right;\">   5.79311e-06</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        228.708 </td></tr>\n",
       "<tr><td>train_model_00262</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000337126</td><td style=\"text-align: right;\">         110</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              629</td><td style=\"text-align: right;\">         0.130334</td><td style=\"text-align: right;\">   2.75281e-06</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        247.549 </td></tr>\n",
       "<tr><td>train_model_00263</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000282568</td><td style=\"text-align: right;\">          88</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              272</td><td style=\"text-align: right;\">         0.72631 </td><td style=\"text-align: right;\">   0.00461861 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        100.141 </td></tr>\n",
       "<tr><td>train_model_00264</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.00046382 </td><td style=\"text-align: right;\">          39</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              561</td><td style=\"text-align: right;\">         0.50956 </td><td style=\"text-align: right;\">   3.1953e-06 </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        179.149 </td></tr>\n",
       "<tr><td>train_model_00265</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000935711</td><td style=\"text-align: right;\">          47</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              381</td><td style=\"text-align: right;\">         0.643418</td><td style=\"text-align: right;\">   0.000152001</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        186.902 </td></tr>\n",
       "<tr><td>train_model_00266</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">9.58289e-05</td><td style=\"text-align: right;\">         100</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              976</td><td style=\"text-align: right;\">         0.687184</td><td style=\"text-align: right;\">   0.00490639 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         98.8963</td></tr>\n",
       "<tr><td>train_model_00267</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000161724</td><td style=\"text-align: right;\">          69</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              564</td><td style=\"text-align: right;\">         0.316966</td><td style=\"text-align: right;\">   0.0059127  </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        101.978 </td></tr>\n",
       "<tr><td>train_model_00268</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000252393</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              330</td><td style=\"text-align: right;\">         0.967866</td><td style=\"text-align: right;\">   2.67773e-05</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         85.8948</td></tr>\n",
       "<tr><td>train_model_00269</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000155693</td><td style=\"text-align: right;\">          61</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              875</td><td style=\"text-align: right;\">         0.557392</td><td style=\"text-align: right;\">   2.22897e-07</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        609.275 </td></tr>\n",
       "<tr><td>train_model_00270</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000527904</td><td style=\"text-align: right;\">          82</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              997</td><td style=\"text-align: right;\">         0.352106</td><td style=\"text-align: right;\">   1.8847e-05 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        101.572 </td></tr>\n",
       "<tr><td>train_model_00271</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000535273</td><td style=\"text-align: right;\">          76</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              171</td><td style=\"text-align: right;\">         0.663284</td><td style=\"text-align: right;\">   7.98528e-06</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        101.193 </td></tr>\n",
       "<tr><td>train_model_00272</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000124753</td><td style=\"text-align: right;\">         135</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              116</td><td style=\"text-align: right;\">         0.297057</td><td style=\"text-align: right;\">   2.07729e-05</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        105.27  </td></tr>\n",
       "<tr><td>train_model_00273</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000373963</td><td style=\"text-align: right;\">          82</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              396</td><td style=\"text-align: right;\">         0.774262</td><td style=\"text-align: right;\">   7.55295e-05</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        211.706 </td></tr>\n",
       "<tr><td>train_model_00274</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000931922</td><td style=\"text-align: right;\">          80</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              995</td><td style=\"text-align: right;\">         0.444164</td><td style=\"text-align: right;\">   4.77467e-05</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        249.899 </td></tr>\n",
       "<tr><td>train_model_00275</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000303675</td><td style=\"text-align: right;\">          28</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              902</td><td style=\"text-align: right;\">         0.590402</td><td style=\"text-align: right;\">   0.000254458</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        188.232 </td></tr>\n",
       "<tr><td>train_model_00276</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000731719</td><td style=\"text-align: right;\">         133</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              999</td><td style=\"text-align: right;\">         0.363414</td><td style=\"text-align: right;\">   2.9552e-05 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        101.023 </td></tr>\n",
       "<tr><td>train_model_00277</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000918797</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              499</td><td style=\"text-align: right;\">         0.116449</td><td style=\"text-align: right;\">   1.22335e-05</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        105.239 </td></tr>\n",
       "<tr><td>train_model_00278</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000511349</td><td style=\"text-align: right;\">          50</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              263</td><td style=\"text-align: right;\">         0.228942</td><td style=\"text-align: right;\">   6.28087e-07</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         86.3424</td></tr>\n",
       "<tr><td>train_model_00279</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000338906</td><td style=\"text-align: right;\">          99</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              304</td><td style=\"text-align: right;\">         0.788236</td><td style=\"text-align: right;\">   1.77436e-05</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         92.9715</td></tr>\n",
       "<tr><td>train_model_00280</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">7.7962e-05 </td><td style=\"text-align: right;\">          28</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              578</td><td style=\"text-align: right;\">         0.48006 </td><td style=\"text-align: right;\">   0.000618226</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         86.9434</td></tr>\n",
       "<tr><td>train_model_00281</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000756712</td><td style=\"text-align: right;\">          37</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              136</td><td style=\"text-align: right;\">         0.130964</td><td style=\"text-align: right;\">   3.14923e-05</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         81.6098</td></tr>\n",
       "<tr><td>train_model_00282</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.00020846 </td><td style=\"text-align: right;\">          90</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              285</td><td style=\"text-align: right;\">         0.433233</td><td style=\"text-align: right;\">   2.56397e-06</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         98.3053</td></tr>\n",
       "<tr><td>train_model_00283</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.00063542 </td><td style=\"text-align: right;\">         142</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              478</td><td style=\"text-align: right;\">         0.450466</td><td style=\"text-align: right;\">   3.65584e-07</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        102.299 </td></tr>\n",
       "<tr><td>train_model_00284</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000264937</td><td style=\"text-align: right;\">          63</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              836</td><td style=\"text-align: right;\">         0.354748</td><td style=\"text-align: right;\">   6.19627e-06</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        208.9   </td></tr>\n",
       "<tr><td>train_model_00285</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000246124</td><td style=\"text-align: right;\">          78</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              989</td><td style=\"text-align: right;\">         0.606501</td><td style=\"text-align: right;\">   0.00165504 </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        230.418 </td></tr>\n",
       "<tr><td>train_model_00286</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">9.60325e-05</td><td style=\"text-align: right;\">         121</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              880</td><td style=\"text-align: right;\">         0.32976 </td><td style=\"text-align: right;\">   0.000135652</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        108.263 </td></tr>\n",
       "<tr><td>train_model_00287</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000108258</td><td style=\"text-align: right;\">          38</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              750</td><td style=\"text-align: right;\">         0.357652</td><td style=\"text-align: right;\">   0.000380759</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         76.3512</td></tr>\n",
       "<tr><td>train_model_00288</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">8.62306e-05</td><td style=\"text-align: right;\">          41</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              935</td><td style=\"text-align: right;\">         0.689369</td><td style=\"text-align: right;\">   2.28574e-06</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         80.6402</td></tr>\n",
       "<tr><td>train_model_00289</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">5.25201e-05</td><td style=\"text-align: right;\">          52</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              836</td><td style=\"text-align: right;\">         0.684381</td><td style=\"text-align: right;\">   2.70381e-07</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         91.3533</td></tr>\n",
       "<tr><td>train_model_00290</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000516254</td><td style=\"text-align: right;\">          34</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              500</td><td style=\"text-align: right;\">         0.144256</td><td style=\"text-align: right;\">   0.00102033 </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        208.734 </td></tr>\n",
       "<tr><td>train_model_00291</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000572887</td><td style=\"text-align: right;\">          75</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              232</td><td style=\"text-align: right;\">         0.468046</td><td style=\"text-align: right;\">   0.000322306</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        102.14  </td></tr>\n",
       "<tr><td>train_model_00292</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">5.43715e-05</td><td style=\"text-align: right;\">          60</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              455</td><td style=\"text-align: right;\">         0.310327</td><td style=\"text-align: right;\">   2.10315e-05</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         93.9219</td></tr>\n",
       "<tr><td>train_model_00293</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000160813</td><td style=\"text-align: right;\">         110</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              940</td><td style=\"text-align: right;\">         0.620451</td><td style=\"text-align: right;\">   1.35825e-05</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        197.938 </td></tr>\n",
       "<tr><td>train_model_00294</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000189645</td><td style=\"text-align: right;\">         113</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              376</td><td style=\"text-align: right;\">         0.83803 </td><td style=\"text-align: right;\">   0.00916031 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        102.161 </td></tr>\n",
       "<tr><td>train_model_00295</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">9.40301e-05</td><td style=\"text-align: right;\">         108</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              432</td><td style=\"text-align: right;\">         0.516671</td><td style=\"text-align: right;\">   3.29462e-07</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        110.248 </td></tr>\n",
       "<tr><td>train_model_00296</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000734803</td><td style=\"text-align: right;\">         143</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              263</td><td style=\"text-align: right;\">         0.994776</td><td style=\"text-align: right;\">   1.63166e-06</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        205.505 </td></tr>\n",
       "<tr><td>train_model_00297</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000127394</td><td style=\"text-align: right;\">          86</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              554</td><td style=\"text-align: right;\">         0.910671</td><td style=\"text-align: right;\">   3.10049e-07</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         89.8794</td></tr>\n",
       "<tr><td>train_model_00298</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">7.40736e-05</td><td style=\"text-align: right;\">          44</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              749</td><td style=\"text-align: right;\">         0.54525 </td><td style=\"text-align: right;\">   0.00235975 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         87.5707</td></tr>\n",
       "<tr><td>train_model_00299</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.000685772</td><td style=\"text-align: right;\">         103</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">              635</td><td style=\"text-align: right;\">         0.395167</td><td style=\"text-align: right;\">   1.27907e-07</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         90.8505</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from pycox import datasets\n",
    "\n",
    "kfold = KFold(5, shuffle=True)\n",
    "df_all = datasets.gbsg.read_df()\n",
    "gen = kfold.split(df_all)\n",
    "\n",
    "# ray.init(webui_host='0.0.0.0')\n",
    "\n",
    "odesurv_bench_vals_rot = []\n",
    "for g in gen:\n",
    "    df_train = df_all.iloc[g[0]]\n",
    "    df_test =  df_all.iloc[g[1]]\n",
    "    conc, ibs, ibnll = odesurv_benchmark(df_train,df_test,\"rot\")\n",
    "    odesurv_bench_vals_rot.append([conc,ibs,ibnll])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c=0.6793196362142764+-0.009407195337106437\n",
      "ibs=0.17484367616111518+-0.0026219616036148308\n",
      "ibnll=0.5192838730250453+-0.007654624145028555\n"
     ]
    }
   ],
   "source": [
    "print(\"c=\"+str(np.mean(np.array(odesurv_bench_vals_rot)[:,0]))+\"+-\"+str(np.std(np.array(odesurv_bench_vals_rot)[:,0])))\n",
    "print(\"ibs=\"+str(np.mean(np.array(odesurv_bench_vals_rot)[:,1]))+\"+-\"+str(np.std(np.array(odesurv_bench_vals_rot)[:,1])))\n",
    "print(\"ibnll=\"+str(np.mean(np.array(odesurv_bench_vals_rot)[:,2]))+\"+-\"+str(np.std(np.array(odesurv_bench_vals_rot)[:,2])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "def odesurv_benchmark(df_train, df_test, name, fold_num):\n",
    "    torch.cuda.empty_cache()\n",
    "    df_train, df_val = train_test_split(df_train, test_size=0.2)\n",
    "    \n",
    "    Tmax = df_train[\"duration\"].max()*1.2\n",
    "    \n",
    "    def train_model(config):\n",
    "        train_loader = make_dataloader(df_train,int(len(df_train)*config[\"batch_size\"]))\n",
    "        val_loader = make_dataloader(df_val,len(df_val))\n",
    "        \n",
    "        num_in = 14\n",
    "        num_latent = config[\"num_latent\"]\n",
    "        layers_encoder =  [config[\"encoder_neurons\"]]*config[\"num_encoder_layers\"]\n",
    "        dropout_encoder = [config[\"encoder_dropout\"]]*config[\"num_encoder_layers\"]\n",
    "        layers_odefunc =  [config[\"odefunc_neurons\"]]*config[\"num_odefunc_layers\"]\n",
    "        dropout_odefunc = []\n",
    "        \n",
    "        trans_matrix = torch.tensor([[np.nan,1],[np.nan,np.nan]]).to(device)\n",
    "\n",
    "        encoder = Encoder(num_in,num_latent,layers_encoder, dropout_encoder).to(device)\n",
    "        odefunc = ODEFunc(trans_matrix,num_in,num_latent,layers_odefunc,dropout_odefunc,config[\"softplus_beta\"]).to(device)\n",
    "        block = ODEBlock(odefunc).to(device)\n",
    "        odesurv = ODEsurv(block,encoder,Tmax,config[\"multiplier\"]).to(device)\n",
    "\n",
    "        optimizer = torch.optim.Adam(odesurv.parameters(), weight_decay = config[\"weight_decay\"], lr=config[\"lr\"])\n",
    "#         scheduler = torch.optim.lr_scheduler.StepLR(optimizer, config[\"scheduler_epoch\"], gamma=config[\"scheduler_gamma\"])\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=config[\"scheduler_gamma\"], patience=config[\"scheduler_epoch\"])\n",
    "\n",
    "        conc_max = -np.inf\n",
    "        for i in range(1000):\n",
    "            odesurv.train()\n",
    "            for mini,ds in enumerate(train_loader):\n",
    "                myloss,_,_ = loss(odesurv,*ds,mu=config[\"mu\"])\n",
    "                optimizer.zero_grad()\n",
    "                myloss.backward()    \n",
    "                optimizer.step()\n",
    "       \n",
    "            odesurv.eval()\n",
    "            with torch.no_grad():\n",
    "                train_loss = 0.\n",
    "                for mini,ds in enumerate(train_loader):\n",
    "                    myloss,_,_ = loss(odesurv,*ds,mu=config[\"mu\"])\n",
    "                    train_loss += myloss.item()\n",
    "                lossval,conc,ibs,ibnll,reg,loglik = 0., 0., 0., 0., 0., 0.\n",
    "                for _,ds in enumerate(val_loader):\n",
    "                    t1,t2,t3 = loss(odesurv,*ds,mu=config[\"mu\"])\n",
    "                    lossval += t1.item()\n",
    "                    loglik += t2.item()\n",
    "                    reg += t3.item()\n",
    "                    t1,t2,t3 = measures(odesurv,torch.tensor([1.,0.],device=device),*ds)\n",
    "                    conc += t1\n",
    "                    ibs += t2\n",
    "                    ibnll += t3\n",
    "                track.log(concordance=conc/len(val_loader),int_brier_score=ibs/len(val_loader),int_bin_nll = ibnll/len(val_loader), loss=lossval/len(val_loader), loglik = loglik/len(val_loader), reg=reg/len(val_loader), train_loss=train_loss/len(val_loader))\n",
    "                scheduler.step(lossval/len(val_loader))\n",
    "                if conc/len(val_loader) > conc_max:\n",
    "                    torch.save(odesurv.state_dict(), 'checkpoint.pt')\n",
    "                    \n",
    "    \n",
    "    dt_string = name+\"_fold_\"+str(fold_num)+datetime.now().strftime(\"_date_%d_%m_%Y_time_%H_%M_%S\")\n",
    "    analysis = tune.run(train_model, name=dt_string, num_samples=1, stop={\"training_iteration\": 110}, config=search_space, resources_per_trial={\"cpu\": 1, \"gpu\": 1}, verbose=1, raise_on_failed_trial=False)\n",
    "#     analysis = tune.run(train_model, name=dt_string, num_samples=300, scheduler=ASHAScheduler(metric=\"loss\", mode=\"min\"), config=search_space, resources_per_trial={\"cpu\": 1, \"gpu\": 0.25}, verbose=1, raise_on_failed_trial=False)\n",
    "#     analysis = tune.run(train_model, name=dt_string, num_samples=50, scheduler=ASHAScheduler(metric=\"concordance\", mode=\"max\", max_t=80, grace_period=20, reduction_factor=2, brackets=3), config=search_space, resources_per_trial={\"cpu\": 1, \"gpu\": 0.25}, verbose=1, raise_on_failed_trial=False)\n",
    "    best_config = analysis.get_best_config(metric=\"concordance\", mode=\"max\")\n",
    "    logdir = analysis.get_best_logdir(\"concordance\", mode=\"max\")\n",
    "    \n",
    "    num_in = 14\n",
    "    num_latent = best_config[\"num_latent\"]\n",
    "    layers_encoder =  [best_config[\"encoder_neurons\"]]*best_config[\"num_encoder_layers\"]\n",
    "    dropout_encoder = [best_config[\"encoder_dropout\"]]*best_config[\"num_encoder_layers\"]\n",
    "    layers_odefunc =  [best_config[\"odefunc_neurons\"]]*best_config[\"num_odefunc_layers\"]\n",
    "    dropout_odefunc = []\n",
    "\n",
    "    trans_matrix = torch.tensor([[np.nan,1],[np.nan,np.nan]]).to(device)\n",
    "\n",
    "    encoder = Encoder(num_in,num_latent,layers_encoder, dropout_encoder).to(device)\n",
    "    odefunc = ODEFunc(trans_matrix,num_in,num_latent,layers_odefunc,dropout_odefunc,best_config[\"softplus_beta\"]).to(device)\n",
    "    block = ODEBlock(odefunc).to(device)\n",
    "    odesurv = ODEsurv(block,encoder,Tmax,best_config[\"multiplier\"]).to(device)\n",
    "    \n",
    "    odesurv.load_state_dict(torch.load(os.path.join(logdir, \"checkpoint.pt\")))\n",
    "\n",
    "    loader = make_dataloader(df_test,len(df_test))\n",
    "    odesurv.eval()\n",
    "    with torch.no_grad():\n",
    "        conc,ibs,ibnll = 0., 0., 0.\n",
    "        for _,ds in enumerate(loader):\n",
    "            t1,t2,t3 = measures(odesurv,torch.tensor([1.,0.],device=device),*ds)\n",
    "            conc += t1\n",
    "            ibs += t2\n",
    "            ibnll += t3\n",
    "    return conc/len(loader), ibs/len(loader), ibnll/len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn_pandas import DataFrameMapper\n",
    "import pandas as pd\n",
    "\n",
    "def make_dataloader(df,batchsize):\n",
    "    cols_standardize = ['x0', 'x7', 'x8', 'x9', 'x10', 'x11', 'x12', 'x13']\n",
    "    cols_leave = ['x1', 'x2', 'x3', 'x4', 'x5', 'x6']\n",
    "\n",
    "    standardize = [([col], StandardScaler()) for col in cols_standardize]\n",
    "    leave = [(col, None) for col in cols_leave]\n",
    "\n",
    "    x_mapper = DataFrameMapper(standardize + leave)\n",
    "    X = x_mapper.fit_transform(df).astype('float32')\n",
    "    \n",
    "    X = torch.from_numpy(X).float().to(device)\n",
    "    T = torch.from_numpy(df[[\"duration\"]].values).float().flatten().to(device)\n",
    "    T[T==0] = 1e-8\n",
    "    E = torch.from_numpy(df[[\"event\"]].values).float().flatten().to(device)\n",
    "\n",
    "    Tstart = torch.from_numpy(np.array([0 for i in range(T.shape[0])])).float().to(device)\n",
    "    From = torch.tensor([1],device=device).repeat((T.shape))\n",
    "    To = torch.tensor([2],device=device).repeat((T.shape))\n",
    "    trans = torch.tensor([1],device=device).repeat((T.shape))\n",
    "\n",
    "    dataset = TensorDataset(X,Tstart,T,From,To,trans,E)\n",
    "    loader = DataLoader(dataset, batch_size=batchsize, shuffle=True)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune import track\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "# search_space = {\n",
    "#     \"lr\": tune.grid_search([6e-4,8e-4,1e-3,2e-3]),\n",
    "#     \"weight_decay\": 1e-4,\n",
    "#     \"num_latent\": 5,\n",
    "#     \"encoder_neurons\": 50,\n",
    "#     \"num_encoder_layers\": 1,\n",
    "#     \"encoder_dropout\": 0.,\n",
    "#     \"odefunc_neurons\": 800,\n",
    "#     \"num_odefunc_layers\": 2,\n",
    "#     \"batch_size\": 1/16,\n",
    "#     \"multiplier\": 2.,\n",
    "#     \"mu\": 0.1,\n",
    "#     \"softplus_beta\": 1.,\n",
    "#     \"scheduler_epoch\": 100,\n",
    "#     \"scheduler_gamma\": 0.1\n",
    "# }\n",
    "\n",
    "search_space = {\n",
    "    \"lr\": tune.grid_search([1e-4,5e-4,1e-3]),\n",
    "    \"weight_decay\": tune.grid_search([1e-7,1e-5,1e-3]),\n",
    "    \"num_latent\": 200,\n",
    "    \"encoder_neurons\": 400,\n",
    "    \"num_encoder_layers\": 2,\n",
    "    \"encoder_dropout\": 0.,\n",
    "    \"odefunc_neurons\": tune.grid_search([400,1000]),\n",
    "    \"num_odefunc_layers\": tune.grid_search([2,4]),\n",
    "    \"batch_size\": 1/16,\n",
    "    \"multiplier\": 2.,\n",
    "    \"mu\": 1e-4,\n",
    "    \"softplus_beta\": 1.,\n",
    "    \"scheduler_epoch\": 20,\n",
    "    \"scheduler_gamma\": 0.1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 32.5/59.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/16 CPUs, 4/4 GPUs, 0.0/22.41 GiB heap, 0.0/7.71 GiB objects<br>Result logdir: /home/jupyter/ray_results/support_fold_1_date_30_05_2020_time_05_40_31<br>Number of trials: 36 (1 ERROR, 19 PENDING, 4 RUNNING, 12 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name       </th><th>status    </th><th>loc             </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  num_odefunc_layers</th><th style=\"text-align: right;\">  odefunc_neurons</th><th style=\"text-align: right;\">  weight_decay</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_model_00010</td><td>ERROR     </td><td>                </td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">             1000</td><td style=\"text-align: right;\">         1e-07</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         2266.83</td></tr>\n",
       "<tr><td>train_model_00017</td><td>PENDING   </td><td>                </td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">              400</td><td style=\"text-align: right;\">         1e-05</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_model_00018</td><td>PENDING   </td><td>                </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">             1000</td><td style=\"text-align: right;\">         1e-05</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_model_00019</td><td>PENDING   </td><td>                </td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">             1000</td><td style=\"text-align: right;\">         1e-05</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_model_00020</td><td>PENDING   </td><td>                </td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">             1000</td><td style=\"text-align: right;\">         1e-05</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_model_00021</td><td>PENDING   </td><td>                </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">             1000</td><td style=\"text-align: right;\">         1e-05</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_model_00022</td><td>PENDING   </td><td>                </td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">             1000</td><td style=\"text-align: right;\">         1e-05</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_model_00023</td><td>PENDING   </td><td>                </td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">             1000</td><td style=\"text-align: right;\">         1e-05</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_model_00024</td><td>PENDING   </td><td>                </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              400</td><td style=\"text-align: right;\">         0.001</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "<tr><td>train_model_00013</td><td>RUNNING   </td><td>10.142.0.7:19457</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              400</td><td style=\"text-align: right;\">         1e-05</td><td style=\"text-align: right;\">    70</td><td style=\"text-align: right;\">         7477.84</td></tr>\n",
       "<tr><td>train_model_00014</td><td>RUNNING   </td><td>10.142.0.7:19450</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              400</td><td style=\"text-align: right;\">         1e-05</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         2857.35</td></tr>\n",
       "<tr><td>train_model_00015</td><td>RUNNING   </td><td>10.142.0.7:19456</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">              400</td><td style=\"text-align: right;\">         1e-05</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         1654.44</td></tr>\n",
       "<tr><td>train_model_00016</td><td>RUNNING   </td><td>10.142.0.7:21140</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">              400</td><td style=\"text-align: right;\">         1e-05</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">          570.82</td></tr>\n",
       "<tr><td>train_model_00000</td><td>TERMINATED</td><td>                </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              400</td><td style=\"text-align: right;\">         1e-07</td><td style=\"text-align: right;\">   110</td><td style=\"text-align: right;\">        13263.7 </td></tr>\n",
       "<tr><td>train_model_00001</td><td>TERMINATED</td><td>                </td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              400</td><td style=\"text-align: right;\">         1e-07</td><td style=\"text-align: right;\">   110</td><td style=\"text-align: right;\">         9326.21</td></tr>\n",
       "<tr><td>train_model_00002</td><td>TERMINATED</td><td>                </td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              400</td><td style=\"text-align: right;\">         1e-07</td><td style=\"text-align: right;\">   110</td><td style=\"text-align: right;\">         8283.17</td></tr>\n",
       "<tr><td>train_model_00003</td><td>TERMINATED</td><td>                </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">              400</td><td style=\"text-align: right;\">         1e-07</td><td style=\"text-align: right;\">   110</td><td style=\"text-align: right;\">        10881.6 </td></tr>\n",
       "<tr><td>train_model_00004</td><td>TERMINATED</td><td>                </td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">              400</td><td style=\"text-align: right;\">         1e-07</td><td style=\"text-align: right;\">   110</td><td style=\"text-align: right;\">        17983.3 </td></tr>\n",
       "<tr><td>train_model_00005</td><td>TERMINATED</td><td>                </td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">              400</td><td style=\"text-align: right;\">         1e-07</td><td style=\"text-align: right;\">   110</td><td style=\"text-align: right;\">        14228.3 </td></tr>\n",
       "<tr><td>train_model_00006</td><td>TERMINATED</td><td>                </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">             1000</td><td style=\"text-align: right;\">         1e-07</td><td style=\"text-align: right;\">   110</td><td style=\"text-align: right;\">         8165.36</td></tr>\n",
       "<tr><td>train_model_00007</td><td>TERMINATED</td><td>                </td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">             1000</td><td style=\"text-align: right;\">         1e-07</td><td style=\"text-align: right;\">   110</td><td style=\"text-align: right;\">        24373.8 </td></tr>\n",
       "</tbody>\n",
       "</table><br>... 16 more trials not shown (11 PENDING, 4 TERMINATED)<br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name       </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                                                                                                        </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_model_00010</td><td style=\"text-align: right;\">           1</td><td>/home/jupyter/ray_results/support_fold_1_date_30_05_2020_time_05_40_31/train_model_10_lr=0.0005,num_odefunc_layers=4,odefunc_neurons=1000,weight_decay=1e-07_2020-05-30_12-58-21tln5yyf8/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from pycox import datasets\n",
    "\n",
    "kfold = KFold(5, shuffle=True)\n",
    "df_all = datasets.support.read_df()\n",
    "gen = kfold.split(df_all)\n",
    "\n",
    "\n",
    "ray.init(webui_host='0.0.0.0')\n",
    "\n",
    "odesurv_bench_vals = []\n",
    "fold_num = 1\n",
    "for g in gen:\n",
    "    df_train = df_all.iloc[g[0]]\n",
    "    df_test =  df_all.iloc[g[1]]\n",
    "    conc, ibs, ibnll = odesurv_benchmark(df_train,df_test,\"support\",fold_num)\n",
    "    fold_num+=1\n",
    "    odesurv_bench_vals.append([conc,ibs,ibnll])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"c=\"+str(np.mean(np.array(odesurv_bench_vals)[:,0]))+\"+-\"+str(np.std(np.array(odesurv_bench_vals)[:,0])))\n",
    "print(\"ibs=\"+str(np.mean(np.array(odesurv_bench_vals)[:,1]))+\"+-\"+str(np.std(np.array(odesurv_bench_vals)[:,1])))\n",
    "print(\"ibnll=\"+str(np.mean(np.array(odesurv_bench_vals)[:,2]))+\"+-\"+str(np.std(np.array(odesurv_bench_vals)[:,2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c=0.6153642201152687+-0.00585844798470677\n",
      "ibs=0.19639579818750258+-0.003522886894064154\n",
      "ibnll=0.5746934067995662+-0.008562732425823203\n"
     ]
    }
   ],
   "source": [
    "print(\"c=\"+str(np.mean(np.array(odesurv_bench_vals)[:,0]))+\"+-\"+str(np.std(np.array(odesurv_bench_vals)[:,0])))\n",
    "print(\"ibs=\"+str(np.mean(np.array(odesurv_bench_vals)[:,1]))+\"+-\"+str(np.std(np.array(odesurv_bench_vals)[:,1])))\n",
    "print(\"ibnll=\"+str(np.mean(np.array(odesurv_bench_vals)[:,2]))+\"+-\"+str(np.std(np.array(odesurv_bench_vals)[:,2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c=0.6132351299915724+-0.010722358185681976\n",
      "ibs=0.19424970554238233+-0.0031933519251837066\n",
      "ibnll=0.5706360183291317+-0.007300024663354327\n"
     ]
    }
   ],
   "source": [
    "print(\"c=\"+str(np.mean(np.array(odesurv_bench_vals)[:,0]))+\"+-\"+str(np.std(np.array(odesurv_bench_vals)[:,0])))\n",
    "print(\"ibs=\"+str(np.mean(np.array(odesurv_bench_vals)[:,1]))+\"+-\"+str(np.std(np.array(odesurv_bench_vals)[:,1])))\n",
    "print(\"ibnll=\"+str(np.mean(np.array(odesurv_bench_vals)[:,2]))+\"+-\"+str(np.std(np.array(odesurv_bench_vals)[:,2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c=0.618216622635997+-0.006597502544798807\n",
      "ibs=0.1935314575770763+-0.004279757496571185\n",
      "ibnll=0.5689731250634114+-0.009453219670385243\n"
     ]
    }
   ],
   "source": [
    "print(\"c=\"+str(np.mean(np.array(odesurv_bench_vals)[:,0]))+\"+-\"+str(np.std(np.array(odesurv_bench_vals)[:,0])))\n",
    "print(\"ibs=\"+str(np.mean(np.array(odesurv_bench_vals)[:,1]))+\"+-\"+str(np.std(np.array(odesurv_bench_vals)[:,1])))\n",
    "print(\"ibnll=\"+str(np.mean(np.array(odesurv_bench_vals)[:,2]))+\"+-\"+str(np.std(np.array(odesurv_bench_vals)[:,2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c=0.6102313589577636+-0.0038118403064905135\n",
      "ibs=0.19567585951137095+-0.002301237997946347\n",
      "ibnll=0.5741010303897506+-0.00575819840005718\n"
     ]
    }
   ],
   "source": [
    "print(\"c=\"+str(np.mean(np.array(odesurv_bench_vals)[:,0]))+\"+-\"+str(np.std(np.array(odesurv_bench_vals)[:,0])))\n",
    "print(\"ibs=\"+str(np.mean(np.array(odesurv_bench_vals)[:,1]))+\"+-\"+str(np.std(np.array(odesurv_bench_vals)[:,1])))\n",
    "print(\"ibnll=\"+str(np.mean(np.array(odesurv_bench_vals)[:,2]))+\"+-\"+str(np.std(np.array(odesurv_bench_vals)[:,2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c=0.622009475948327+-0.009656146122673258\n",
      "ibs=0.19717123718515026+-0.00618661731145133\n",
      "ibnll=0.5829257759083994+-0.019397555262266865\n"
     ]
    }
   ],
   "source": [
    "print(\"c=\"+str(np.mean(np.array(odesurv_bench_vals)[:,0]))+\"+-\"+str(np.std(np.array(odesurv_bench_vals)[:,0])))\n",
    "print(\"ibs=\"+str(np.mean(np.array(odesurv_bench_vals)[:,1]))+\"+-\"+str(np.std(np.array(odesurv_bench_vals)[:,1])))\n",
    "print(\"ibnll=\"+str(np.mean(np.array(odesurv_bench_vals)[:,2]))+\"+-\"+str(np.std(np.array(odesurv_bench_vals)[:,2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
